<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>杨一瓶</title>
  
  <subtitle>非常に広い肩</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-08-08T08:06:25.710Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Yangyiqing</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>目录导航</title>
    <link href="http://yoursite.com/2028/04/24/navigation/"/>
    <id>http://yoursite.com/2028/04/24/navigation/</id>
    <published>2028-04-24T03:21:51.000Z</published>
    <updated>2018-08-08T08:06:25.710Z</updated>
    
    <content type="html"><![CDATA[<p><strong>机器学习理论</strong></p><blockquote><p>[<a href="http://yangyiqing.cn/2018/03/20/gbdt/" target="_blank" rel="noopener">1.梯度提升树GBDT</a>] [ <a href="http://yangyiqing.cn/2018/03/19/adaboost/" target="_blank" rel="noopener">2.浅谈boosting与Adaboost</a>] [ <a href="http://yangyiqing.cn/2018/03/16/svm/" target="_blank" rel="noopener">3.浅谈SVM与感知机</a>] [ <a href="http://yangyiqing.cn/2018/03/16/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">4.广义线性模型</a>]  [ <a href="http://yangyiqing.cn/2018/03/14/%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">5.逻辑斯蒂回归</a>]  [ <a href="http://yangyiqing.cn/2018/03/11/%E7%94%9F%E6%88%90%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">6.对生成模型与判别模型的理解</a>]  [ <a href="http://yangyiqing.cn/2018/03/09/%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">7.决策树原理</a>]  [ <a href="http://yangyiqing.cn/2018/02/01/DTree/" target="_blank" rel="noopener">8.决策树生成代码</a>] [ <a href="http://yangyiqing.cn/2018/05/01/sparseMatrix/" target="_blank" rel="noopener">9.稀疏矩阵实践及意义</a>] [ <a href="http://yangyiqing.cn/2018/05/04/ffm/" target="_blank" rel="noopener">10.FM和FFM模型原理及实践</a>] [ <a href="http://yangyiqing.cn/2017/06/09/word2vec/" target="_blank" rel="noopener">11.Softmax and CBOW</a>] [ <a href="http://yangyiqing.cn/2017/06/14/suishou/" target="_blank" rel="noopener">12.零碎知识点</a>] [ <a href="http://yangyiqing.cn/2017/06/21/auc/" target="_blank" rel="noopener">13.加权AUC和MAP</a>]  [<a href="http://yangyiqing.cn/2018/06/22/bn/" target="_blank" rel="noopener">14.Batch Normalization</a>] [<a href="http://yangyiqing.cn/2018/07/25/markov/" target="_blank" rel="noopener">15.隐马尔科夫模型和维特比算法</a>] [<a href="http://yangyiqing.cn/2018/07/26/mdp/" target="_blank" rel="noopener">15.马尔科夫决策过程MDP</a>] [<a href="http://yangyiqing.cn/2018/08/07/l1/" target="_blank" rel="noopener">16.L1和L2正则化</a>] [<a href="http://yangyiqing.cn/2018/08/08/drop/" target="_blank" rel="noopener">17.Drop-out</a>] [<a href="http://yangyiqing.cn/2018/08/08/LFM/" target="_blank" rel="noopener">18.LFM隐因子模型</a>]</p></blockquote><p><strong>数据挖掘比赛</strong></p><blockquote><p>[<a href="http://yangyiqing.cn/2018/02/08/2017CCF/" target="_blank" rel="noopener">1.2017 CCF 大数据竞赛top4%</a>] [<a href="http://yangyiqing.cn/2017/12/30/JDD/" target="_blank" rel="noopener">2.2017-JDD京东金融算法大赛15th解决方案</a>] </p></blockquote><p><strong>算法题</strong></p><blockquote><p>[<a href="http://yangyiqing.cn/2018/01/07/%E5%89%91%E6%8C%87offer-1/" target="_blank" rel="noopener">1.剑指offer</a>] [<a href="http://yangyiqing.cn/2017/12/28/leetcode/" target="_blank" rel="noopener">2.Leetcode</a>] </p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;机器学习理论&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[&lt;a href=&quot;http://yangyiqing.cn/2018/03/20/gbdt/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;1.梯度提升树GBDT&lt;/
      
    
    </summary>
    
    
      <category term="导航" scheme="http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"/>
    
  </entry>
  
  <entry>
    <title>LFM隐因子模型</title>
    <link href="http://yoursite.com/2018/08/08/LFM/"/>
    <id>http://yoursite.com/2018/08/08/LFM/</id>
    <published>2018-08-08T08:04:01.000Z</published>
    <updated>2018-08-08T08:05:14.689Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LFM-隐因子模型"><a href="#LFM-隐因子模型" class="headerlink" title="LFM-隐因子模型"></a>LFM-隐因子模型</h1><p><strong>声明：本文绝大部分转自<a href="https://www.cnblogs.com/zhangchaoyang/articles/5517186.html" target="_blank" rel="noopener">博客园-Orisun</a></strong></p><p>LFM即隐因子模型，我们可以把隐因子理解为主题模型中的主题、HMM中的隐藏变量。比如一个用户喜欢《推荐系统实践》这本书，背后的原因可能是该用户喜欢推荐系统、或者是喜欢数据挖掘、亦或者是喜欢作者项亮本人等等，假如真的是由于这3个原因导致的，那如果项亮出了另外一本数据挖掘方面的书，我们可以推测该用户也会喜欢，这“背后的原因”我们称之为隐因子。所以LFM的其中思路就是先计算用户对各个隐因子的喜好程度$(p_1,p_2,p_3….p_f)$，再计算物品在各个隐因子上的概率分布$(q_1,q_2,q_3….q_f)$，<strong>两个向量做内积即得到用户对物品的喜好程度</strong>，下面就讲这两个向量怎么求。</p><p>假设我们已经有了一个评分矩阵$R_{m,n}$， $m$个用户对$n$个物品的评分全在这个矩阵里，当然这是一个高度稀疏的矩阵，我们用$r_{u,i}$表示用户$u$对物品$i$的评分。LFM认为$R_{m,n}=P_{m,f} · Q_{f,n}$，即$R$是两个矩阵的乘积(所以LFM又被称为矩阵分解法，MF，matrix factorization model )，$F$是隐因子的个数，$P$ 的每一行代表一个用户对各个隐因子的偏向程度(举个例子比如用户对电影的评分矩阵分解以后，用户矩阵的隐因子可以认为是电影的类别，比如喜剧片，恐怖片等)，$Q$的每一列代表一个物品在各个隐因子上的概率分布：</p><p><img src="/2018/08/08/LFM/1.png" width="180"></p><p><strong>上面的式子表示用户$u$对物品$i$的评分为，用户$u$的行向量中的每个元素乘以物品$i$的列向量中的对应元素的乘积的累加和</strong>。</p><p>机器学习训练的目标是使得<strong>对所有的$r_{u,i}≠0$,</strong> $r_{u,i}$和$r’_{u,i}$尽可能相等，即使得<strong>平方差loss最小：</strong></p><p><img src="/2018/08/08/LFM/2.png" width="260"></p><p>为了防止过拟合，可以加个<strong>L2正则项：</strong></p><p><img src="/2018/08/08/LFM/3.png" width="450"></p><p>采用梯度下降法求解上面的无约束最优化问题，在第$t+1$轮迭代中$P$和$Q$的值分别应该是:</p><p><img src="/2018/08/08/LFM/4.png" width="600"></p><p>以上就是梯度下降法的所有公式，我们注意到：</p><p><img src="/2018/08/08/LFM/5.png" width="700"></p><p>随机梯度下降法(SGD,Stochastic Gradient Descent)没有严密的理论证明，但是在实践中它通常比传统的梯度下降法需要更少的迭代次数就可以收敛，它有两个特点：</p><p><img src="/2018/08/08/LFM/6.png" width="700"></p><p>SGD单轮迭代的时间复杂度也是$m <em> F </em> n’$，但由于它是单个参数地更新，且更新单个参数时只利用到一个样本（一个评分），更新后的参数立即可用于更新剩下的参数，所以SGD比批量的梯度下降需要更少的迭代次数。</p><p><strong>在训练模型的时候我们只要求模型尽量拟合$r_{u,i} ≠ 0$的情况，对于$r_{u,i}=0$的情况我们也不希望$r’<em>{u,i}=0$，因为$r</em>{u,i}=0$只表示用户$u$没有对物品$i$评分，并不代表用$u$户对物品$i$的喜好程度为0。而恰恰$r’<em>{u,i}$能反映用户$u$对物品$i$的喜好程度，对所有$r’</em>{u,i}$降序排列，取出$TopK$就是用户$u$的推荐列表。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line">__author__ = <span class="string">"orisun"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LFM</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rating_data, F, alpha=<span class="number">0.1</span>, lmbd=<span class="number">0.1</span>, max_iter=<span class="number">500</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">            rating_data是list&lt;(user,list&lt;(position,rate)&gt;)&gt;类型</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.F = F</span><br><span class="line">        self.P = dict()  <span class="comment"># R=PQ^T，代码中的Q相当于博客中Q的转置</span></span><br><span class="line">        self.Q = dict()</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        self.lmbd = lmbd</span><br><span class="line">        self.max_iter = max_iter</span><br><span class="line">        self.rating_data = rating_data</span><br><span class="line"></span><br><span class="line">        <span class="string">'''随机初始化矩阵P和Q'''</span></span><br><span class="line">        <span class="keyword">for</span> user, rates <span class="keyword">in</span> self.rating_data:</span><br><span class="line">            self.P[user] = [random.random() / math.sqrt(self.F)</span><br><span class="line">                            <span class="keyword">for</span> x <span class="keyword">in</span> range(self.F)]</span><br><span class="line">            <span class="keyword">for</span> item, _ <span class="keyword">in</span> rates:</span><br><span class="line">                <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> self.Q:</span><br><span class="line">                    self.Q[item] = [random.random() / math.sqrt(self.F)</span><br><span class="line">                                    <span class="keyword">for</span> x <span class="keyword">in</span> range(self.F)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">            随机梯度下降法训练参数P和Q</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> range(self.max_iter):</span><br><span class="line">            <span class="keyword">for</span> user, rates <span class="keyword">in</span> self.rating_data:</span><br><span class="line">                <span class="keyword">for</span> item, rui <span class="keyword">in</span> rates:</span><br><span class="line">                    hat_rui = self.predict(user, item)</span><br><span class="line">                    err_ui = rui - hat_rui</span><br><span class="line">                    <span class="keyword">for</span> f <span class="keyword">in</span> range(self.F):</span><br><span class="line">                        self.P[user][f] += self.alpha * (err_ui * self.Q[item][f] - self.lmbd * self.P[user][f])</span><br><span class="line">                        self.Q[item][f] += self.alpha * (err_ui * self.P[user][f] - self.lmbd * self.Q[item][f])</span><br><span class="line">            self.alpha *= <span class="number">0.9</span>  <span class="comment"># 每次迭代步长要逐步缩小</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, user, item)</span>:</span></span><br><span class="line">        <span class="string">'''预测用户user对物品item的评分</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> sum(self.P[user][f] * self.Q[item][f] <span class="keyword">for</span> f <span class="keyword">in</span> range(self.F))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="string">'''用户有A B C，物品有a b c d'''</span></span><br><span class="line">    rating_data = list()</span><br><span class="line">    rate_A = [(<span class="string">'a'</span>, <span class="number">1.0</span>), (<span class="string">'b'</span>, <span class="number">1.0</span>)]</span><br><span class="line">    rating_data.append((<span class="string">'A'</span>, rate_A))</span><br><span class="line">    rate_B = [(<span class="string">'b'</span>, <span class="number">1.0</span>), (<span class="string">'c'</span>, <span class="number">1.0</span>)]</span><br><span class="line">    rating_data.append((<span class="string">'B'</span>, rate_B))</span><br><span class="line">    rate_C = [(<span class="string">'c'</span>, <span class="number">1.0</span>), (<span class="string">'d'</span>, <span class="number">1.0</span>)]</span><br><span class="line">    rating_data.append((<span class="string">'C'</span>, rate_C))</span><br><span class="line"></span><br><span class="line">    lfm = LFM(rating_data, <span class="number">2</span>)</span><br><span class="line">    lfm.train()</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>]:</span><br><span class="line">        print(item, lfm.predict(<span class="string">'A'</span>, item))  <span class="comment"># 计算用户A对各个物品的喜好程度</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;LFM隐因子模型原理及源码&lt;/center&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Drop-out</title>
    <link href="http://yoursite.com/2018/08/08/drop/"/>
    <id>http://yoursite.com/2018/08/08/drop/</id>
    <published>2018-08-08T06:40:15.000Z</published>
    <updated>2018-08-08T06:42:34.847Z</updated>
    
    <content type="html"><![CDATA[<p>Drop out 是随机的丢弃一部分的神经元，在本次的训练中不参与计算和更新，drop out可以有效的防止过拟合，具体的原因：</p><p><strong>1.相当于多个神经网络的ensemble</strong></p><p><strong>Drop out 掉不同的神经元就相当于在训练不同的神经网络，整个drop out的过程就相当于对很多个不同的神经网络取平均</strong>，而不同的网络产生不同的过拟合，一些互为反向的过拟合可以互相抵消来削弱整体上的过拟合。</p><p><img src="/2018/08/08/drop/1.png"></p><p><strong>2.减少神经元之间复杂的共适应关系</strong></p><p>因为drop-out的关系导致两个神经元不一定每次都在一个drop-out网络中出现，（<strong>这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况</strong>），迫使网络去学习更加鲁邦的特征，换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的模式（鲁棒性）</p><p><strong>3.Tensorflow - tf.nn.dropout()源码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf_export("nn.dropout")</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(x, keep_prob, noise_shape=None, seed=None, name=None)</span>:</span>  <span class="comment"># pylint: disable=invalid-name</span></span><br><span class="line">  <span class="string">"""Computes dropout.</span></span><br><span class="line"><span class="string">  根据概率keep_prob,将输入根据1/keep_prob进行缩放，否则输出0，缩放的目的是为了保持期望和不变。</span></span><br><span class="line"><span class="string">  By default, each element is kept or dropped independently.  If `noise_shape`</span></span><br><span class="line"><span class="string">  is specified, it must be</span></span><br><span class="line"><span class="string">  [broadcastable](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)</span></span><br><span class="line"><span class="string">  to the shape of `x`, and only dimensions with `noise_shape[i] == shape(x)[i]`</span></span><br><span class="line"><span class="string">  will make independent decisions.  For example, if `shape(x) = [k, l, m, n]`</span></span><br><span class="line"><span class="string">  and `noise_shape = [k, 1, 1, n]`, each batch and channel component will be</span></span><br><span class="line"><span class="string">  kept independently and each row and column will be kept or not kept together.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    x: A floating point tensor.</span></span><br><span class="line"><span class="string">    keep_prob: A scalar `Tensor` with the same type as x. The probability</span></span><br><span class="line"><span class="string">      that each element is kept.</span></span><br><span class="line"><span class="string">    noise_shape: A 1-D `Tensor` of type `int32`, representing the</span></span><br><span class="line"><span class="string">      shape for randomly generated keep/drop flags.</span></span><br><span class="line"><span class="string">    seed: A Python integer. Used to create random seeds. See</span></span><br><span class="line"><span class="string">      @&#123;tf.set_random_seed&#125;</span></span><br><span class="line"><span class="string">      for behavior.</span></span><br><span class="line"><span class="string">    name: A name for this operation (optional).</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A Tensor of the same shape of `x`.</span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: If `keep_prob` is not in `(0, 1]` or if `x` is not a floating</span></span><br><span class="line"><span class="string">      point tensor.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="keyword">with</span> ops.name_scope(name, <span class="string">"dropout"</span>, [x]) <span class="keyword">as</span> name:</span><br><span class="line">    x = ops.convert_to_tensor(x, name=<span class="string">"x"</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> x.dtype.is_floating:</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">"x has to be a floating point tensor since it's going to"</span></span><br><span class="line">                       <span class="string">" be scaled. Got a %s tensor instead."</span> % x.dtype)</span><br><span class="line">    <span class="keyword">if</span> isinstance(keep_prob, numbers.Real) <span class="keyword">and</span> <span class="keyword">not</span> <span class="number">0</span> &lt; keep_prob &lt;= <span class="number">1</span>:</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">"keep_prob must be a scalar tensor or a float in the "</span></span><br><span class="line">                       <span class="string">"range (0, 1], got %g"</span> % keep_prob)</span><br><span class="line">    keep_prob = ops.convert_to_tensor(</span><br><span class="line">        keep_prob, dtype=x.dtype, name=<span class="string">"keep_prob"</span>)</span><br><span class="line">    keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Do nothing if we know keep_prob == 1</span></span><br><span class="line">    <span class="keyword">if</span> tensor_util.constant_value(keep_prob) == <span class="number">1</span>:</span><br><span class="line">      <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    noise_shape = _get_noise_shape(x, noise_shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># uniform [keep_prob, 1.0 + keep_prob)</span></span><br><span class="line">    random_tensor = keep_prob</span><br><span class="line">    random_tensor += random_ops.random_uniform(</span><br><span class="line">        noise_shape, seed=seed, dtype=x.dtype)</span><br><span class="line">    <span class="comment"># 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)</span></span><br><span class="line">    binary_tensor = math_ops.floor(random_tensor)</span><br><span class="line">    ret = math_ops.div(x, keep_prob) * binary_tensor</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> context.executing_eagerly():</span><br><span class="line">      ret.set_shape(x.get_shape())</span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure><p><strong>4.具体使用</strong></p><p>train的时候置为x(0,1], test的时候置为1即可</p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;Dropout如何防止过拟合和源码&lt;/center&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>L1和L2正则化</title>
    <link href="http://yoursite.com/2018/08/07/l1/"/>
    <id>http://yoursite.com/2018/08/07/l1/</id>
    <published>2018-08-07T12:30:52.000Z</published>
    <updated>2018-08-07T12:32:35.358Z</updated>
    
    <content type="html"><![CDATA[<p>loss函数以均方误差为例：</p><p><img src="/2018/08/07/l1/1.png" width="300"></p><p>引入L1正则化(参数绝对值) :</p><p><img src="/2018/08/07/l1/2.png" width="370"></p><p>引入L2正则化(参数平方):</p><p><img src="/2018/08/07/l1/3.png" width="380"></p><p><strong>先下结论：L1正则化可以参数产生稀疏性(更容易为0),而L2正则化可以使参数更接近于0</strong></p><p><strong>一： 几何解释</strong></p><p><img src="/2018/08/07/l1/4.png" width="400"></p><ul><li>如果不加L1和L2正则化的时候，对于线性回归这种目标函数凸函数的话，我们最终的结果就是最里边的紫色的小圈圈等高线上的点。</li><li>当加入L1正则化的时候，$|w_1| + |w_2| = F$ 的图像，也就是一个菱形，代表这些曲线上的点算出来的1范数$|w_1| + |w_2|$都为$F$，我们现在的目标是不仅是原曲线算得值要小（越来越接近中心的紫色圈圈），还要使得这个菱形越小越好（F越小越好）。那么还和原来一样的话，过中心紫色圈圈的那个菱形明显很大，因此我们要取到一个恰好的值。那么如何求值呢.</li></ul><p><img src="/2018/08/07/l1/5.png" width="400"></p><p>理论上菱形与等高线相交的任意一点都可以作为解，但是为了使得经验风险和正则化项的和最小，肯定是取一个能够相交但是和又最小的点作为解，也就是菱形和等高线的相切点，从图中容易发现，相切的点更容易落在坐标轴上，也就是菱形的顶点处。当然这只是根据观察得到的结论，不够严谨，后面会给出简单证明。从图形上看，L2容易产生接近于0的解也是同理。</p><p><strong>二：公式解释</strong></p><p><strong>1.L2为何趋近于0</strong></p><p>给出loss函数：</p><p><img src="/2018/08/07/l1/6.png" width="150"></p><p>为了更新w，求loss对w的导数：</p><p><img src="/2018/08/07/l1/7.png" width="150"></p><p>对b求导数：</p><p><img src="/2018/08/07/l1/8.png" width="110"></p><p>可以发现L2正则化对b没有影响，但是对w的更新有影响：</p><p><img src="/2018/08/07/l1/10.png" width="220"></p><p>如果没有L2正则项，w前面的系数是1，当然由于后面的项，w有可能增大可能减小，但是与原来相比，由于1减去的值为正，因此系数&lt;1，所以L2正则化会整体上让w变的更小，而我们认为w越小的模型越不容易过拟合，因此L2正则化有防止过拟合的能力。</p><p><strong>2.L1为何可以产生稀疏性</strong></p><p>仍然给出loss函数:</p><p>$$C = C_0 + \lambda|w|$$</p><p>为了更新w，求loss对w的导数，绝对值的导数在w&gt;0的时候为1，w&lt;0的时候为-1</p><p>因此($\eta$就是$\lambda$)：</p><p><img src="/2018/08/07/l1/11.png" width="350"></p><p>当$|\eta|&gt; \frac{\vartheta L0}{\vartheta w}$的时候，当$w&gt;0$的时候，导数大于0，loss函数单调递增，当$w&lt;0$时候，导数小于0，loss函数单调递减，因此可以在0的地方取得极小值，也就是w=0，因此当$|\eta|&gt; \frac{\vartheta L0}{\vartheta w}$的时候会产生0解，这也是L1正则化会产生稀疏解的原因。</p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;L1产生稀疏解和L2防止过拟合的原因&lt;/center&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>马尔科夫决策过程</title>
    <link href="http://yoursite.com/2018/07/26/mdp/"/>
    <id>http://yoursite.com/2018/07/26/mdp/</id>
    <published>2018-07-26T08:28:20.000Z</published>
    <updated>2018-07-26T08:41:58.581Z</updated>
    
    <content type="html"><![CDATA[<p>之前了解了马尔科夫过程或者马尔科夫性，总的来说就是<strong>无后效性：系统的下一个状态只与现在有关，与之前没有任何关系。</strong></p><p>下面是普通的马尔科夫模型：</p><p><img src="/2018/07/26/mdp/1.png" width="300"></p><p>MDP在这基础上又引入了动作action和reward的概念：</p><p><img src="/2018/07/26/mdp/2.png" width="300"></p><p><strong>MDP的构成：由五元组构成：</strong></p><p><img src="/2018/07/26/mdp/3.png"></p><p>注意这里的状态转移概率是靠动作来触发的，不同的动作触发不同的状态转移概率，比如下图中在1.1执行上这个动作以后，往上的概率是0.8,向下的概率是0.1。</p><p><img src="/2018/07/26/mdp/4.png" width="500"></p><p><strong>MDP的动态过程如下：</strong></p><p>agent在某个时刻执行动作a，然后根据状态转移概率转移到状态s1,重复这样的过程可以得到过程:<br><img src="/2018/07/26/mdp/5.png" width="300"></p><p>经过上面的路径转移，可以得到对应的回报函数：</p><p><img src="/2018/07/26/mdp/6.png" width="310"></p><p>有时候回报函数只与s有关，上式可以重新写为：</p><p><img src="/2018/07/26/mdp/7.png" width="270"></p><p><strong>最终的目标是选择一组最佳的动作，使得全部的回报加权和期望最大：</strong></p><p><img src="/2018/07/26/mdp/8.png" width="360"></p><p>可以发现每一个时刻的t的回报都是被阻尼系数给打了折扣的，而且时间越长折扣越大，<strong>为了得到最大的期望回报，智能体将会尽量最先拿最大回报。</strong></p><p>下面是MDP动态过程的一个示意图：</p><p><img src="/2018/07/26/mdp/9.png" width="500"></p><hr><p>然后来定义从$s_0$到$a_0$的映射，也就是<strong>“策略”</strong>，一个策略也就是一个从状态到动作的映射函数，一个策略确定了以后，那么状态产生的动作也就确定了。</p><p>对于每一个策略$\pi$，可以将回报表示成数学形式，也就是我们常说的<strong>值函数</strong>：</p><p><img src="/2018/07/26/mdp/10.png" width="400"></p><p><img src="/2018/07/26/mdp/15.png" width="300"></p><p>即给定初始状态$s_0$和策略$\pi$后的<strong>累积折扣回报期望（Expected Sum Of Discounted Rewards）</strong></p><p>对于一个<strong>确定的策略</strong>，它的值函数满足<strong>贝尔曼等式(贝尔曼方程)</strong>：</p><p><img src="/2018/07/26/mdp/11.png" width="280"></p><p><img src="/2018/07/26/mdp/12.png" width="700"></p><p>所以，对任意一个确定的策略，都可以解出其值函数。</p><p>值函数是<strong>给出一个当前状态和策略，然后计算出期望的收益值来。</strong>那我们利用值函数的目的是为了固定当前状态S以后，选出一个最优的策略$\pi$来使得值函数的值最大，也就是期望的收益最大。</p><p>值函数是指定当前状态和策略能计算出值来用于选择最优的策略，也就是约束条件只有指定当前状态，那如果想<strong>把action也考虑在内的话，就指定当前状态和当前要执行的动作计算期望收益</strong>，也就是：</p><p><img src="/2018/07/26/mdp/14.png" width="450"></p><p>同样的，也是为了在<strong>当前状态s</strong>和<strong>当前a的约束下</strong>，找到<strong>收益最大的策略$\pi$</strong>。</p><h3 id="价值函数和动作价值函数的关系"><a href="#价值函数和动作价值函数的关系" class="headerlink" title="价值函数和动作价值函数的关系"></a>价值函数和动作价值函数的关系</h3><p><img src="/2018/07/26/mdp/17.png" width="350"></p><h3 id="定理"><a href="#定理" class="headerlink" title="定理"></a>定理</h3><p>1.对于<strong>任意一个MDP，存在一个最优策略</strong>$\pi_<em>$，使得对于所有的$\pi$, $\pi_</em> ≥ \pi$</p><p>2.<strong>最优策略对应的价值函数就是最优价值函数</strong></p><p>3<strong>.最优策略对应的动作函数(Q函数)就是最优动作函数</strong></p><p>4.$\pi_<em>$有一个有趣的特性，**即$\pi_</em>$是针对所有状态s的，*<em>确定了每一个状态s和下一个动作a，每一个状态通过$\pi_</em>$都可以取得最大的回报</p><h3 id="最优策略求解"><a href="#最优策略求解" class="headerlink" title="最优策略求解"></a>最优策略求解</h3><p>最优策略求解主要有两种：<strong>1.策略迭代 2.值迭代</strong></p><h4 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h4><p>策略迭代主要由两部分组成：</p><p><strong>1.策略评估 Policy Evaluation : 更新</strong> $v_\pi$，也就是根据当前的策略去计算每个状态的收益：</p><p><img src="/2018/07/26/mdp/20.png" width="400"></p><p><strong>初始状态是不产生收益的，收益产生在根据一个动作a从状态s转移到s’的时候</strong></p><p>每一次迭代对每个状态s计算其v(s),知道某一轮迭代的所有状态s的变化量绝对值不超过阈值则认为收敛</p><p><strong>2.策略改进 Policy improvment</strong></p><p>上面的v(s)在计算的时候是包含了全部的action的，<strong>策略改进就是对每个状态只保留带来收益最高的那个(或多个，有可能相同)的action。</strong></p><p><img src="/2018/07/26/mdp/21.png" width="400"></p><p><strong>然后再回去对每个状态计算v(s):</strong><br><img src="/2018/07/26/mdp/22.png" width="300"></p><p><strong>所以其实求最优策略的过程就是不断的重复上述两个过程直到收敛：</strong></p><h4 id="值迭代"><a href="#值迭代" class="headerlink" title="值迭代"></a>值迭代</h4><p>值迭代主要是重复一个过程：</p><p>首先初始化，<strong>然后迭代以下过程：</strong></p><p>1.对于每一个状态s，计算每个action带来的期望收益v(s,a)，然后选收益最大的那个v(s,a)其实也就是Q函数-动作函数的值来作为当前的v(s)值，然后继续迭代。</p><p><strong>相当于跟策略迭代一样，也去算每一个动作函数的值，但是值迭代是直接把最大值作为当前v(s)来迭代，而策略迭代只是根据动作函数的值来迭代策略，这也很清楚的阐释了为什么叫策略迭代和值迭代的原因</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;马尔科夫决策过程，策略迭代，值迭代&lt;/center&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>隐马尔科夫模型和马尔科夫链</title>
    <link href="http://yoursite.com/2018/07/25/markov/"/>
    <id>http://yoursite.com/2018/07/25/markov/</id>
    <published>2018-07-25T06:59:00.000Z</published>
    <updated>2018-07-26T08:47:09.480Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概率图模型"><a href="#概率图模型" class="headerlink" title="概率图模型"></a>概率图模型</h2><p>概率图模型是是一类用图来表达变量相关关系的概率模型，它以图为表示工具，最常见的是用一个结点表示一个或一组随机变量，结点之间的边表示变量之间的概率相关关系，即“变量关系图”。</p><p>根据<strong>边的性质不同</strong>，概率图模型可以大致分为两类：第一类是使用<strong>有向无环图</strong>表示变量之间的依赖关系，称为有<strong>向图模型或贝叶斯网</strong>；第二类是使用<strong>无向图</strong>表示变量之间的相关关系，称为<strong>无向图模型或者马尔科夫网</strong>。</p><h2 id="隐马尔科夫模型"><a href="#隐马尔科夫模型" class="headerlink" title="隐马尔科夫模型"></a>隐马尔科夫模型</h2><p>隐马尔科夫模型(Hidden Markov Model,简称HMM)是结构最简单的动态贝叶斯网(dynamic Bayesian network)，这是一种著名的有向图模型，<strong>主要用于时序建模</strong>，在语音识别、自然语音处理等领域等有广泛应用。</p><p>隐马尔科夫模型由状态变量yi(通常认为是不可见的)以及观测变量xi构成。</p><p><img src="/2018/07/25/markov/1.png" width="300"></p><p><strong>在任一时刻，观测变量的取值仅仅依赖于状态变量，即xt由yt确定</strong>，与其他状态变量及观测变量的取值无关。同时，<strong>t时刻的状态yt仅仅依赖于t-1时刻的状态yt-1</strong>，与其余的n-2个以前的状态无关，这就是所谓的“马尔科夫链(Markov chian)”,基于这种依赖关系，所有变量的联合概率分布函数为：</p><p><img src="/2018/07/25/markov/2.png" width="450"></p><p>确定一个马尔科夫模型<strong>需要如下三组参数：</strong></p><p><strong>1.状态转移概率：</strong>模型在各个状态之间转换的概率，通常记为矩阵$A=[a_{ij}]_{NxN}$，其中:<br><img src="/2018/07/25/markov/3.png" width="350"></p><p>表示在任意时刻$t$,如果状态为$s_i$,则在下一个状态为$s_j$的概率</p><p><strong>2.输出观测概率：</strong>模型根据当前状态获得各个观测值的概率，通常记为矩阵$B=[b_{ij}]_{NxM}$，其中</p><p><img src="/2018/07/25/markov/4.png" width="400"></p><p>表示在任意时刻t，如果状态为$s_i$,则观测值$o_j$被获取的概率</p><p><strong>3.初始状态概率</strong>：模型在初始时刻各状态出现的概率，通常记为$\pi=(\pi_1,\pi_2,\pi_3….\pi_N)$，其中</p><p><img src="/2018/07/25/markov/5.png" width="250"></p><p>表示模型的初始状态为$s_i$的概率。</p><hr><p>通过指定状态空间，观测空间，和上述三组参数，就能确定一个隐马尔科夫模型，通常用其参数$\lambda = [A,B,\pi]$来指代，<strong>给定隐马尔科夫模型$\lambda$,它按如下过程产生观测序列${x_1,x_2,x_3…,x_n}$:</strong></p><p><img src="/2018/07/25/markov/6.png" width="450"></p><p>在实际中，<strong>应用隐马尔科夫模型主要来解决三个问题：</strong></p><p><img src="/2018/07/25/markov/7.png" width="550"></p><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p>假设我手里有<strong>三个不同的骰子:</strong></p><ul><li>D6: (1,2,3,4,5,6)</li><li>D4: (1,2,3,4)</li><li>D8: (1,2,3,4,5,6,7,8)</li></ul><p>对于每个骰子的每个数出现的概率是相等的，<strong>分别是1/6,1/4和1/8</strong></p><p><img src="/2018/07/25/markov/8.png" width="400"></p><p>假设我们掷骰子的时候，<strong>随机选到这三个骰子中的每一个概率相等</strong>，那么每次都会得到一个数字，那么投10词就会得到一个<strong>“观测序列”:1 6 3 5 2 7 3 5 2 4</strong>, 而状态序列一般是我们不知道的(就是我们每次选用的骰子组成的序列)。</p><p>一般来说，<strong>HMM中说到的马尔可夫链</strong>其实是指<strong>隐含状态链</strong>，因为<strong>隐含状态（骰子）之间存在转换概率</strong>（transition probability),也就是我们在上面提到过的$A=[a_{ij}]<em>{NxN}$，不过在这里整个矩阵的所有值都是1/3。同样的，每个骰子都有一个<strong>输出概率</strong>，因为我们的骰子是均匀分布的，所以$B=[b</em>{ij}]_{NxM}$中比如D8那一行就是8个1/8。</p><hr><p>其实对于HMM来说，<strong>如果提前知道所有隐含状态之间的转换概率和所有隐含状态到所有可见状态之间的输出概率，做模拟是相当容易的</strong>。但是应用HMM模型时候呢，<strong>往往是缺失了一部分信息的</strong>，有时候你知道骰子有几种，每种骰子是什么，但是不知道掷出来的骰子序列；有时候你只是看到了很多次掷骰子的结果，剩下的什么都不知道。如果应用算法去估计这些缺失的信息，就成了一个很重要的问题，也就是上面提到过的三个基本问题，在这里<strong>用掷骰子的问题再具象一下：</strong></p><p><strong>(1).知道骰子有几种（隐含状态数量），每种骰子是什么（转换概率），根据掷骰子掷出的结果（可见状态链），我想知道每次掷出来的都是哪种骰子（隐含状态链）。</strong></p><blockquote><p>也就是基本问题中的求隐藏状态</p></blockquote><p>这个问题呢，在语音识别领域呢，叫做解码问题。这个问题其实有两种解法，会给出两个不同的答案。<strong>每个答案都对，只不过这些答案的意义不一样</strong>。<strong>第一种解法求最大似然状态路径</strong>，说通俗点呢，就是我求一串骰子序列，这串骰子序列产生观测结果的概率最大。第二种解法呢，就不是求一组骰子序列了，而是<strong>求每次掷出的骰子分别是某种骰子的概率</strong>。比如说我看到结果后，我可以求得第一次掷骰子是D4的概率是0.5，D6的概率是0.3，D8的概率是0.2.第一种解法我会在下面说到，但是第二种解法我就不写在这里了，如果大家有兴趣，我们另开一个问题继续写吧。</p><p><strong>(2).还是知道骰子有几种（隐含状态数量），每种骰子是什么（转换概率），根据掷骰子掷出的结果（可见状态链），我想知道掷出这个结果的概率。</strong></p><blockquote><p> 也就是基本问题中的模型与观测序列之间的匹配程度</p></blockquote><p>看似这个问题意义不大，因为你掷出来的结果很多时候都对应了一个比较大的概率。问这个问题的目的呢，其实是检测观察到的结果和已知的模型是否吻合。如果很多次结果都对应了比较小的概率，那么就说明我们已知的模型很有可能是错的，有人偷偷把我们的骰子給换了。</p><p><strong>(3).知道骰子有几种（隐含状态数量），不知道每种骰子是什么（转换概率），观测到很多次掷骰子的结果（可见状态链），我想反推出每种骰子是什么（转换概率）。</strong> </p><blockquote><p>也就是基本问题中的如何训练模型使其能最好的描述观测数据</p></blockquote><p> 这个问题很重要，因为这是最常见的情况。很多时候我们只有可见结果，不知道HMM模型里的参数，我们需要从可见结果估计出这些参数，这是建模的一个必要步骤。</p><hr><p><strong>第一种问题：反推隐含状态链</strong></p><p> 举例来说，我知道我有三个骰子，六面骰，四面骰，八面骰。我也知道我掷了十次的结果（1 6 3 5 2 7 3 5 2 4），我不知道每次用了那种骰子，我想知道最有可能的骰子序列。其实最简单而暴力的方法就是<strong>穷举所有可能的骰子序列</strong>，然后依照第零个问题的解法把每个序列对应的概率算出来。然后<strong>我们从里面把对应最大概率的序列挑出来就行了</strong>。如果马尔可夫链不长，当然可行。<strong>如果长的话，穷举的数量太大，就很难完成了。</strong></p><p>根据状态序列计算观测序列的公式也很简单，例如：</p><p><img src="/2018/07/25/markov/9.png" width="200"></p><p><strong>就是概率相乘(上面提到的联合概率分布)：</strong></p><p>$$P = P(D6) <em> P(1|D6) </em> P(D8|D6) <em> P(6|D8) </em> P(D8|D8) <em> P(3|D8) = \frac{1}{3} </em>  \frac{1}{6} <em> \frac{1}{3} </em> \frac{1}{8} <em> \frac{1}{3} </em> \frac{1}{8}$$</p><p>既然穷举法不适用的话，有一种比较著名的算法<strong>Viterbi algorithm(维特比算法)</strong></p><p><strong>下面解决三个基本问题：</strong></p><p><strong>1.概率计算问题 2.参数学习问题 3.预测问题</strong> </p><h2 id="概率计算算法"><a href="#概率计算算法" class="headerlink" title="概率计算算法"></a>概率计算算法</h2><p><strong>主要分为：</strong></p><ul><li>直接计算法</li><li>前向算法</li><li>后向算法</li><li>一些概率与期望值的计算</li></ul><h3 id="直接计算法"><a href="#直接计算法" class="headerlink" title="直接计算法"></a>直接计算法</h3><p>计算观测序列的最直观的方法就是直接计算法，也就是根据所有能产生观测序列的隐藏序列计算联合分布的累加和。</p><p><img src="/2018/07/25/markov/13.png" width="450"></p><p>但是这种穷举法的时间复杂度非常高$O(TN^T)$，肯定是不适用的。</p><h3 id="前向算法"><a href="#前向算法" class="headerlink" title="前向算法"></a>前向算法</h3><p>首先定义前向概率：</p><p><img src="/2018/07/25/markov/14.png" width="600"></p><p>那么就可以递推的求得前向概率及观测序列概率$P(O|入)$</p><p><img src="/2018/07/25/markov/15.png" width="550"></p><p><strong>具体解释：</strong></p><p><strong>(1)</strong>先是<strong>对每个隐藏对象求初值</strong>(比如上面举得例子，就是D4,D6,D8)，比如观测到的$o_1$是3，那么(1)就是这样计算的：</p><p>$\alpha_1(1) = \frac{1}{3} * \frac{1}{4} (D4)​$</p><p>$\alpha_1(2) = \frac{1}{3} * \frac{1}{6} (D6)$</p><p>$\alpha_1(3) = \frac{1}{3} * \frac{1}{8} (D8)$</p><p><strong>(2)</strong>其实(2)表达的跟(1)是一个意思，(1)可以看成(2)的一种特殊形式，因为<strong>初始的隐藏状态是由初始概率$\pi$决定的而且不存在从其他状态转移到当前状态</strong>，而后面就是由<strong>状态转移矩阵</strong>来决定的，因此对于每一个$\alpha_2(i)$其函数左侧部分是由各个$\alpha_1(i)$乘上其到当前$\alpha$的状态转移矩阵的值的累加和，右侧仍然是一个普通的观测值输出概率。</p><p><strong>(3)</strong> 当对于所有的i计算完$\alpha_T(i)$以后，相当于已经计算完所有可能的情况了，而且所有可能情况的概率都在前向算法中累加了，因此<strong>最后计算的这i个(比如我们的例子中是3个)隐藏状态结尾的概率累加就是所有可能产生观测序列的隐藏序列的概率累加和</strong>。</p><p>前向算法<strong>高效的关键</strong>就是其<strong>局部计算前向概率</strong>， 然后<strong>利用路径结构将前向概率“递推”到全局</strong>，得到$P(O|入)$ ，<strong>减少计算量的原因在于每一次计算直接引用前一个时刻的计算结果，避免重复计算</strong>。</p><h3 id="后向算法"><a href="#后向算法" class="headerlink" title="后向算法"></a>后向算法</h3><p>后向算法其实和前向算法类似，都是一个递推，通过下图可以比较前向和后向算法的主要区别：</p><p><img src="/2018/07/25/markov/16.png" width="250"><img src="/2018/07/25/markov/17.png" width="220"></p><p>前向算法的t时刻的$\alpha_i$是根据t-1时刻的$\alpha$累加得到的，后向算法的t时刻的$\beta_i$是根据t+1时刻的$\beta$累加得到的。</p><p><strong>后向算法定义：</strong><br><img src="/2018/07/25/markov/18.png" width="600"></p><p><strong>注意，当前时刻为t的时候，$\beta_t(i)$得到是t+1时刻到T的观测序列概率！</strong>根据下面的递推公式也能比较直观的反应出来：</p><p><img src="/2018/07/25/markov/19.png" width="550"></p><p>可以看到(2)式中<strong>最右边的项是递推项</strong>，而<strong>左边两项计算的就是t+1时刻每个可能的隐藏状态输出值为观测值$o_{t+1}$的概率</strong>。</p><h2 id="参数学习算法"><a href="#参数学习算法" class="headerlink" title="参数学习算法"></a>参数学习算法</h2><h3 id="隐藏状态转移参数和观测值生成参数的监督学习算法"><a href="#隐藏状态转移参数和观测值生成参数的监督学习算法" class="headerlink" title="隐藏状态转移参数和观测值生成参数的监督学习算法"></a>隐藏状态转移参数和观测值生成参数的监督学习算法</h3><p>假设给定包含S个长度相同的观测序列和对应的状态序列，那么可以用极大似然估计来估计隐马尔科夫模型的参数</p><p><strong>1.对于模型转移概率的估计</strong></p><p><img src="/2018/07/25/markov/20.png" width="550"></p><p>比如要计算从1转移到2的概率，就是统计<strong>（1转移到2的频次/1转移到所有其他状态的频次）</strong></p><p><strong>2.对于观测值生成概率的估计</strong></p><p><img src="/2018/07/25/markov/21.png" width="360"></p><p>跟1差不多的，比如从状态1生成0.13的概率，就是统计<strong>（1状态生成0.13的频次/1状态生成所有值的频次）</strong></p><p><strong>3.对于初始状态$\pi$的估计</strong></p><p>也就是初始状态的频次统计占总的初始状态的频次</p><h3 id="Baum-Welch算法-也就是EM算法"><a href="#Baum-Welch算法-也就是EM算法" class="headerlink" title="Baum-Welch算法(也就是EM算法)"></a>Baum-Welch算法(也就是EM算法)</h3><p>由于一般得到的训练集并没有被标注(没有对应的状态序列)，这个时候来学习对应的转移和生成参数就不能使用监督学习算法了$\lambda={A,B,\pi}$  </p><p>(推导过程略)</p><p><img src="/2018/07/25/markov/22.png" width="550"></p><p><img src="/2018/07/25/markov/23.png" width="550"></p><p><img src="/2018/07/25/markov/24.png" width="400"></p><p><img src="/2018/07/25/markov/25.png" width="420"></p><p>其中<strong>10.24是</strong>给定观测$O$,在<strong>时刻t处于状态$q_i$的概率</strong>。而<strong>10.26是</strong>给定观测O，<strong>在t时刻处于状态$q_i$而在$t+1$时刻处于状态$q_j$的概率</strong>。</p><p>所以其实对于$a_{ij}^{n+1}$而言，如果把分母拿到左边，其实就是:</p><p>(<strong>所有时刻处于状态$q_i$概率的累加</strong>) x (<strong>要估计的状态转换系数</strong>)=(<strong>所有时刻当前状态是<em>$q_i$</em>而下一时刻是$q_j$的概率累加和</strong>)。</p><p>对于$b_j(k)^{n+1}$而言，就是：</p><p>(<strong>所有时刻处于状态$q_i$概率的累加</strong>) x <strong>(要估计的观测生成参数)</strong> = (<strong>所有时刻当前状态是<em>$q_i$</em>而生成的观测为$o_t=v_t$的累加和)</strong></p><p>所以，弄懂式子24和26的含义，就弄懂了隐马尔科夫模型参数估计的含义。</p><h2 id="预测算法"><a href="#预测算法" class="headerlink" title="预测算法"></a>预测算法</h2><p>预测算法的目的是<strong>根据已知的观测序列来预测状态序列</strong>，预测算法主要有</p><ul><li>近似算法</li><li>维特比算法</li></ul><h3 id="近似算法"><a href="#近似算法" class="headerlink" title="近似算法"></a>近似算法</h3><p><strong>近似算法的主要思想是：</strong></p><p>在每个时刻t选择在该时刻最有可能出现的状态i,从而得到一个状态序列，也就是上面提到的式子10.24,给定观测$O_i$,当前状态是$q_i$的概率是：</p><p><img src="/2018/07/25/markov/24.png" width="430"></p><p>然后都算一遍取最大就可以了，举个简单的例子，假设掷骰子第三次投出来的是4，那么对于D4,D6,D8而言，投出4的概率最大的状态是D4,那么就预测当前的状态为D4.</p><p><strong>近似算法最明显的缺点就是：</strong></p><p><strong>不能保证预测的状态序列整体是最有可能的状态序列</strong>，因为这种方法<strong>仅仅考虑了观测值生成概率而没有概率状态转移概率</strong>，比如根据这种方法预测的第二位是D4，第三位是D6，而完全有可能D4到D6的转移概率为0，也就是<strong>预测出来的状态序列根本不会发生。</strong></p><h3 id="维特比算法-Viterbi-algorithm"><a href="#维特比算法-Viterbi-algorithm" class="headerlink" title="维特比算法 Viterbi algorithm"></a>维特比算法 Viterbi algorithm</h3><p>维特比算法是<strong>动态规划</strong>解马尔科夫预测问题，即用动态规划<strong>求概率最大路径</strong>。</p><p><strong>维特比算法基于这样一种思想：</strong></p><p>从后往前推，如果存在一条最优路径，那么它从节点$i_t$到终点$i_T$的路径一定是最优的，不然就存在另外的从$i_t$到$i_t$的更优路径，与事实不符，所以就可以从终点往前回溯得到最终的最优路径。</p><p>这里定义了两个变量$\delta$和$\psi$,$\delta$定义了在时刻t状态为i的所有单个路径中最大概率为：</p><p><img src="/2018/07/25/markov/26.png" width="450"></p><p><img src="/2018/07/25/markov/27.png" width="600"></p><p><strong>看一个例子更容易理解：</strong></p><p><img src="/2018/07/25/markov/29.png" width="600"></p><p><img src="/2018/07/25/markov/30.png" width="600"></p><p>其实看上班这个图10.4就可以理解了，在t=1的时候，按照$\pi <em> b$初始化，在t=2的时候，对于每一个i，都有t=1的时候的三种状态通过状态转移矩阵$A$转移过来再乘上对应的$b$可以得到一个概率，比如t=2的状态为3的时候算最大概率:<br>对于t=1的时候状态3来说，3-&gt;3 : {0.5 </em> 0.28} * 0.3 = 0.042</p><p>对于t=1的时候状态2来说，2-&gt;3 : {0.2 <em> 0.16} </em> 0.3 = 0.0096</p><p>对于t=1的时候状态1来说，1-&gt;3 : {0.3 <em> 0.1} </em> 0.3 = 0.009</p><p>因此t=2,状态为3的最大的概率是从t=1的时候的状态3来的，i=2的也是从3过来的……..如此从前往后建立路径。</p><p><strong>最终t=3时刻的最大概率为0.0147，因此向后回溯，状态序列为3,3,3</strong></p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p>1.《机器学习》 - 周志华</p><p>2.<a href="https://www.cnblogs.com/skyme/p/4651331.html" target="_blank" rel="noopener">一文搞懂HMM（隐马尔可夫模型）</a></p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;隐马尔科夫模型，三个基本问题，前向和后向算法，维特比算法等&lt;/center&gt;&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>xDeepFM:Combining Explicit and Implicit Feature Interactions for Recommender Systems</title>
    <link href="http://yoursite.com/2018/07/18/xDeepFM/"/>
    <id>http://yoursite.com/2018/07/18/xDeepFM/</id>
    <published>2018-07-18T04:16:35.000Z</published>
    <updated>2018-07-18T04:21:02.574Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h1><p>1.现在诸如DeepFM和Deep&amp;Wide等模型都可以自动学习隐式的高维交互特征，并结合了低维特征，<strong>但是有一个缺点就是它们的高维特征都是在bite-wise的层面上进行交互的</strong>。本片论文提出了一种<strong>压缩交互网络(Compressed Interaction Network(CIN))</strong>，能够学习显式的交互特征并且是在vector-wise的级别，CIN带有一些CNN和RNN的特点，最终作者将整个模型命名为”eXtreme Deep Factorization Machine(xDeepFM)”。</p><p>2.本文提出的模型有两个优点：</p><ul><li>能够显式的学习有明确边界的高维交互特征</li><li>能够学习隐式的低维和高维特征</li></ul><p>个人理解这里作者对implicit和explicit的理解是交互特征的维度是否明确，在这里翻译为隐式和显式。</p><h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>1.简单介绍了单值离散特征和多值离散特征，然后介绍三个<strong>手动提取交互特征的缺点：</strong></p><ul><li>挖掘出高质量的交互特征需要非常专业的领域知识并且需要做大量的尝试，很耗时间。</li><li>在大型的推荐系统中，原生特征是海量的，手动挖掘交叉特征几乎不可能。</li><li>挖掘不出肉眼不可见的交叉特征</li></ul><p>2.然后介绍了<strong>经典的FM模型</strong>，用提取隐向量然后做内积的形式来提取交叉特征，扩展的FM模型可以提取随机的高维特征，但是<strong>主要的缺陷是：</strong></p><ul><li>会学习所有的交叉特征，其中肯定会包含无用的交叉组合，另外一篇论文指出引入无用的交叉特征会引入噪音并降低模型的表现。</li></ul><p>3.介绍了引入了DNN的组合模型，”Factorisation-machine supported Neural Network <strong>(FNN)</strong>“，它在DNN之前使用了预训练的field embedding。</p><p>4.介绍了<strong>PNN</strong>(Product-based Neural Network),在embedding layer和DNN Input之间插入了一层product layer,不依赖于pre-trained FM。</p><p>5.FNN和PNN的缺点都是忽略了低维交互特征，<strong>Wide&amp;Deep和DeepFM</strong>模型通过混合架构解决了这种问题，但是它们同样存在缺点：</p><ul><li>它们学习到的高维特征是一种implicit fasion,没有一种公式可以明确推论出最终学习出来的交叉特征到底是多少维的</li><li>另一方面，其DNN部分是在bit-wise的层面下进行学习的，而经典的FM架构是在vetor-wise层面学习的</li></ul><p>6.本文提出的方法基于<strong>DCN(Deep &amp; Cross Network)</strong>模型，其目标是有效率的捕捉到<strong>边界明确的交叉特征</strong>。</p><h1 id="PRELEMINARIES"><a href="#PRELEMINARIES" class="headerlink" title="PRELEMINARIES"></a>PRELEMINARIES</h1><h2 id="Embedding-Layer"><a href="#Embedding-Layer" class="headerlink" title="Embedding Layer"></a>Embedding Layer</h2><p>介绍一些基于”univalent”,”multivalent”进行embedding的基础知识，这里不介绍了:</p><p><img src="/2018/07/18/xDeepFM/1.png" width="400"></p><h2 id="Implicit-High-order-Interactions"><a href="#Implicit-High-order-Interactions" class="headerlink" title="Implicit High-order Interactions"></a>Implicit High-order Interactions</h2><p>前向传播过程：</p><p><img src="/2018/07/18/xDeepFM/2.png" width="300"></p><p>这种架构是bit-wise层面的，意思是说，<strong>即使是同一个filed embedding，不同的element之间也会互相影响。</strong></p><p>PNN和DeepFM基于上面的缺点进行了改进，除了DNN component,<strong>还添加了two-way interation layer到架构中，这样就既有vector-wise也有bit-wise的component了</strong>。PNN和DeepFM的区别就是DeepFM是把product layer直接作为结果连到输出层，<strong>而PNN是把product layer放在DNN和embedding layer之间</strong></p><p><img src="/2018/07/18/xDeepFM/3.png" width="500"></p><h2 id="Explicit-High-order-Interactions"><a href="#Explicit-High-order-Interactions" class="headerlink" title="Explicit High-order Interactions"></a>Explicit High-order Interactions</h2><p>这里主要介绍了Cross Network(cross net)也是本文主要借鉴的一种模型，下面是该模型的架构：</p><p><img src="/2018/07/18/xDeepFM/4.png" width="450"></p><p>该模型的主要目标是显示的构建高维交互特征，不像DNN前向传播的全连接层那样，每个隐藏层是通过如下公式计算出来的：</p><p><img src="/2018/07/18/xDeepFM/6.png" width="240"></p><p>通过推导可以看出其实<strong>每一个隐含层都是x0的一个scalar multiple,</strong>这当然不是代表隐含层是x0的线性表达，只是说因为每一层原生x0都会参与计算，因此对x0非常敏感。<strong>但是其缺点为：</strong></p><ul><li>crossnet的输出是一种特殊形式，即x0的scalar multiple</li><li>交互特征仍然是bit-wise层面的</li></ul><h1 id="OUR-PROPSED-MODEL"><a href="#OUR-PROPSED-MODEL" class="headerlink" title="OUR PROPSED MODEL"></a>OUR PROPSED MODEL</h1><h2 id="Compressed-Interation-Network"><a href="#Compressed-Interation-Network" class="headerlink" title="Compressed Interation Network"></a>Compressed Interation Network</h2><p>本论文设计了一种新的cross network, 称为<strong>Compressed Interaction Network (CIN)</strong>, 设计的时候主要<strong>考虑了下面三个方面：</strong></p><ul><li>交互特征是在vector-wise层面的(主要基于crossnet改进了这点)</li><li>高维交互特征是显式的</li><li>网络的复杂度不会因为交互层级的增加而增加</li></ul><p><strong>下面介绍了一些在CIN的中的概念：</strong></p><p>既然在CIN中是vector-wise层级的，那么每一个unit是一个vector，因此field embedding的输出是一个mxD的矩阵(D:embedding size,m:filed size)，CIN的第k层是一个Hk x D的矩阵(Hk代表的是CIN中每一层的向量数量，H0=m),<strong>下面是第CIN第k层的h-emb的计算公式：</strong></p><p><img src="/2018/07/18/xDeepFM/7.png" width="240"></p><p>还是比较直观的，其中○代表Hadamard product:</p><p><img src="/2018/07/18/xDeepFM/8.png" width="260"></p><p><strong>可以发现k-th layer的计算也是和crossnet一样依赖于(k-1)-th layer和 0-th layer,因此交互特征是显式的，而且交互的层级随着网络结构的加深而增加(在这点上和crossnet是一样的)，同时通过公式也可以很明显的看出，模型是vector-wise的:</strong></p><p><img src="/2018/07/18/xDeepFM/9.png" width="130"></p><p>如果公式不好理解的话，可以通过如下图示来理解：</p><p><img src="/2018/07/18/xDeepFM/10.png" width="800"></p><p>图(a)和(b)表示了如何从这一层的隐藏层（Hk x D）和X^0层（m X D）来产生下一层隐藏层的（Hk+1 x D）,图示所示计算方法是为了更好的展现为什么模型有CNN的思想，先通过X0和Xk的第i列做一个outer product(matrix multiplication)得到一个Hk x m的矩阵(0&lt;=i&lt;D), 然后W就像是CNN中的filter,来过滤产生每个feature map的第i列，这样CNN中的”compressed”在CIN中就指代 Hk x D矩阵压缩为Hk+1 x D矩阵。</p><p>需要注意的是，<strong>CIN的输出是除了X0以外每一层的feature map的sum pooling横向拼接的结果。</strong>然后根据所需要进行的任务套一个激活函数就行了，比如sigmoid:</p><p><img src="/2018/07/18/xDeepFM/11.png" width="180"></p><h2 id="CIN-Analysis"><a href="#CIN-Analysis" class="headerlink" title="CIN Analysis"></a>CIN Analysis</h2><p>本文从空间复杂度，时间复杂度和多项式逼近等方面进行了分析，这里只介绍参数：</p><p>从公式也可以看出:</p><p><img src="/2018/07/18/xDeepFM/7.png" width="240"></p><p>计算第k-th layer的第h(0&lt;h&lt;=Hk)个emb需要(Hk-1 x m)个参数，而k-th layer共有Hk个emb vetor，因此<strong>计算k-th layer在第k-1 th需要(Hk x Hk-1 x m)个参数</strong> 。</p><h2 id="Combination-with-Implicit-Networks"><a href="#Combination-with-Implicit-Networks" class="headerlink" title="Combination with Implicit Networks"></a>Combination with Implicit Networks</h2><p>由于DeepFM可以对低维交叉特征和隐式的高维交叉特征有一个比较好的支持了，因此直接将CIN加入到DeepFM中以补足其缺少有明确边界的vetor-wise层级的部分，最终的公式如下：</p><p><img src="/2018/07/18/xDeepFM/12.png" width="320"></p><p>最终的模型结构如下：</p><p><img src="/2018/07/18/xDeepFM/13.png" width="550"></p><h1 id="EXPRIMENTS"><a href="#EXPRIMENTS" class="headerlink" title="EXPRIMENTS"></a>EXPRIMENTS</h1><p>这部分就不说了，反正就是好多实验，自己的模型就是吊吊吊，有用的信息是：</p><ul><li>对于这种高维稀疏特征来说，基于FM思想的模型例如DeepFM,Deep&amp;Wide,PNN等比LR不知道高到哪里去了</li><li>并不是混合模型就一定好，但是单用DNN component一般效果比较差</li><li>这种用于高维稀疏特征的混合模型一般在比较浅层的比如2-3层的网络结构下会取得最好的效果</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;xDeep论文精度&lt;/center&gt;
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>DNN for Youtube Recommendation</title>
    <link href="http://yoursite.com/2018/07/10/youtube/"/>
    <id>http://yoursite.com/2018/07/10/youtube/</id>
    <published>2018-07-10T03:54:39.000Z</published>
    <updated>2018-07-10T03:57:01.579Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><strong>本文主要介绍两种模型：</strong></p><ul><li>深度候选集生成模型 “Deep candidate generation model”</li><li>深度排序模型</li></ul><h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><ul><li>推荐系统</li><li>深度学习</li><li>海量数据</li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>Youtube是目前世界上最大的视频网站，其服务的用户超10亿，由于Youtube的推荐系统主要面临着以下三个挑战：</p><ul><li>规模：Youtube每秒钟就有总长几万小时的视频上传，那些在小数据上表现好的算法放在这里效果很差</li><li>新鲜度：传统的E&amp;E问题</li><li>噪音：用户历史行为稀疏不完整，没有非常明确的满意度反馈，视频信息非结构化</li></ul><h2 id="系统总览"><a href="#系统总览" class="headerlink" title="系统总览"></a>系统总览</h2><p>系统总体架构如下：</p><p><img src="/2018/07/10/youtube/1.png" width="500"></p><p>首先是召回模型根据简单的用户历史和上下文信息从几百万的视频库中召回几百个候选视频，然后排序模型基于大量的特征对每个视频进行打分，将最终排序的结果呈现给用户，最终效果通过线上A/B test来验证。</p><h2 id="CANDIDATE-GENERATION"><a href="#CANDIDATE-GENERATION" class="headerlink" title="CANDIDATE GENERATION"></a>CANDIDATE GENERATION</h2><p>在深度候选集召回模型中，需要从海量的视频中进行召回，以前用的非常多的方法是矩阵分解，本文的方法其实是一种非线性的矩阵分解(其实很好理解，一会看到模型结构你们就懂了)</p><h3 id="Recommendation-as-Classfication"><a href="#Recommendation-as-Classfication" class="headerlink" title="Recommendation as Classfication"></a>Recommendation as Classfication</h3><p>这里是将召回问题转化为了一个多分类问题，因为类别如此之大，可以说是极限多分类了，最终的score由如下公式输出:</p><p><img src="/2018/07/10/youtube/2.png" width="250"></p><p>就是一个很正经的softmax而已，这里需要注意的是<strong>u即我们要学习的user向量，v代表的是video向量</strong>。</p><h3 id="Efficient-Extreme-Multiclass"><a href="#Efficient-Extreme-Multiclass" class="headerlink" title="Efficient Extreme Multiclass"></a>Efficient Extreme Multiclass</h3><p>类别如此之多的多分类模型，肯定是需要采取一些trick来避免庞大的计算量的。其中最主要的是“<strong>Sampled softmax</strong>”,感兴趣的可以自己去研究一下(其实这些sampled方法差不多，都是用极大似然的方法，通过一个人工设定的分布Q，来近似的表达真实的大数据集分布来避免全量计算)。</p><p>在线上阶段的时候需要在几十毫秒内计算出需要找回的TopN，因此问题可以简化为<strong>点积空间中的最近邻搜索问题</strong>。(这里论文里没展开说，我觉得从<strong>局部敏感哈希</strong>和<strong>用户向量的KNN</strong>入手都是可行的)</p><h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p>最关键的部分：</p><p><img src="/2018/07/10/youtube/3.png" width="600"></p><p>借鉴了CBOW,输入是最近看过的N个视频id,还有搜索过的N个词，将不定长(个数不定)的稀疏向量通过加和或平均等手段转为定长向量，作为DNN的输入，其他特征如人口统计特征等不经emb直接作为输入。其中，emb参数矩阵作为整个网络的一部分参与训练，在神经网络反向传播过程中更新参数。在训练阶段，最后一层经过softmax输出概率(使用sampled softmax)，最终训练得到user vector和video vector矩阵，在线上阶段，通过最近邻搜索来进行有效率的打分和召回。</p><h3 id="Heterogeneous-Signals"><a href="#Heterogeneous-Signals" class="headerlink" title="Heterogeneous Signals"></a>Heterogeneous Signals</h3><p>使用DNN来作为矩阵分解的一种泛化的好处就是可以任意加入连续特征或类别特征。例如ID类特征做embedding处理，像简单的二值特征、性别、登录状态、年龄等可以直接作为DNN的输入，实数特征做normalization处理。</p><p><strong>“example age” Feature</strong></p><p>关于如何将新鲜度作为特征，论文这里给出了一种方法，将视频已上传时长作为特征，训练集末尾的视频该特征置为0，从而取得了很不错的效果：</p><p><img src="/2018/07/10/youtube/4.png" width="500"></p><p>从上图可以看出，新上传的视频召回的概率大大增加，而这也与用户喜欢新视频的意愿相符。</p><h3 id="Label-and-Context-Selection"><a href="#Label-and-Context-Selection" class="headerlink" title="Label and Context Selection"></a>Label and Context Selection</h3><p>训练集的选取是从整个Youtube视频库中选取的而不仅仅是展示(推荐)给用户的那些视频，否则新内容将很难有机会露脸，这样，如果用户是观看了其他的source而不是我们推荐的视频，那我们就可以协同过滤很快的把这种特性传播给其他用户。</p><p>还有一个需要注意的问题是要注意搜索词的顺序问题。例如一个用户刚刚搜索完”泰勒斯威夫特”，那么马上展示给她的肯定全部都是“泰勒斯威夫特”的内容，而我们当然不希望直接给用户推荐全部都是用户马上搜索过的内容(为了多样性的考虑)，因此在实际的应用中，需要打乱搜索词的顺序。</p><p>另外，用户观看存在一种模式，例如对于一些电视剧，往往会一直追着看，因此未来时间的消费(观看)就和过去的消费产生了关联，因此在选取训练集的时候，不能随机选取hold out来训练，而是选取训练选取的当前时间点之前的视频来训练，避免造成future information leak。</p><p><img src="/2018/07/10/youtube/5.png" width="700"></p><h3 id="Expriments-with-Features-and-Depth"><a href="#Expriments-with-Features-and-Depth" class="headerlink" title="Expriments with Features and Depth"></a>Expriments with Features and Depth</h3><p>在实验特征和网络结构的时候，选取了1M视频和1M的搜索词来构建vocabulary，隐向量的长度为256，作为特征的最近观看视频和搜索词条数为50。整体的网络是一个塔型的DNN。</p><p><img src="/2018/07/10/youtube/6.png" width="400"></p><h2 id="RANKING"><a href="#RANKING" class="headerlink" title="RANKING"></a>RANKING</h2><p>在召回一批视频以后，最后一步要对每个视频进行打分然后按照排序顺序呈现给用户，在深度排序模型中，与召回模型不同，为了综合考虑各方面的因素，需要更精细的特征例如点赞数，评论数，转发数等。同时，每个视频的召回来源也是非常重要的。</p><p>深度排序的架构和深度召回模型很相似，最后用修改过的logistic regression来对每个视频打分，最终的排序依据是一个针对观看时长的简单函数。采取观看时长而不是点击率来进行排序是因为有些“骗子视频”的点击率非常高，但是内容十分糟糕，用户往往点击进去以后很快就退出了。</p><p>模型如下图：</p><p><img src="/2018/07/10/youtube/7.png" width="600"></p><h3 id="Feature-Representation"><a href="#Feature-Representation" class="headerlink" title="Feature Representation"></a>Feature Representation</h3><p>类别特征主要分为”univalent”和”Multivalent”特征，其中univalent就是单值离散特征，而Multivalent就是多值离散特征，比如最近观看过的视频ID，在这里的处理是过embedding后采取均值处理。</p><p><strong>“Feature Engineering”</strong></p><p>尽管DNN有这种“自动学习特征”的特性，实际上还是需要大量的人工来提取特征的工作，关于特征工程部分本文不做过多的介绍：</p><p>Main challenge : 如何将用户的近期行为与候选视频的打分联系起来？</p><p>Some hints：</p><ul><li>候选集来自哪个channel</li><li>用户历史上从这个channel观看了多少个视频</li><li>用户上一次观看和候选视频相同topic的视频是什么时间</li><li>这个候选视频的来源是什么(pass infromation from candidate generation)</li></ul><p>Others: 描述与视频被推荐的频次相关的特征也非常重要，例如某视频多次被推荐但是无人观看</p><p><strong>“Embedding Categorical Features”</strong></p><p>关于高维离散特征，除了上述的处理办法，还有两个trick：</p><ol><li>对于出现次数较少的视频不被编码到vocabulary中，以此来减少vocabulary的长度</li><li>共享底层的embedding</li></ol><p><strong>“Normalizing Continuous Featuers”</strong></p><p>连续特征归一化这里使用了累积分布(估计跟min-max等差别不大)，也就是概率密度函数的积分：</p><p><img src="/2018/07/10/youtube/8.png" width="100"></p><p>同时，将归一化结果的根号和平方也作为特征输入。</p><h3 id="Model-Expected-Watch-Time"><a href="#Model-Expected-Watch-Time" class="headerlink" title="Model Expected Watch Time"></a>Model Expected Watch Time</h3><p>使用了”weighted logistic regression”，正样本(标有观看时长的视频)，负样本(观看时长小于某一threshold)，在训练的时候使用观看时长进行加权，对正样本使用其观看时长进行加权，对负样本进行单位加权，这样，最终学习到的目标为:</p><p><img src="/2018/07/10/youtube/9.png" width="50"></p><p>其中N为训练样本总数，k是正样本的数量，Ti是第i个视频的观看时长，经过推导后，其近似于e的指数，用来近似代替预估观看时长。</p><h3 id="Expriments-with-Hidden-Layers"><a href="#Expriments-with-Hidden-Layers" class="headerlink" title="Expriments with Hidden Layers"></a>Expriments with Hidden Layers</h3><p><img src="/2018/07/10/youtube/10.png" width="450"></p><h3 id="CONCLUSIONS"><a href="#CONCLUSIONS" class="headerlink" title="CONCLUSIONS"></a>CONCLUSIONS</h3><p>模型很简单就不复现了，本文除了讲模型外其他的一些点比较关键吧，比如训练集的选取，每个用户等量训练集来避免倾向活跃用户等，以及”example age”这种指代不明，耐人寻味的东西(因为请教过公司一个之前在youtube做推荐的大佬，说这个东西也就是说说而已。。)等。</p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;谷歌Youtube团队的DNN推荐模型&lt;/center&gt;
    
    </summary>
    
    
      <category term="推荐算法" scheme="http://yoursite.com/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Batch Normalization</title>
    <link href="http://yoursite.com/2018/06/22/bn/"/>
    <id>http://yoursite.com/2018/06/22/bn/</id>
    <published>2018-06-22T07:42:29.000Z</published>
    <updated>2018-06-22T07:55:57.751Z</updated>
    
    <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>批标准化（Batch Normalization ）简称BN算法，<strong>是为了克服神经网络层数加深导致难以训练而诞生的一个算法</strong>。根据ICS理论，<strong>当训练集的样本数据和目标样本集分布不一致的时候，训练得到的模型无法很好的泛化</strong>。</p><p>而在神经网络中，<strong>每一层的输入在经过层内操作之后必然会导致与原来对应的输入信号分布不同</strong>,,并且前层神经网络的增加会被后面的神经网络不对的累积放大。这个问题的一个<strong>解决思路</strong>就是根据训练样本与目标样本的比例对训练样本进行一个矫正，而BN算法（批标准化）则可以用来<strong>规范化某些层或者所有层的输入</strong>，从而<strong>固定每层输入信号的均值与方差。</strong></p><p><img src="/2018/06/22/bn/1.png" width="420" alt="none" align="center"></p><h1 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h1><p>我们知道在神经网络训练开始前，都要对输入数据做一个归一化处理，那么具体<strong>为什么需要归一化呢？归一化后有什么好处呢？</strong></p><ol><li>原因在于<strong>神经网络学习过程本质就是为了学习数据分布</strong>，一旦<strong>训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低</strong>；另外一方面，一旦每批训练数据的分布各不相同(batch 梯度下降)，那么<strong>网络就要在每次迭代都去学习适应不同的分布</strong>，这样将会大大降低网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因。</li><li>对于深度网络的训练是一个复杂的过程，只要网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。<strong>一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布</strong>，所以如果训练过程中，<strong>训练数据的分布一直在发生变化，那么将会影响网络的训练速度</strong>。</li><li>我们知道网络一旦train起来，那么参数就要发生更新<strong>(每一层的参数更新变化导致分布变化)</strong>，除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。以网络第二层为例：网络的第二层输入，是由第一层的参数和input计算得到的，而第一层的参数在整个训练过程中一直在变化，因此必然会引起后面每一层输入数据分布的改变。<strong>我们把网络中间层在训练过程中，数据分布的改变称之为：“Internal  Covariate Shift”。Paper所提出的算法，就是要解决在训练过程中，中间层数据分布发生改变的情况</strong>，于是就有了Batch  Normalization，这个牛逼算法的诞生。</li></ol><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>批标准化一般用在非线性映射（激活函数）之前，对<code>y= Wx + b</code>进行规范化，是结果(输出信号的各个维度)的均值都为0,方差为1,让每一层的输入有一个稳定的分布会有利于网络的训练</p><p>在<strong>神经网络收敛过慢</strong>或者<strong>梯度爆炸时</strong>的那个无法训练的情况下都可以尝试。</p><p><strong>归一化公式：</strong></p><p><img src="/2018/06/22/bn/3.png" width="220" alt="none" align="center"></p><p>E(xk)指的是每一批训练数据神经元xk的平均值；然后分母就是每一批数据神经元xk激活度的一个标准差了。</p><p>这么简单的想法，为什么之前没人用呢？然而<strong>其实实现起来并不是那么简单的</strong>。其实如果是仅仅使用上面的归一化公式，对网络某一层A的输出数据做归一化，然后送入网络下一层B，这样是<strong>会影响到本层网络A所学习到的特征的</strong>。打个比方，比如我网络中间某一层学习到特征数据本身就分布在S型激活函数的两侧，你强制把它给我归一化处理、标准差也限制在了1，把数据变换成分布于s函数的中间部分，这样就<strong>相当于我这一层网络所学习到的特征分布被你搞坏了</strong>，这可怎么办？</p><p><strong>训练</strong></p><p>关于DNN中的normalization，大家都知道<strong>白化（whitening）</strong>，只是在模型训练过程中进行<strong>白化操作会带来过高的计算代价和运算时间</strong>。因此本文提出两种简化方式：<strong>1）直接对输入信号的每个维度做规范化</strong>（“normalize each scalar feature independently”）；<strong>2）在每个mini-batch中计算得到mini-batch mean和variance来替代整体训练集的mean和variance</strong>.</p><p>论文中提到的方法：变换重构，引入了可学习参数γ、β，这就是算法关键之处：</p><p><img src="/2018/06/22/bn/4.png" width="180" alt="none" align="center"></p><p>每一个神经元xk都会有一对这样的参数γ、β。这样其实当：</p><p><img src="/2018/06/22/bn/5.png" width="230" alt="none" align="center"></p><p>是可以恢复出原始的某一层所学到的特征的。因此我们引入了这个可学习重构参数γ、β，<strong>让我们的网络可以学习恢复出原始网络所要学习的特征分布</strong>。最后Batch Normalization网络层的前向传导过程公式就是：</p><p><img src="/2018/06/22/bn/6.png" width="330" alt="none" align="center"></p><p>上面的公式中m指的是mini-batch size。</p><p>源码实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">m = K.mean(X, axis=<span class="number">-1</span>, keepdims=<span class="keyword">True</span>)<span class="comment">#计算均值  </span></span><br><span class="line">std = K.std(X, axis=<span class="number">-1</span>, keepdims=<span class="keyword">True</span>)<span class="comment">#计算标准差  </span></span><br><span class="line">X_normed = (X - m) / (std + self.epsilon)<span class="comment">#归一化  </span></span><br><span class="line">out = self.gamma * X_normed + self.beta<span class="comment">#重构变换</span></span><br></pre></td></tr></table></figure><p><strong>预测</strong></p><p>(1) 一个网络一旦训练完了，就没有了min-batch这个概念了。测试阶段我们一般只输入一个测试样本，看看结果而已。因此测试样本，前向传导的时候，上面的均值u、标准差σ 要哪里来？其实网络一旦训练完毕，参数都是固定的，这个时候即使是每批训练样本进入网络，那么BN层计算的均值u、和标准差都是固定不变的。我们可以采用这些数值来作为测试样本所需要的均值、标准差，于是最后测试阶段的u和σ 计算公式如下：</p><p> <img src="/2018/06/22/bn/7.png" width="200" alt="none" align="center"></p><p>上面简单理解就是：对于均值来说直接计算所有batch u值的平均值；然后对于标准偏差采用每个batch σB的无偏估计。最后测试阶段，BN的使用公式就是：</p><p><img src="/2018/06/22/bn/8.png" width="310" alt="none" align="center"></p><p>(2) 根据文献说，BN可以应用于一个神经网络的任何神经元上。文献主要是把BN变换，置于网络激活函数层的前面。在没有采用BN的时候，激活函数层是这样的：</p><p><img src="/2018/06/22/bn/9.png" width="110" alt="none" align="center"></p><p>也就是我们希望一个激活函数，比如s型函数s(x)的自变量x是经过BN处理后的结果。因此前向传导的计算公式就应该是：</p><p><img src="/2018/06/22/bn/11.png" width="110" alt="none" align="center"></p><p>其实因为偏置参数b经过BN层后其实是没有用的，最后也会被均值归一化，当然BN层后面还有个β参数作为偏置项，所以b这个参数就可以不用了。因此最后把BN层+激活函数层就变成了：</p><p><img src="/2018/06/22/bn/12.png" width="130" alt="none" align="center"></p><p><img src="/2018/06/22/bn/14.png" width="630" alt="none" align="center"></p><p>我们引入一些 batch normalization 的公式. 这三步就是我们在刚刚一直说的 normalization 工序, 但是公式的后面还有一个反向操作, 将 normalize 后的数据再扩展和平移. <strong>原来这是为了让神经网络自己去学着使用和修改这个扩展参数 gamma, 和 平移参数 β, 这样神经网络就能自己慢慢琢磨出前面的 normalization 操作到底有没有起到优化的作用, 如果没有起到作用, 我就使用 gamma 和 belt 来抵消一些 normalization 的操作.</strong></p><h1 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h1><ol><li><p>减少了参数的人为选择,<strong>可以取消dropout和L2正则项参数</strong>,或者采取更小的L2正则项约束参数</p></li><li><p><strong>减少了对学习率的要求</strong>，你可以<strong>选择比较大的初始学习率</strong>，让你的训练速度飙涨。以前还需要慢慢调整学习率，甚至在网络训练到一半的时候，还需要想着学习率进一步调小的比例选择多少比较合适，现在我们可以采用初始很大的学习率，然后学习率的衰减速度也很大，因为这个算法收敛很快。当然这个算法即使你选择了较小的学习率，也比以前的收敛速度快，因为它具有快速训练收敛的特性；</p></li><li><p><strong>可以不再使用局部响应归一化了</strong>,BN本身就是归一化网络(局部响应归一化-AlexNet)</p></li><li><p>更破坏原来的数据分布,<strong>一定程度上缓解过拟合</strong></p></li></ol><h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><p>BN在TensorFlow中主要有两个函数:<code>tf.nn.moments</code>以及<code>tf.nn.batch_normalization</code>,两者需要配合使用,前者用来返回均值和方差,后者用来进行批处理(BN)</p><h2 id="tf-nn-moments"><a href="#tf-nn-moments" class="headerlink" title="tf.nn.moments"></a>tf.nn.moments</h2><p> <strong>TensorFlow中的函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">moments(</span><br><span class="line">    x,</span><br><span class="line">    axes,</span><br><span class="line">    shift=<span class="keyword">None</span>,</span><br><span class="line">    name=<span class="keyword">None</span>,</span><br><span class="line">    keep_dims=<span class="keyword">False</span></span><br><span class="line">)</span><br><span class="line">  Returns:</span><br><span class="line">    Two `Tensor` objects: `mean` <span class="keyword">and</span> `variance`.</span><br></pre></td></tr></table></figure><p>其中参数 x 为要传递的tensor, axes是个int数组,传递要进行计算的维度,返回值是<code>两个张量: mean and variance</code>,我们需要利用这个函数计算出BN算法需要的前两项,公式见前面的原理部分 </p><p>参考代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算Wx_plus_b 的均值与方差,其中axis = [0] 表示想要标准化的维度</span></span><br><span class="line">img_shape= [<span class="number">128</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">64</span>]</span><br><span class="line">Wx_plus_b = tf.Variable(tf.random_normal(img_shape))</span><br><span class="line">axis = list(range(len(img_shape)<span class="number">-1</span>)) <span class="comment"># [0,1,2] </span></span><br><span class="line">wb_mean, wb_var = tf.nn.moments(Wx_plus_b, axis)</span><br></pre></td></tr></table></figure><p><strong>我们已经假设图片的shape[128, 32, 32, 64],它的运算方式如图:</strong></p><p> <img src="/2018/06/22/bn/13.png" width="510" alt="none" align="center"></p><h2 id="tf-nn-batch-normalization"><a href="#tf-nn-batch-normalization" class="headerlink" title="tf.nn.batch_normalization"></a>tf.nn.batch_normalization</h2><p><strong>TensorFlow中的函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">batch_normalization(</span><br><span class="line">    x,</span><br><span class="line">    mean,</span><br><span class="line">    variance,</span><br><span class="line">    offset,</span><br><span class="line">    scale,</span><br><span class="line">    variance_epsilon,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>其中x为输入的tensor,mean,variance由moments()求出,而offset,scale一般分别初始化为0和1,variance_epsilon一般设为比较小的数字即可<strong>,参考代码如下:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scale = tf.Variable(tf.ones([<span class="number">64</span>]))</span><br><span class="line">offset = tf.Variable(tf.zeros([<span class="number">64</span>]))</span><br><span class="line">variance_epsilon = <span class="number">0.001</span></span><br><span class="line">Wx_plus_b = tf.nn.batch_normalization(Wx_plus_b, wb_mean, wb_var, offset, scale, variance_epsilon)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据公式我们也可以自己写一个</span></span><br><span class="line">Wx_plus_b1 = (Wx_plus_b - wb_mean) / tf.sqrt(wb_var + variance_epsilon)</span><br><span class="line">Wx_plus_b1 = Wx_plus_b1 * scale + offset</span><br><span class="line"><span class="comment"># 因为底层运算方式不同,实际上自己写的最后的结果与直接调用tf.nn.batch_normalization获取的结果并不一致</span></span><br></pre></td></tr></table></figure><h1 id="使用BN的深层原因"><a href="#使用BN的深层原因" class="headerlink" title="使用BN的深层原因"></a>使用BN的深层原因</h1><p>说到底，BN的提出还是为了克服深度神经网络难以训练的弊病。其实BN背后的insight非常简单，只是在文章中被Google复杂化了。<br>首先来说说“Internal Covariate Shift”。文章的title除了BN这样一个关键词，还有一个便是“ICS”。大家都知道在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如，transfer learning/domain adaptation等。而covariate shift就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同，即：对所有<img src="https://www.zhihu.com/equation?tex=x%5Cin+%5Cmathcal%7BX%7D" alt="x\in \mathcal{X}">,<img src="https://www.zhihu.com/equation?tex=P_s%28Y%7CX%3Dx%29%3DP_t%28Y%7CX%3Dx%29" alt="P_s(Y|X=x)=P_t(Y|X=x)">，但是<img src="https://www.zhihu.com/equation?tex=P_s%28X%29%5Cne+P_t%28X%29" alt="P_s(X)\ne P_t(X)">. 大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。<br>那么好，为什么前面我说Google将其复杂化了。其实如果严格按照解决covariate shift的路子来做的话，大概就是上“importance weight”（<a href="https://link.zhihu.com/?target=http%3A//120.52.72.36/www.jmlr.org/c3pr90ntcsf0/papers/volume8/sugiyama07a/sugiyama07a.pdf" target="_blank" rel="noopener">ref</a>）之类的机器学习方法。可是这里Google仅仅说“通过mini-batch来规范化某些层/所有层的输入，从而可以固定每层输入信号的均值与方差”就可以解决问题。如果covariate shift可以用这么简单的方法解决，那前人对其的研究也真真是白做了。此外，试想，均值方差一致的分布就是同样的分布吗？当然不是。显然，ICS只是这个问题的“包装纸”嘛，仅仅是一种high-level demonstration。<br>那BN到底是什么原理呢？说到底还是<strong>为了防止“梯度弥散”</strong>。关于梯度弥散，大家都知道一个简单的栗子：<img src="https://www.zhihu.com/equation?tex=0.9%5E%7B30%7D%5Capprox+0.04" alt="0.9^{30}\approx 0.04">。在BN中，是通过将activation规范为均值和方差一致的手段使得原本会减小的activation的scale变大。可以说是一种更有效的local response normalization方法</p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p><a href="https://blog.csdn.net/hjimce/article/details/50866313" target="_blank" rel="noopener">hjmce</a></p><p><a href="https://blog.csdn.net/fontthrone/article/details/76652772" target="_blank" rel="noopener">tianFont</a></p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;Batch Normalization原理及实现&lt;/center&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>FFM与FM模型原理与实践</title>
    <link href="http://yoursite.com/2018/05/04/ffm/"/>
    <id>http://yoursite.com/2018/05/04/ffm/</id>
    <published>2018-05-04T14:31:54.000Z</published>
    <updated>2018-05-11T04:21:28.861Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是FM模型？"><a href="#什么是FM模型？" class="headerlink" title="什么是FM模型？"></a>什么是FM模型？</h1><p>因子分解机(Factorization Machine, FM)是由Steffen Rendle提出的一种基于矩阵分解的机器学习算法，其开源库为libFM，<strong>FM模型有以下优势：</strong><br>1.FM模型对稀疏矩阵有非常好的适应性，这点比SVM强很多。<br>2.FM拥有线性的复杂度<br>3.FM具有非常好的适应性<br>所以，其突出的特点就是<strong>对高维度稀疏矩阵</strong>效果比较好。</p><h1 id="FM模型原理"><a href="#FM模型原理" class="headerlink" title="FM模型原理"></a>FM模型原理</h1><p>FM模型表示为：<br>$$y:=w_0+\sum_{i=1}^n{w_ix_i}+\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}&lt;v_i,v_j&gt;x_ix_j$$<br><strong>前半部分</strong>为传统的线性模型，<strong>后半部分</strong>表示两个互异特征分量之间的关系,$&lt;v_i,v_j&gt;$的表达式如下：<br>$$&lt;v_i,v_j&gt;:=\sum_{f=1}^kv_{i,f}·v_{j,f}$$<br>表示长度为k的两个向量的点积。$v_i$表示的是系数矩阵$V$的第$i$维隐向量。<strong>k是定义factorization维度的超参数</strong>，是正整数因子分解机FM也可以推广到高阶的形式，即将更多互异特征分量之间的相互关系考虑进来。</p><hr><p><strong>先从多项式模型开始看起：</strong>$$y:=w_0+\sum_{i=1}^n{w_ix_i}+\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}w_{ij}x_ix_j$$<br>从上述公式可以看出来，组合特征的参数一共有$\frac{n(n-1)}{2}$个，然而，在数据稀疏性普遍存在的实际应用场景中，二次项参数的训练是很困难的。其原因是，每个参数 $w_{ij}$ 的训练<strong>需要大量 $x_i$ 和 $x_j$ 都非零的样本</strong>；由于样本数据本来就比较稀疏，满足“$x_i$ 和 $x_j$ 都非零”的样本将会非常少。<strong>训练样本的不足</strong>，很容易导致参数 $w_{ij}$ 不准确，最终将严重影响模型的性能。</p><p><strong>那么，如何解决二次项参数的训练问题呢？</strong><br>矩阵分解提供了一种解决思路。在model-based的协同过滤中，一个rating矩阵可以分解为user矩阵和item矩阵，每个user和item都可以采用一个隐向量表示。比如在下图中的例子中，我们把每个user表示成一个二维向量，同时把每个item表示成一个二维向量，两个向量的点积就是矩阵中user对item的打分。<br><img src="/2018/05/04/ffm/1.png" alt=""><br>类似地，所有二次项参数 $w_{ij}$ 可以组成一个对称阵 $W$（为了方便说明FM的由来，对角元素可以设置为正实数），那么这个矩阵就可以分解为 $W=V^TV$，<strong>$V$ 的第 $j$ 列便是第 $j$ 维特征的隐向量</strong>。换句话说，每个参数 $w_{ij}=⟨v_i,v_j⟩$，这就是FM模型的核心思想，因此可以得到FM的模型:<br>$$y:=w_0+\sum_{i=1}^n{w_ix_i}+\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}&lt;v_i,v_j&gt;x_ix_j$$<br>其中，$v_i$ 是第 $i$ 维特征的隐向量，$⟨⋅,⋅⟩$ 代表向量点积。隐向量的长度为 k,包含 k 个描述特征的因子，根据上述公式，二次项的参数数量减少为 $kn$个，远少于多项式模型的参数数量。另外，参数因子化使得 $x_hx_i$ 的参数和 $x_ix_j$ 的参数不再是相互独立的，因此我们可以在样本稀疏的情况下相对合理地估计FM的二次项参数。具体来说，$x_hx_i$ 和 $x_ix_j$ 的系数分别为 $⟨v_h,v_i⟩$ 和 $⟨v_i,v_j⟩$，它们之间有共同项 $v_i$。也就是说，所有包含“$x_i$ 的非零组合特征”（存在某个 $j≠i$，使得 $x_ix_j≠0$）的样本都可以用来学习隐向量 vi，这很大程度上避免了数据稀疏性造成的影响。而在多项式模型中，$w_{hi}$ 和 $w_{ij}$ 是相互独立的。</p><h1 id="FM复杂度分析"><a href="#FM复杂度分析" class="headerlink" title="FM复杂度分析"></a>FM复杂度分析</h1><p>直观上看，FM的复杂度是$ O(kn^2)$的，但是通过下式可以化简二次项:<br><img src="/2018/05/04/ffm/3.png" width="400" height="400" alt="none" align="center"><br><img src="/2018/05/04/ffm/2.png" width="380" height="80" alt="none" align="center"><br>可以将复杂度优化到$O(kn)$，所以FM可以在线性时间内对样本做出预测。</p><p><strong>FM的训练复杂度：</strong>，使用SGD训练模型，各参数的梯度如下：<br><img src="/2018/05/04/ffm/4.png" width="380" alt="none" align="center"></p><p>其中，<strong>$v_{j,f}$ 是隐向量 $v_j$ 的第 $f$ 个元素</strong>。由于 $\sum_{j=1}^n{v_{j,f}x_j}$ 只与 $f$ 有关，而与$ i $无关，在每次迭代过程中，<strong>只需计算一次所有$ f$ 的  $\sum_{j=1}^n{v_{j,f}x_j}$，就能够方便地得到所有 $v_{i,f}$ 的梯度。</strong>显然，计算所有 $f$ 的 $\sum_{j=1}^n{v_{j,f}x_j}$ 的复杂度是 $O(kn)$；已知 $\sum_{j=1}^n{v_{j,f}x_j}$时，计算每个参数梯度的复杂度是 $O(1)$；得到梯度后，更新每个参数的复杂度是 $O(1)$；模型参数一共有 $nk+n+1$ 个。因此，FM参数训练的复杂度也是 $O(kn)$。<strong>综上可知，FM可以在线性时间训练和预测，是一种非常高效的模型。</strong></p><h1 id="FFM模型"><a href="#FFM模型" class="headerlink" title="FFM模型"></a>FFM模型</h1><p>FFM模型是FM模型的一种升级版，具体体现在引入了”域”field这个概念，相同性质的特征同属于一个field。简单来讲，<strong>同一个类别特征生成的one-hot特征都可以放在同一个field中</strong>，在FFM中，每一维特征 $x_i，$针对其它特征的每一种field $f_j$，都会学习一个隐向量 $v_i$,$f_j$。因此，<strong>隐向量不仅与特征相关，也与field相关</strong>。举个例子来说，用户ID One-hot以后产生了500个特征，那么其中“id00001”这个特征就会和Age，gender这两个field去产生不同的隐向量，这与“Age”，“gender”的内在差异相符，这也是FFM种“field-aware”的由来。</p><p><strong>FM和FFM的联系：</strong><br><strong>假设样本的 $n $个特征属于 $f$ 个field，那么FFM的二次项有$ nf$个隐向量。而在FM模型中，每一维特征的隐向量只有一个。FM可以看作FFM的特例，是把所有特征都归属到一个field时的FFM模型</strong>，即通过引入field的概念，来对产生的隐向量进行细化，从而达到更好的挖掘交叉特征的目的。</p><p>根据其field特性，可以得到FFM模型的公式：<br>$$y:=w_0+\sum_{i=1}^n{w_ix_i}+\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}&lt;v_if_j,v_jf_i&gt;x_ix_j$$<br>其中，$f_j$ 是第$ j$ 个特征所属的field。如果隐向量的长度为 $k$，那么FFM的二次参数有 $nfk$ 个，远多于FM模型的 $nk$ 个。此外，<strong>由于隐向量与field相关，FFM二次项并不能够化简，其预测复杂度是 $O(kn^2)$。</strong></p><hr><p>下面举一个例子：<br><img src="/2018/05/04/ffm/6.png" width="300" alt="none" align="center"></p><p>这条记录可以编码成5个特征，其中“Genre=Comedy”和“Genre=Drama”属于同一个field，“Price”是数值型，不用One-Hot编码转换。为了方便说明FFM的样本格式，我们将所有的特征和对应的field映射成整数编号。<br><img src="/2018/05/04/ffm/7.png" width="400" alt="none" align="center"><br>那么，FFM的组合特征有10项，如下图所示。<br><img src="/2018/05/04/ffm/8.png" width="600" alt="none" align="center"><br>其中，红色是field编号，蓝色是特征编号，绿色是此样本的特征取值。二次项的系数是通过与特征field相关的隐向量点积得到的，二次项共有 $\frac{n(n−1)}{2}$ 个。</p><h1 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h1><p><strong>主要用于CTR/CVR预估。</strong>总体来讲可以分为这几类：<br><strong>(1) 回归问题</strong><br>损失函数为平方损失函数。<br><strong>(2) 分类问题</strong><br>使用logitloss作为优化标准，使用sigmoid函数<br><strong>(3) 排序</strong></p><h1 id="FFM训练和参数"><a href="#FFM训练和参数" class="headerlink" title="FFM训练和参数"></a>FFM训练和参数</h1><p>同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个field。所有的特征必须转换成“field_id:feat_id:value”格式，field_id代表特征所属field的编号，feat_id是特征编号，value是特征的值。<br>数值型的特征比较容易处理，只需分配单独的field编号，如用户评论得分、商品的历史CTR/CVR等。<strong>categorical特征需要经过One-Hot编码成数值型，编码产生的所有特征同属于一个field，</strong>而特征的值只能是0或1，如用户的性别、年龄段，商品的品类id等。除此之外，<strong>还有第三类特征</strong>，如用户浏览/购买品类，有多个品类id且用一个数值衡量用户浏览或购买每个品类商品的数量。这类特征按照categorical特征处理，不同的只是特征的值不是0或1，而是代表用户浏览或购买数量的数值。按前述方法得到field_id之后，再对转换后特征顺序编号，得到feat_id，特征的值也可以按照之前的方法获得。</p><hr><p><strong>在训练FFM的过程中，有许多小细节值得特别关注。</strong><br><strong>1.样本层面的数据是推荐进行归一化的。</strong><br><strong>2.特征归一化。</strong>尤其是数值型特征归一化。CTR/CVR模型采用了多种类型的源特征，包括数值型和categorical类型等。但是，categorical类编码后的特征取值只有0或1，较大的数值型特征会造成样本归一化后categorical类生成特征的值非常小，没有区分性。例如，一条用户-商品记录，用户为“男”性，商品的销量是5000个（假设其它特征的值为零），那么归一化后特征“sex=male”（性别为男）的值略小于0.0002，而“volume”（销量）的值近似为1。特征“sex=male”在这个样本中的作用几乎可以忽略不计，这是相当不合理的。因此，将源数值型特征的值归一化到 [0，1] 是非常必要的。<br><strong>3.省略零值特征。</strong>零值特征对模型完全没有贡献。包含零值特征的一次项和组合项均为零，对于训练模型参数或者目标值预估是没有作用的。因此，可以省去零值特征，提高FFM模型训练和预测的速度，这也是稀疏样本采用FFM的显著优势。</p><hr><p>FM模型，将参数$w_{ij}$对应的矩阵$W$，利用矩阵分解表示为$W=V^TV$, 矩阵$V∈R^{k×n}$, <strong>可以通过调节$k$来调节模型的泛化能力。</strong></p><p><img src="/2018/05/04/ffm/5.png" width="500" alt="none" align="center"></p><h1 id="FFM使用"><a href="#FFM使用" class="headerlink" title="FFM使用"></a>FFM使用</h1><p>这里使用了xlearn库：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xlearn <span class="keyword">as</span> xl</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training task</span></span><br><span class="line">ffm_model = xl.create_ffm() <span class="comment"># Use field-aware factorization machine</span></span><br><span class="line">ffm_model.setTrain(<span class="string">"./small_train.txt"</span>)  <span class="comment"># Training data</span></span><br><span class="line">ffm_model.setValidate(<span class="string">"./small_test.txt"</span>)  <span class="comment"># Validation data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># param:</span></span><br><span class="line"><span class="comment">#  0. binary classification</span></span><br><span class="line"><span class="comment">#  1. learning rate: 0.2</span></span><br><span class="line"><span class="comment">#  2. regular lambda: 0.002</span></span><br><span class="line"><span class="comment">#  3. evaluation metric: accuracy</span></span><br><span class="line">param = &#123;<span class="string">'task'</span>:<span class="string">'binary'</span>, <span class="string">'lr'</span>:<span class="number">0.2</span>, </span><br><span class="line">         <span class="string">'lambda'</span>:<span class="number">0.002</span>, <span class="string">'metric'</span>:<span class="string">'acc'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start to train</span></span><br><span class="line"><span class="comment"># The trained model will be stored in model.out</span></span><br><span class="line">ffm_model.fit(param, <span class="string">'./model.out'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prediction task</span></span><br><span class="line">ffm_model.setTest(<span class="string">"./small_test.txt"</span>)  <span class="comment"># Test data</span></span><br><span class="line">ffm_model.setSigmoid()  <span class="comment"># Convert output to 0-1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Start to predict</span></span><br><span class="line"><span class="comment"># The output result will be stored in output.txt</span></span><br><span class="line">ffm_model.predict(<span class="string">"./model.out"</span>, <span class="string">"./output.txt"</span>)</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------</span><br><span class="line">           _</span><br><span class="line">          | |</span><br><span class="line">     __  _| |     ___  __ _ _ __ _ __</span><br><span class="line">     \ \/ / |    / _ \/ _` | <span class="string">'__| '</span>_ \ </span><br><span class="line">      &gt;  &lt;| |___|  __/ (_| | |  | | | |</span><br><span class="line">     /_/\_\_____/\___|\__,_|_|  |_| |_|</span><br><span class="line"></span><br><span class="line">        xLearn   -- <span class="number">0.31</span> Version --</span><br><span class="line">----------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">[ ACTION     ] Read Problem ...</span><br><span class="line">[------------] First check <span class="keyword">if</span> the text file has been already converted to binary format.</span><br><span class="line">[------------] Binary file (./small_train.txt.bin) NOT found. Convert text file to binary file.</span><br><span class="line">[------------] First check <span class="keyword">if</span> the text file has been already converted to binary format.</span><br><span class="line">[------------] Binary file (./small_test.txt.bin) NOT found. Convert text file to binary file.</span><br><span class="line">[------------] Number of Feature: <span class="number">9991</span></span><br><span class="line">[------------] Number of Field: <span class="number">18</span></span><br><span class="line">[------------] Time cost <span class="keyword">for</span> reading problem: <span class="number">0.00</span> (sec)</span><br><span class="line">[ ACTION     ] Initialize model ...</span><br><span class="line">[------------] Model size: <span class="number">5.56</span> MB</span><br><span class="line">[------------] Time cost <span class="keyword">for</span> model initial: <span class="number">0.00</span> (sec)</span><br><span class="line">[ ACTION     ] Start to train ...</span><br><span class="line">[------------] Epoch      Train log_loss       Test log_loss       Test Accuarcy     Time cost (sec)</span><br><span class="line">[   <span class="number">10</span>%      ]     <span class="number">1</span>            <span class="number">0.594795</span>            <span class="number">0.535269</span>            <span class="number">0.770000</span>                <span class="number">0.00</span></span><br><span class="line">[   <span class="number">20</span>%      ]     <span class="number">2</span>            <span class="number">0.537182</span>            <span class="number">0.551915</span>            <span class="number">0.770000</span>                <span class="number">0.00</span></span><br><span class="line">[   <span class="number">30</span>%      ]     <span class="number">3</span>            <span class="number">0.519317</span>            <span class="number">0.532339</span>            <span class="number">0.770000</span>                <span class="number">0.00</span></span><br><span class="line">[   <span class="number">40</span>%      ]     <span class="number">4</span>            <span class="number">0.507896</span>            <span class="number">0.535436</span>            <span class="number">0.770000</span>                <span class="number">0.00</span></span><br><span class="line">[   <span class="number">50</span>%      ]     <span class="number">5</span>            <span class="number">0.493365</span>            <span class="number">0.534694</span>            <span class="number">0.775000</span>                <span class="number">0.00</span></span><br><span class="line">[   <span class="number">60</span>%      ]     <span class="number">6</span>            <span class="number">0.481961</span>            <span class="number">0.534718</span>            <span class="number">0.775000</span>                <span class="number">0.00</span></span><br><span class="line">[   <span class="number">70</span>%      ]     <span class="number">7</span>            <span class="number">0.470308</span>            <span class="number">0.528452</span>            <span class="number">0.775000</span>                <span class="number">0.00</span></span><br><span class="line">[   <span class="number">80</span>%      ]     <span class="number">8</span>            <span class="number">0.464966</span>            <span class="number">0.534071</span>            <span class="number">0.770000</span>                <span class="number">0.00</span></span><br><span class="line">[   <span class="number">90</span>%      ]     <span class="number">9</span>            <span class="number">0.456130</span>            <span class="number">0.534956</span>            <span class="number">0.770000</span>                <span class="number">0.00</span></span><br><span class="line">[ ACTION     ] Early-stopping at epoch <span class="number">7</span></span><br><span class="line">[ ACTION     ] Start to save model ...</span><br><span class="line">[------------] Model file: ./model.out</span><br><span class="line">[------------] Time cost <span class="keyword">for</span> saving model: <span class="number">0.00</span> (sec)</span><br><span class="line">[ ACTION     ] Finish training</span><br><span class="line">[ ACTION     ] Clear the xLearn environment ...</span><br><span class="line">[------------] Total time cost: <span class="number">0.02</span> (sec)</span><br><span class="line">----------------------------------------------------------------------------------------------</span><br><span class="line">           _</span><br><span class="line">          | |</span><br><span class="line">     __  _| |     ___  __ _ _ __ _ __</span><br><span class="line">     \ \/ / |    / _ \/ _` | <span class="string">'__| '</span>_ \ </span><br><span class="line">      &gt;  &lt;| |___|  __/ (_| | |  | | | |</span><br><span class="line">     /_/\_\_____/\___|\__,_|_|  |_| |_|</span><br><span class="line"></span><br><span class="line">        xLearn   -- <span class="number">0.31</span> Version --</span><br><span class="line">----------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">[ ACTION     ] Load model ...</span><br><span class="line">[------------] Load model <span class="keyword">from</span> ./model.out</span><br><span class="line">[------------] Loss function: cross-entropy</span><br><span class="line">[------------] Score function: ffm</span><br><span class="line">[------------] Number of Feature: <span class="number">9991</span></span><br><span class="line">[------------] Number of K: <span class="number">4</span></span><br><span class="line">[------------] Number of field: <span class="number">18</span></span><br><span class="line">[------------] Time cost <span class="keyword">for</span> loading model: <span class="number">0.00</span> (sec)</span><br><span class="line">[ ACTION     ] Read Problem ...</span><br><span class="line">[------------] First check <span class="keyword">if</span> the text file has been already converted to binary format.</span><br><span class="line">[------------] Binary file (./small_test.txt.bin) found. Skip converting text to binary.</span><br><span class="line">[------------] Time cost <span class="keyword">for</span> reading problem: <span class="number">0.00</span> (sec)</span><br><span class="line">[ ACTION     ] Start to predict ...</span><br><span class="line">[------------] The test loss <span class="keyword">is</span>: <span class="number">0.528452</span></span><br><span class="line">[ ACTION     ] Clear the xLearn environment ...</span><br><span class="line">[------------] Total time cost: <span class="number">0.00</span> (sec)</span><br></pre></td></tr></table></figure><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p><a href="https://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html" target="_blank" rel="noopener">美团技术团队</a><br><a href="https://blog.csdn.net/google19890102/article/details/45532745" target="_blank" rel="noopener">zhiyong_will</a><br><a href="https://blog.csdn.net/Leo00000001/article/details/71330927?locationNum=13&amp;fps=1" target="_blank" rel="noopener">Leo000000001</a><br><a href="https://www.jianshu.com/p/965d9d004675" target="_blank" rel="noopener">简书</a></p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;FFM、FM模型原理、应用场景与实践&lt;/center&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>稀疏矩阵-使用scipy</title>
    <link href="http://yoursite.com/2018/05/01/sparseMatrix/"/>
    <id>http://yoursite.com/2018/05/01/sparseMatrix/</id>
    <published>2018-05-01T15:08:52.000Z</published>
    <updated>2018-05-04T14:09:55.933Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是稀疏矩阵"><a href="#什么是稀疏矩阵" class="headerlink" title="什么是稀疏矩阵"></a>什么是稀疏矩阵</h1><p>对于那些零元素的数目<strong>远远多于非零元素</strong>，并且非零元素的<strong>分布没有规律</strong>的矩阵称为稀疏矩阵(sparse Matrix)，相反的，如果大部分是非0元素，那么为稠密矩阵。</p><h1 id="稀疏矩阵的存储格式"><a href="#稀疏矩阵的存储格式" class="headerlink" title="稀疏矩阵的存储格式"></a>稀疏矩阵的存储格式</h1><p>由于稀疏矩阵中非零元素较少，零元素较多，因此可以采用只存储非零元素的方式来进行压缩存储，采用的一般方法就是三元组，分别来保存非0元素的行号，列号和对应的非0值，常用的存储格式有以下几种：</p><h2 id="COO-Coordinate-Format-坐标格式"><a href="#COO-Coordinate-Format-坐标格式" class="headerlink" title="COO(Coordinate Format)-坐标格式"></a>COO(Coordinate Format)-坐标格式</h2><p>COO采用三个数组row、col和data来保存非零元素的信息，这三个数组的长度相同。</p><p><strong>优点：</strong></p><p>灵活，简单，仅存储坐标和值。</p><p><strong>缺点：</strong></p><p>不支持增删改，一旦创建之后转换为其他格式的矩阵才能做更多的操作。</p><p><strong>图示：</strong></p><p><img src="/2018/05/01/sparseMatrix/1.jpg" width="500" height="200" alt="none" align="center"></p><p>上图示例：row=0,column=1,values=7,则对应于稀疏矩阵中0行1列的位置值为7，所以稀疏矩阵有多少个非0值，三元组的长度就有多长。</p><h2 id="CSR-Compressed-Sparse-Row-Format-压缩稀疏行格式"><a href="#CSR-Compressed-Sparse-Row-Format-压缩稀疏行格式" class="headerlink" title="CSR(Compressed Sparse Row Format)-压缩稀疏行格式"></a>CSR(Compressed Sparse Row Format)-压缩稀疏行格式</h2><p>这种格式是把<strong>行的信息压缩存储</strong>了，CSR由三个数组决定:<strong>values,columns,row_ptr</strong></p><p><strong>数组values</strong>：保存非0实数，按行优先保存(即从0行到最后一行，每一行从左到右的顺序)</p><p><strong>Columns:</strong> 和values长度相同，给出每个非0值所在的列标</p><p>Row_ptr: 长度为稀疏矩阵的行数，保存稀疏矩阵的每行第一个非零元素在values的索引。</p><p><strong>图示：</strong></p><p><img src="/2018/05/01/sparseMatrix/2.png" width="400" height="300" alt="none" align="center"></p><p><img src="/2018/05/01/sparseMatrix/3.png" width="400" height="300" alt="none" align="center"></p><p>如上图所示，val和col_ind很好理解，row_ptr第一个值1代表第一行的第一个非0元素在val中的位置为1，<strong>或者说是截止到这一行的第一个非0元素，一共出现了n个非0元素</strong>，以此类推，第二行的3出现时，一共有3个非零元素，<strong>这样就知道每一行有多少个元素了，根据列号可以还原出稀疏矩阵来，以此来压缩行信息</strong>。</p><h2 id="BSR-Block-Compressed-Sparse-Row-Format-分块压缩"><a href="#BSR-Block-Compressed-Sparse-Row-Format-分块压缩" class="headerlink" title="BSR(Block Compressed Sparse Row Format)-分块压缩"></a>BSR(Block Compressed Sparse Row Format)-分块压缩</h2><p>分块压缩其实是基于CSR的，可以分块压缩的稀疏矩阵要求能分解成小的矩阵块，然后再按照CSR的原理进行压缩</p><p><strong>图示：</strong></p><p><img src="/2018/05/01/sparseMatrix/4.png" width="400" height="300" alt="none" align="center"><br><img src="/2018/05/01/sparseMatrix/5.png" width="400" height="300" alt="none" align="center"></p><p>上面的稀疏矩阵的BSR格式为：<br>values  =  (1 0 2 1 6 7 8 2 1 4 5 1 4 3 0 0 7 2 0 0)<br>columns  = (0   1   1   1   2)<br>pointerB = (0   2   3)<br>pointerE = (2   3   5)</p><h2 id="其他格式"><a href="#其他格式" class="headerlink" title="其他格式"></a>其他格式</h2><p>DIA(Diagonal Storage Format)-对角线格式<br>ELLPACK (ELL)<br>Hybrid (HYB)<br>dok_matrix<br>lil_matrix</p><h1 id="不同压缩存储格式的优缺点"><a href="#不同压缩存储格式的优缺点" class="headerlink" title="不同压缩存储格式的优缺点"></a>不同压缩存储格式的优缺点</h1><p><strong>1.BSR-分块压缩矩阵</strong><br>BSR更适合于有密集子矩阵的稀疏矩阵，分块矩阵通常出现在向量值有限的离散元中，在这种情景下，比CSR和CSC算术操作更有效。</p><p><strong>2.CSC-列压缩格式</strong><br>高效的CSC +CSC,CSC*CSC算术运算；高效的列切片操作。但是矩阵内积操作没有CSR,BSR快；行切片操作慢（相比CSR）；稀疏结构的变化代价高（相比LIL 或者 DOK）。</p><p><strong>3.CSR-行压缩格式</strong><br>高效的CSR + CSR, CSR *CSR算术运算；高效的行切片操作；高效的矩阵内积内积操作。但是列切片操作慢（相比CSC）；稀疏结构的变化代价高（相比LIL 或者 DOK）。CSR格式在存储稀疏矩阵时非零元素平均使用的字节数(Bytes per Nonzero Entry)最为稳定（float类型约为8.5，double类型约为12.5）。CSR格式常用于读入数据后进行稀疏矩阵计算。</p><p><strong>4.COO-坐标压缩</strong><br>便利快捷的在不同稀疏格式间转换；允许重复录入，允许重复的元素；从CSR\CSC格式转换非常快速，COO格式常用于从文件中进行稀疏矩阵的读写，如matrix market即采用COO格式。例如某个CSV文件中可能有这样三列：“用户ID，商品ID，评价值”。采用numpy.loadtxt或pandas.read_csv将数据读入之后，可以通过coo_matrix快速将其转换成稀疏矩阵：矩阵的每行对应一位用户，每列对应一件商品，而元素值为用户对商品的评价。但是coo_matrix不支持元素的存取和增删，一旦创建之后，除了将之转换成其它格式的矩阵，几乎无法对其做任何操作和矩阵运算。</p><p><strong>5.DIA-对角压缩</strong><br>对角存储格式(DIA)和ELL格式在进行稀疏矩阵-矢量乘积(sparse matrix-vector products)时效率最高，所以它们是应用迭代法(如共轭梯度法)解稀疏线性系统最快的格式；DIA格式存储数据的非零元素平均使用的字节数与矩阵类型有较大关系，适合于StructuredMesh结构的稀疏矩阵（float类型约为4.05，double类型约为8.10）。对于Unstructured Mesh以及Random Matrix，DIA格式使用的字节数是CSR格式的十几倍。</p><p><strong>综合来讲</strong><br>1.COO和CSR格式比起DIA和ELL来，更加灵活，易于操作；<br>2.ELL的优点是快速，而COO优点是灵活，二者结合后的HYB格式是一种不错的稀疏矩阵表示格式；<br>3.CSR格式在存储稀疏矩阵时非零元素平均使用的字节数(Bytes per Nonzero Entry)最为稳定（float类型约为8.5，double类型约为12.5），而DIA格式存储数据的非零元素平均使用的字节数与矩阵类型有较大关系，适合于StructuredMesh结构的稀疏矩阵（float类型约为4.05，double类型约为8.10，对于Unstructured Mesh以及Random Matrix,DIA格式使用的字节数是CSR格式的十几倍；<br>4.<strong>一些线性代数计算库：COO格式常用于从文件中进行稀疏矩阵的读写，如matrix market即采用COO格式，而CSR格式常用于读入数据后进行稀疏矩阵计算。</strong></p><h1 id="Scipy实践"><a href="#Scipy实践" class="headerlink" title="Scipy实践"></a>Scipy实践</h1><p><a href="https://docs.scipy.org/doc/scipy/reference/sparse.html" target="_blank" rel="noopener">官方文档</a></p><h2 id="稀疏矩阵类"><a href="#稀疏矩阵类" class="headerlink" title="稀疏矩阵类"></a>稀疏矩阵类</h2><p><img src="/2018/05/01/sparseMatrix/5.png" width="400" height="300" alt="none" align="center"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新建一个稀疏矩阵</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> sparse</span><br><span class="line">sparse_array = np.array([</span><br><span class="line">    [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">8</span>,<span class="number">11</span>],</span><br><span class="line">    [<span class="number">0</span>,<span class="number">0</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">    [<span class="number">3</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">    [<span class="number">18</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 转换为coo存储格式</span></span><br><span class="line">sparse_coo = sparse.coo_matrix(sparse_array)</span><br><span class="line"><span class="comment"># 查看类型</span></span><br><span class="line">type(sparse_coo)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Output:scipy.sparse.coo.coo_matrix</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># scipy.sparse.spmatrix 是所有存储格式类的基类</span></span><br><span class="line"><span class="comment"># 它有两个属性，'nnz'非0元素的个数，'shape'矩阵的shape</span></span><br><span class="line">print(sparse_coo.nnz)</span><br><span class="line">print(sparse_coo.shape)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Output: <span class="number">9</span></span><br><span class="line">(<span class="number">4</span>, <span class="number">6</span>)</span><br></pre></td></tr></table></figure><p>比较常用的method：tocoo(),tocsc(),tocsr()….</p><h2 id="构建稀疏矩阵"><a href="#构建稀疏矩阵" class="headerlink" title="构建稀疏矩阵"></a>构建稀疏矩阵</h2><ol><li>eye(m[, n, k, dtype, format]) - 构建一个对角线值1为1的矩阵</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sparse.eye(<span class="number">4</span>,<span class="number">4</span>).toarray()</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">array([[ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure><p>2.identity(n[, dtype, format]) - 返回一个nxn的对角线矩阵，跟上面那个差不多<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sparse.identity(<span class="number">4</span>, dtype=<span class="string">'float32'</span>, format=<span class="string">'csc'</span>).toarray()</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">array([[ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>]], dtype=float32)</span><br></pre></td></tr></table></figure></p><p>3.kron(A, B[, format]) - 计算稀疏矩阵A和B的克罗内克积<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># format指定返回的压缩格式</span></span><br><span class="line">sparse.kron(sparse_coo,sparse_coo,format=<span class="string">'csc'</span>).toarray()</span><br></pre></td></tr></table></figure></p><p>4.kronsum(A, B[, format]) - 计算矩阵的克罗内克和</p><p>5.diags(diagonals[, offsets, shape, format, dtype])    - 从对角线构建一个稀疏矩阵<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用起来比较灵活，有很多种定义的方法，但是都要使用diagnols和offset来定义（offset是离对角线的距离）</span></span><br><span class="line">offset = [<span class="number">-1</span>,<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">diagnols = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">-2</span>,<span class="number">1</span>,<span class="number">-2</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>]]</span><br><span class="line">sparse.diags(diagnols,offset).toarray()</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">array([[<span class="number">-2.</span>,  <span class="number">2.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">3.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">2.</span>, <span class="number">-2.</span>,  <span class="number">3.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure></p><p>6.spdiags(data, diags, m, n[, format]) - 利用原矩阵和offset构建对角矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># offset长度要和行数对应，把每一行放到了第i个offset的对角线上。</span></span><br><span class="line">sparse.spdiags(sparse_array,[<span class="number">0</span>,<span class="number">-1</span>,<span class="number">1</span>,<span class="number">2</span>],<span class="number">6</span>,<span class="number">6</span>).toarray()</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">array([[ <span class="number">1</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>],</span><br><span class="line">       [ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">0</span>,  <span class="number">0</span>],</span><br><span class="line">       [ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>],</span><br><span class="line">       [ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">4</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>],</span><br><span class="line">       [ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">3</span>,  <span class="number">8</span>,  <span class="number">0</span>],</span><br><span class="line">       [ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">1</span>, <span class="number">11</span>]])</span><br></pre></td></tr></table></figure><p>7.scipy.sparse.block_diag - 根据提供的子矩阵构建块矩阵<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">A = coo_matrix([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">B = coo_matrix([[<span class="number">5</span>], [<span class="number">6</span>]])</span><br><span class="line">C = coo_matrix([[<span class="number">7</span>]])</span><br><span class="line">block_diag((A, B, C)).toarray()</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">      [<span class="number">3</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">      [<span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>],</span><br><span class="line">      [<span class="number">0</span>, <span class="number">0</span>, <span class="number">6</span>, <span class="number">0</span>],</span><br><span class="line">      [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">7</span>]])</span><br></pre></td></tr></table></figure></p><p>8.scipy.sparse.tril - 得到距离中心对角线距离为k的对角线以下的矩阵<br>9.scipy.sparse.triu - 得到距离中心对角线距离为k的对角线以上的矩阵<br>10.scipy.sparse.bmat - 根据子矩阵建立稀疏块矩阵<br>11.scipy.sparse.hstack - 在水平方向连接两个稀疏矩阵<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">sparse_coo.toarray()</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">array([[ 1,  0,  0,  0,  8, 11],</span><br><span class="line">       [ 0,  0,  4,  3,  1,  0],</span><br><span class="line">       [ 3,  0,  0,  0,  0,  0],</span><br><span class="line">       [18,  0,  0,  1,  0,  0]])</span><br><span class="line"></span><br><span class="line">sparse_coo_2.toarray()</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">array([[2, 2],</span><br><span class="line">       [0, 0],</span><br><span class="line">       [0, 1],</span><br><span class="line">       [1, 2]])</span><br><span class="line"></span><br><span class="line">sparse.hstack([sparse_coo,sparse_coo_2]).toarray()</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line"># 可以看到是水平方向上的拼接,所以两个稀疏矩阵的行数要相等</span><br><span class="line">array([[ 1,  0,  0,  0,  8, 11,  2,  2],</span><br><span class="line">       [ 0,  0,  4,  3,  1,  0,  0,  0],</span><br><span class="line">       [ 3,  0,  0,  0,  0,  0,  0,  1],</span><br><span class="line">       [18,  0,  0,  1,  0,  0,  1,  2]], dtype=int64)</span><br></pre></td></tr></table></figure></p><p>11.scipy.sparse.vstack - 在竖直方向上拼接两个稀疏矩阵，与上面的同理，vstack要求两个矩阵的列数(特征数)相等。<br>12.scipy.sparse.rand - 生成具有均匀分布值的给定形状和密度的稀疏矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 前两个参数是shape,density是密度，密度为0意味着没有非0项(全为0)，密度为1意味着全矩阵(没有0)。</span></span><br><span class="line">sparse.rand(<span class="number">3</span>,<span class="number">3</span>,<span class="number">0.3</span>).toarray()</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">array([[ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.03750317</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.16852654</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ]])</span><br><span class="line"></span><br><span class="line">sparse.rand(<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>).toarray()</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">array([[ <span class="number">0.22234732</span>,  <span class="number">0.37775149</span>,  <span class="number">0.70501589</span>],</span><br><span class="line">       [ <span class="number">0.87329993</span>,  <span class="number">0.97699362</span>,  <span class="number">0.62989565</span>],</span><br><span class="line">       [ <span class="number">0.23061425</span>,  <span class="number">0.58412748</span>,  <span class="number">0.3593626</span> ]])</span><br><span class="line"></span><br><span class="line">sparse.rand(<span class="number">3</span>,<span class="number">3</span>,<span class="number">0</span>).toarray()</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">array([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure><p>13.find() - 得到保存的存储数组(非0值和索引)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sparse.find(sparse_coo)</span><br><span class="line"></span><br><span class="line">Output：</span><br><span class="line">(array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], dtype=int32),</span><br><span class="line"> array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>], dtype=int32),</span><br><span class="line"> array([ <span class="number">1</span>,  <span class="number">3</span>, <span class="number">18</span>,  <span class="number">4</span>,  <span class="number">3</span>,  <span class="number">1</span>,  <span class="number">8</span>,  <span class="number">1</span>, <span class="number">11</span>]))</span><br></pre></td></tr></table></figure></p><h1 id="稀疏矩阵的存储和加载"><a href="#稀疏矩阵的存储和加载" class="headerlink" title="稀疏矩阵的存储和加载"></a>稀疏矩阵的存储和加载</h1><p>1.scipy.sparse.save_npz - 将稀疏矩阵存储为.npz格式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用起来很简单</span></span><br><span class="line">sparse.save_npz(<span class="string">'YourFileName.npz'</span>,YourSparseMatrix)</span><br></pre></td></tr></table></figure><p>2.scipy.sparse.load_npz<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 直接load保存的.npz文件即可</span></span><br><span class="line">sparse.load_npz(<span class="string">'YourFileName.npz'</span>)</span><br></pre></td></tr></table></figure></p><p>在使用稀疏矩阵格式如coo,csc等格式的时候，根据官方建议，最好不要使用numpy库里的方法，而是使用sparse库自带的方法，如果要使用numpy的方法，先转换格式。</p><h1 id="稀疏表达的意义"><a href="#稀疏表达的意义" class="headerlink" title="稀疏表达的意义"></a>稀疏表达的意义</h1><p>根据周志华《机器学习》中所说：<br>当样本数据是一个稀疏矩阵时，对学习任务来说会有不少的好处，例如很多问题变得线性可分，储存更为高效等。这便是稀疏表示与字典学习的基本出发点。稀疏矩阵即矩阵的每一行列中都包含了大量的零元素，且这些零元素没有出现在同一行/列，对于一个给定的稠密矩阵，若我们能通过某种方法找到其合适的稀疏表示，则可以使得学习任务更加简单高效，我们称之为稀疏编码（sparse coding）或字典学习（dictionary learning）。</p><p><img src="/2018/05/01/sparseMatrix/9.png" width="600" height="200" alt="none" align="center"></p><p><img src="/2018/05/01/sparseMatrix/10.png" width="700" height="400" alt="none" align="center"></p><h1 id="稀疏矩阵的场景"><a href="#稀疏矩阵的场景" class="headerlink" title="稀疏矩阵的场景"></a>稀疏矩阵的场景</h1><p>1.例如CTR预估中会有大量的category特征和id特征，one-hot以后维度会非常高<br>2.例如对文本特征的提取过程<br>3.图像在小波变换，梯度算子下是（近似）稀疏的；<br>4.分类过程中需要输入在不同的基下面表达不同，这是稀疏性；<br>5.Deep Learning在不断地提出feature的过程也是稀疏性；<br>6.推荐系统背后是因为用户产品评价是一个低秩矩阵；</p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p><a href="https://blog.csdn.net/sunhuaqiang1/article/details/51296803" target="_blank" rel="noopener">孙华强</a></p><p><a href="https://blog.csdn.net/pipisorry/article/details/41762945" target="_blank" rel="noopener">某小皮</a></p><p><a href="https://www.zhihu.com/question/26602796/answer/33431062" target="_blank" rel="noopener">知乎</a></p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;稀疏矩阵科学计算意义，适用场景，scipy库的使用&lt;/center&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow-regression</title>
    <link href="http://yoursite.com/2018/04/24/tensorflow-regression/"/>
    <id>http://yoursite.com/2018/04/24/tensorflow-regression/</id>
    <published>2018-04-24T08:10:34.000Z</published>
    <updated>2018-06-07T06:05:22.082Z</updated>
    
    <content type="html"><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>这里使用天池糖尿病比赛的数据,数据量为6633条数据,90维。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入数据</span></span><br><span class="line">train = pd.read_csv(<span class="string">'trainSet.csv'</span>)</span><br><span class="line">y = train[<span class="string">'血糖'</span>]</span><br><span class="line">train = train.drop(<span class="string">'血糖'</span>,axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 填补空值</span></span><br><span class="line">train = train.fillna(method = <span class="string">'bfill'</span>)</span><br><span class="line">train = train.fillna(method = <span class="string">'pad'</span>)</span><br><span class="line"><span class="comment"># 归一化</span></span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line">train = scaler.fit_transform(train)</span><br><span class="line"><span class="comment"># 定义占位符</span></span><br><span class="line">trainSet = tf.placeholder(tf.float32,shape=[<span class="keyword">None</span>,<span class="number">90</span>])</span><br><span class="line">train_y = tf.placeholder(tf.float32,shape=[<span class="keyword">None</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义线性回归的变量 W and b</span></span><br><span class="line">W = tf.Variable(tf.random_normal(shape=[<span class="number">90</span>,<span class="number">1</span>]))</span><br><span class="line">b = tf.Variable(tf.random_normal(shape=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义相关参数</span></span><br><span class="line">learning_rate = <span class="number">0.05</span></span><br><span class="line">batch_size = <span class="number">50</span></span><br><span class="line">iterations = <span class="number">200</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义目标函数</span></span><br><span class="line">f = tf.add(tf.matmul(trainSet,W),b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明损失函数，这里用平均方差作为损失函数</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(train_y-f))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明优化函数,梯度下降</span></span><br><span class="line">opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line">train_step = opt.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始迭代训练</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="comment"># 可视化</span></span><br><span class="line">    writer = tf.summary.FileWriter(<span class="string">""</span>, sess.graph)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(iterations):</span><br><span class="line">        sess.run(train_step,feed_dict=&#123;train_y:y,trainSet:train&#125;)</span><br><span class="line">        <span class="comment"># 打印当前的损失</span></span><br><span class="line">        temp_loss = sess.run(loss,feed_dict=&#123;train_y:y,trainSet:train&#125;)</span><br><span class="line">        print(temp_loss)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure></p><p>输出<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">7.05357</span></span><br><span class="line"><span class="number">6.78493</span></span><br><span class="line"><span class="number">6.57317</span></span><br><span class="line"><span class="number">6.37899</span></span><br><span class="line"><span class="number">6.19179</span></span><br><span class="line"><span class="number">6.01143</span></span><br><span class="line"><span class="number">5.83728</span></span><br><span class="line"><span class="number">5.67136</span></span><br><span class="line"><span class="number">5.51442</span></span><br><span class="line"><span class="number">5.36657</span></span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line"><span class="number">2.13792</span></span><br><span class="line"><span class="number">2.13694</span></span><br><span class="line"><span class="number">2.13595</span></span><br><span class="line"><span class="number">2.13495</span></span><br><span class="line"><span class="number">2.13402</span></span><br><span class="line"><span class="number">2.13303</span></span><br><span class="line"><span class="number">2.13209</span></span><br></pre></td></tr></table></figure></p><p>Tensorboard 计算图<br><img src="/2018/04/24/tensorflow-regression/1.png" alt=""></p><h1 id="Tensorflow命名空间"><a href="#Tensorflow命名空间" class="headerlink" title="Tensorflow命名空间"></a>Tensorflow命名空间</h1><p>可以看到TensorBoard比较混乱，可以使用name_scope“命名空间”来对代码进行整理<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入数据</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"input_preProcess"</span>):</span><br><span class="line">    train = pd.read_csv(<span class="string">'trainSet.csv'</span>)</span><br><span class="line">    y = train[<span class="string">'血糖'</span>]</span><br><span class="line">    train = train.drop(<span class="string">'血糖'</span>,axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 填补空值</span></span><br><span class="line">    train = train.fillna(method = <span class="string">'bfill'</span>)</span><br><span class="line">    train = train.fillna(method = <span class="string">'pad'</span>)</span><br><span class="line">    <span class="comment"># 归一化</span></span><br><span class="line">    scaler = MinMaxScaler()</span><br><span class="line">    train = scaler.fit_transform(train)</span><br><span class="line">    <span class="comment"># 定义占位符</span></span><br><span class="line">    trainSet = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">90</span>])</span><br><span class="line">    train_y = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>])</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"inference"</span>):</span><br><span class="line">    <span class="comment"># 定义线性回归的变量 W and b</span></span><br><span class="line">    W = tf.Variable(tf.random_normal(shape=[<span class="number">90</span>,<span class="number">1</span>]))</span><br><span class="line">    b = tf.Variable(tf.random_normal(shape=[<span class="number">1</span>]))</span><br><span class="line">    <span class="comment"># 定义相关参数</span></span><br><span class="line">    learning_rate = <span class="number">0.05</span></span><br><span class="line">    batch_size = <span class="number">50</span></span><br><span class="line">    iterations = <span class="number">200</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"object_function"</span>):</span><br><span class="line">    <span class="comment"># 定义目标函数</span></span><br><span class="line">    f = tf.add(tf.matmul(trainSet,W),b)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"Loss"</span>):</span><br><span class="line">    <span class="comment"># 声明损失函数，这里用平均方差作为损失函数</span></span><br><span class="line">    loss = tf.reduce_mean(tf.square(train_y-f))</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"optimizer"</span>):</span><br><span class="line">    <span class="comment"># 声明优化函数,梯度下降</span></span><br><span class="line">    opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line">    train_step = opt.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始迭代训练</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="comment"># 可视化</span></span><br><span class="line">    writer = tf.summary.FileWriter(<span class="string">""</span>, sess.graph)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(iterations):</span><br><span class="line">        sess.run(train_step,feed_dict=&#123;train_y:y,trainSet:train&#125;)</span><br><span class="line">        <span class="comment"># 打印当前的损失</span></span><br><span class="line">        temp_loss = sess.run(loss,feed_dict=&#123;train_y:y,trainSet:train&#125;)</span><br><span class="line">        print(temp_loss)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure></p><p>更新后的TensorBoard视图:<br><img src="/2018/04/24/tensorflow-regression/2.png" alt=""></p><h1 id="TF实现MNIST手写数字识别"><a href="#TF实现MNIST手写数字识别" class="headerlink" title="TF实现MNIST手写数字识别"></a>TF实现MNIST手写数字识别</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入数据</span></span><br><span class="line">MNIST = input_data.read_data_sets(<span class="string">"data"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义相关超参</span></span><br><span class="line">eta = <span class="number">0.01</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">n_epochs = <span class="number">25</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为输入和输出定义placeholder</span></span><br><span class="line"><span class="comment"># 因为每个在MNIST中的像素是28*28 = 784</span></span><br><span class="line"><span class="comment"># 所以每一个图像都有784个特征，是一个1x784的张量</span></span><br><span class="line">X = tf.placeholder(<span class="string">'float32'</span>,[batch_size,<span class="number">784</span>])</span><br><span class="line">y = tf.placeholder(<span class="string">'float32'</span>,[batch_size,<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建要调整的参数变量 weights and bias</span></span><br><span class="line"><span class="comment"># 784，10 是因为要输出一个长度为10的结果向量</span></span><br><span class="line">w = tf.Variable(tf.random_normal(shape=[<span class="number">784</span>,<span class="number">10</span>],stddev=<span class="number">0.1</span>), name=<span class="string">'weights'</span>)</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">10</span>]), name=<span class="string">'bias'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义假设函数</span></span><br><span class="line">logits = tf.matmul(X,w) + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y)</span><br><span class="line">loss = tf.reduce_mean(entropy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度下降</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=eta).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化变量</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里batch_size是每一次训练多少个样本，这里要算每一轮要训练多少次</span></span><br><span class="line">n_batches = int(MNIST.train.num_examples/batch_size)</span><br><span class="line"><span class="comment"># 训练多少轮</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_batches):</span><br><span class="line">        <span class="comment"># 该循环的每个步骤中，我们都会随机抓取训练数据中的batch_size个批处理数据点，然后我们用这些数据点作为参数替换之前的占位符来运行train_step</span></span><br><span class="line">        X_batch,Y_batch = MNIST.train.next_batch(batch_size)</span><br><span class="line">        sess.run([optimizer,loss],&#123;X:X_batch,y:Y_batch&#125;)</span><br><span class="line"><span class="comment"># 测试模型</span></span><br><span class="line">corrects = <span class="number">0</span></span><br><span class="line">n_batches = int(MNIST.test.num_examples/batch_size)</span><br><span class="line">total_correct_preds = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_batches):</span><br><span class="line">    X_batch, Y_batch = MNIST.test.next_batch(batch_size)</span><br><span class="line">    _,loss_batch,logits_batch = sess.run([optimizer,loss,logits],feed_dict=&#123;X:X_batch,y:Y_batch&#125;)</span><br><span class="line">    preds = tf.nn.softmax(logits_batch)</span><br><span class="line">    corrects_preds = tf.equal(tf.argmax(preds,<span class="number">1</span>),tf.argmax(Y_batch,<span class="number">1</span>))</span><br><span class="line">    print(sess.run(corrects_preds))</span><br><span class="line">    accuracy = tf.reduce_sum(tf.cast(corrects_preds,tf.float32))</span><br><span class="line">    print(sess.run(accuracy))</span><br><span class="line">    total_correct_preds += sess.run(accuracy)</span><br><span class="line">print(<span class="string">'Accuracy'</span>,total_correct_preds/MNIST.test.num_examples)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Output:Accuracy <span class="number">0.9076</span></span><br></pre></td></tr></table></figure><p>(github挂了看不了文档，未完待续)</p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p><a href="https://blog.csdn.net/qq_33297776/article/details/79339684" target="_blank" rel="noopener">GuanghaoChen_NEU</a></p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;tensorflow 实现 线性回归 Lasso回归等&lt;/center&gt;
    
    </summary>
    
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>TF_CookBook-Chapter1</title>
    <link href="http://yoursite.com/2018/04/24/cookbook/"/>
    <id>http://yoursite.com/2018/04/24/cookbook/</id>
    <published>2018-04-24T03:20:37.000Z</published>
    <updated>2018-04-24T07:02:57.330Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Tensorflow-基础"><a href="#Tensorflow-基础" class="headerlink" title="Tensorflow 基础"></a>Tensorflow 基础</h1><p>首先要明确tensorflow的工作原理</p><h1 id="Tensorflow-工作原理"><a href="#Tensorflow-工作原理" class="headerlink" title="Tensorflow 工作原理"></a>Tensorflow 工作原理</h1><p>首先准备数据集，变量，占位符，选定机器学习模型，损失函数，优化函数，然后模型训练，调参。Tensorflow通过<strong>计算图</strong>来实现上述过程，这些计算图是<strong>有向无环图</strong>，并且<strong>支持并行运算</strong></p><h1 id="Tensor-张量"><a href="#Tensor-张量" class="headerlink" title="Tensor 张量"></a>Tensor 张量</h1><p>Tensorflow从字面看就是基于tensor，也是TF中一个非常基础和重要的概念“张量”来构建的，<strong>张量包含了0到任意维度的量，其中，0维的叫做常数，1维的叫做向量，二维叫做矩阵，多维度的就直接叫张量</strong>，而tensorflow的整个流程“计算图”也是基于张量来操作的。注意，仅仅创建一个张量，其不会被添加到计算图中，只有把张量赋值给一个变量或者占位符<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将任意维度的numpy数组转为张量</span></span><br><span class="line">converted = tf.convert_to_sensor(numpy数组)</span><br><span class="line"><span class="comment"># 创建指定维度的0张量</span></span><br><span class="line">zero_tar = tf.zeros([行数，列数])</span><br><span class="line"><span class="comment"># 创建指定维度单位张量</span></span><br><span class="line">ones_tar = tf.ones([行数，列数])</span><br><span class="line"><span class="comment"># 创建指定维度的常数填充的张量</span></span><br><span class="line">filled_tar = tf.fill([行数，列数]，要填充的值)</span><br><span class="line"><span class="comment"># 创建一个常数张量</span></span><br><span class="line">constant_tar = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line"><span class="comment"># 创建一个与给定张量形状一样的张良</span></span><br><span class="line">zeros_similar = tf.zeros_like(给定张量)</span><br><span class="line">ones_similar = tf.ones_like(给定张量)</span><br><span class="line"><span class="comment"># 创建序列张量 其中前两个值是起始和结束值（包含在内），最后一个值生成的张量长度</span></span><br><span class="line">linear_tsr = tf.linspace(<span class="number">0.</span>,<span class="number">1.</span>,<span class="number">4</span>)</span><br><span class="line">&gt;&gt;&gt;&gt;[ <span class="number">0.</span>          <span class="number">0.33333334</span>  <span class="number">0.66666669</span>  <span class="number">1.</span>        ]</span><br><span class="line"><span class="comment"># 类似用法 range,包含起始值，不包含结束值，最后一个参数是间隔</span></span><br><span class="line">integer_seq_tsr = tf.range(<span class="number">6</span>,<span class="number">15</span>,<span class="number">3</span>)</span><br><span class="line">&gt;&gt;&gt;&gt;[ <span class="number">6</span>  <span class="number">9</span> <span class="number">12</span>]</span><br><span class="line"><span class="comment"># 创建随机张量</span></span><br><span class="line">randunif_tsr = tf.random_uniform([行数，列数],minval=<span class="number">0</span>,maxval=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#eg.</span></span><br><span class="line"><span class="comment"># randomTsr = tf.random_uniform([4,5],0,2)</span></span><br><span class="line"><span class="comment">#[[ 1.95798659  1.14590573  0.71785665  1.73604393  0.59747076]</span></span><br><span class="line"><span class="comment"># [ 1.58351803  0.64294863  1.01357007  0.17978501  0.50106215]</span></span><br><span class="line"><span class="comment"># [ 1.66894627  1.26074719  0.00528693  1.53832388  0.03066635]</span></span><br><span class="line"><span class="comment"># [ 0.84991193  1.98108554  1.35822701  1.57819223  1.25322342]]</span></span><br><span class="line"><span class="comment"># 创建正太分布的随机数</span></span><br><span class="line">randnorm = tf.random_normal([row_dim,col_dim],mean=<span class="number">0.</span>,stddev=<span class="number">1.</span>)</span><br></pre></td></tr></table></figure></p><p><strong>对张量的每个元素进行操作</strong><br>主要有四个函数：add(),sub(),mul(),div(),参数为张量<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># div() and truediv()</span></span><br><span class="line"><span class="comment">#这两个函数的主要区别是div()返回的是跟原数据类型一致的结果，本来是整数型那么返回的结果也会转为整数型</span></span><br><span class="line">print(tf.truediv(张量A))</span><br><span class="line"><span class="comment">#下面还有一些比较重要的函数</span></span><br><span class="line">tf.abs() <span class="comment"># 张量的绝对值</span></span><br><span class="line">tf.ceil() <span class="comment"># 向上取整</span></span><br><span class="line">tf.cos() <span class="comment"># 余弦值</span></span><br><span class="line">tf.exp() <span class="comment"># e的指数</span></span><br><span class="line">tf.floor() <span class="comment"># 向下取整</span></span><br><span class="line">tf.log() <span class="comment"># 自然对数</span></span><br><span class="line">tf.maximum() <span class="comment"># 比较两个张量的大小，必须shape相同</span></span><br><span class="line">tf.minimum() <span class="comment"># 同理</span></span><br><span class="line">tf.neg() <span class="comment"># 负数</span></span><br><span class="line">tf.pow() <span class="comment"># (张量，次方)</span></span><br><span class="line">tf.round() <span class="comment"># 四舍五入</span></span><br><span class="line">tf.sign() <span class="comment"># 返回符号 -1,0,1,返回同维度，可以来判断张量中正负值个数</span></span><br><span class="line">tf.square() <span class="comment"># 平方</span></span><br></pre></td></tr></table></figure></p><h1 id="Placeholder-占位符"><a href="#Placeholder-占位符" class="headerlink" title="Placeholder 占位符"></a>Placeholder 占位符</h1><p>常常和feed_dict一起用，它表示tensorflow计算图中的输入和输出，要和Variable区分开,PlaceHolder可以接受的数据类型有<strong>:Python scalars, strings, lists, numpy ndarrays, or TensorHandles</strong>,注意，没有Tensor类型！<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.string)</span><br><span class="line">y = tf.placeholder(tf.int32)</span><br><span class="line">z = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"><span class="comment"># feed_dict指明喂给各个placeholder的数据</span></span><br><span class="line">    output = sess.run(x, feed_dict=&#123;x: <span class="string">'Test String'</span>, y: <span class="number">123</span>, z: <span class="number">45.67</span>&#125;)</span><br></pre></td></tr></table></figure></p><h1 id="Variable-变量"><a href="#Variable-变量" class="headerlink" title="Variable 变量"></a>Variable 变量</h1><p>需要注意的地方是变量一定要初始化才行<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">my_var = tf.Variable(tf.zeros[<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="comment"># 变量需要初始化</span></span><br><span class="line">initialize_op = tf.global_variables_initializer()</span><br><span class="line">sess.run(initialize_op)</span><br></pre></td></tr></table></figure></p><h1 id="Session-图会话"><a href="#Session-图会话" class="headerlink" title="Session 图会话"></a>Session 图会话</h1><p>在启动图会话之前，tensorflow的所有操作都是在往计算图中添砖加瓦，并没有产生实际的运算，只有当启动一个会话以后(session -&gt; run),才开始执行整个计算图。<br>有两种调用会话的方式<br><strong>1. 明确的调用会话的生成函数和关闭会话函数</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create a session</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># use this session to run a result</span></span><br><span class="line">sess.run(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># close this session, release memeory</span></span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure></p><p>这种方式需要手动的调用关闭sess的函数来释放资源。<br><strong>2. 上下文管理机制自动释放所有资源</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建会话，并通过上下文机制管理器管理该会话</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(...)</span><br><span class="line"><span class="comment"># 不需要再调用"Session.close()"</span></span><br><span class="line"><span class="comment"># 在退出with statement时，会话关闭和资源释放已自动完成</span></span><br></pre></td></tr></table></figure></p><p><strong>3.默认计算图</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default():</span><br><span class="line">    <span class="comment"># result为某个张量</span></span><br><span class="line">    print(result.eval())</span><br></pre></td></tr></table></figure></p><p>以上，最常用的还是<strong>方式二</strong>，但这三种方式都可以通过<strong>ConfigProto Protocol Buffer</strong>来配置需要生成的会话，如并行线程数、GPU分配策略、运算超时时间 等参数，最常用的两个是<strong>allow_soft_placement</strong>和<strong>log_device_placement</strong>.<br>ConfigProto配置方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">config = tf.ConfigProto(allow_soft_placement=<span class="keyword">True</span>, </span><br><span class="line">                        log_device_placement=<span class="keyword">True</span>)</span><br><span class="line">sess1 = tf.InteractiveSession(config=config)</span><br><span class="line">sess2 = tf.Session(config=config)</span><br><span class="line"><span class="comment">#allow_soft_placement，布尔型，一般设置为True，很好的支持多GPU</span></span><br><span class="line"><span class="comment">#或者不支持GPU时自动将运算放到CPU上。</span></span><br><span class="line"><span class="comment">#log_device_placement，布尔型，为True时日志将会记录每个节点被</span></span><br><span class="line"><span class="comment">#安排在了哪个设备上以方便调试。在生产环境下，通常设置为False可以减少日志量。</span></span><br></pre></td></tr></table></figure></p><h1 id="矩阵操作"><a href="#矩阵操作" class="headerlink" title="矩阵操作"></a>矩阵操作</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 矩阵加减法</span></span><br><span class="line">sess.run(A+B)</span><br><span class="line"><span class="comment"># 矩阵乘法</span></span><br><span class="line">sess.run(tf.matmul(A,B))</span><br><span class="line"><span class="comment"># 矩阵转置</span></span><br><span class="line">sess.run(tf.ranspose(A))</span><br><span class="line"><span class="comment"># 矩阵行列式</span></span><br><span class="line">sess.run(tf.matrix_determinant(D))</span><br><span class="line"><span class="comment"># 矩阵的逆矩阵</span></span><br><span class="line">sess.run(tf.matrix_inverse(D))</span><br><span class="line"><span class="comment"># 矩阵分解,矩阵需要为对称正定矩阵或者可进行LU分解</span></span><br><span class="line">sess.run(tf.cholesky(identity_matrix))</span><br><span class="line"><span class="comment"># 矩阵的特征分解（分解为特征值和特征向量），第一行为特征值，剩下的向量是对应的向量</span></span><br><span class="line">sess.run(tf.self_adjoint_eig(D))</span><br></pre></td></tr></table></figure><h1 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h1><p>例如:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addTwo</span><span class="params">(A,B)</span></span></span><br><span class="line"><span class="function"><span class="title">return</span> <span class="title">tf</span>.<span class="title">add</span><span class="params">(A,B)</span></span></span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(sess.run<span class="params">(addTwo<span class="params">(<span class="number">2</span>,<span class="number">3</span>)</span>)</span>)</span></span></span><br></pre></td></tr></table></figure></p><p>输出<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">5</span></span><br></pre></td></tr></table></figure></p><h1 id="综合例子"><a href="#综合例子" class="headerlink" title="综合例子"></a>综合例子</h1><p><strong>占位符和tf.Variable可以直接run，但是涉及到placeholder就要给出相应的feed_dict</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义张量 4x3 and 3x2</span></span><br><span class="line">A = tf.random_uniform([<span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line">B = tf.random_uniform([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line"><span class="comment"># 生成2行4列服从标准正态分布的随机数</span></span><br><span class="line">A_np = np.random.randn(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">B_np = np.random.randn(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 定义占位符</span></span><br><span class="line">A_placeholder = tf.placeholder(<span class="string">'float32'</span>, shape=[<span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line">B_placeholder = tf.placeholder(<span class="string">'float32'</span>, shape=[<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line"><span class="comment"># 注意tf中的矩阵乘法是tf.matmul不是tf.multiply</span></span><br><span class="line">mul_1 = tf.matmul(A, B)</span><br><span class="line">mul_2 = tf.matmul(A_placeholder, B_placeholder)</span><br><span class="line"><span class="comment">#定义自定义函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mulTwoMatrix</span><span class="params">(M_a,M_b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.matmul(M_a,M_b)</span><br><span class="line"><span class="comment"># 定义session</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 直接打印张量A</span></span><br><span class="line">    print(sess.run(A))</span><br><span class="line">    <span class="comment"># 打印placeholder_A,由A_np赋值</span></span><br><span class="line">    print(sess.run(A_placeholder, feed_dict=&#123;A_placeholder: A_np&#125;))</span><br><span class="line">    <span class="comment"># 打印张量A乘以张量B</span></span><br><span class="line">    print(sess.run(mul_1))</span><br><span class="line">    <span class="comment"># 打印placeholder_A 乘以 placeholder_B</span></span><br><span class="line">    print(sess.run(mul_2, feed_dict=&#123;A_placeholder: A_np, B_placeholder: B_np&#125;))</span><br><span class="line">    <span class="comment"># 打印张量A的四舍五入的结果</span></span><br><span class="line">    print(sess.run(tf.round(A)))</span><br><span class="line">    <span class="comment"># 调用函数打印placeholder_A 乘以 placeholder_B,结果应该是一样的</span></span><br><span class="line">    print(sess.run(mulTwoMatrix(A_placeholder,B_placeholder),feed_dict=&#123;A_placeholder: A_np, B_placeholder: B_np&#125;))</span><br></pre></td></tr></table></figure></p><p>输出<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 直接打印张量A</span></span><br><span class="line">[[ <span class="number">0.45307124</span>  <span class="number">0.62509346</span>  <span class="number">0.25148225</span>]</span><br><span class="line"> [ <span class="number">0.41434121</span>  <span class="number">0.58660126</span>  <span class="number">0.9465214</span> ]</span><br><span class="line"> [ <span class="number">0.37218738</span>  <span class="number">0.91929305</span>  <span class="number">0.71940351</span>]</span><br><span class="line"> [ <span class="number">0.84926152</span>  <span class="number">0.63980663</span>  <span class="number">0.27016711</span>]]</span><br><span class="line"><span class="comment"># 打印placeholder_A,由A_np赋值</span></span><br><span class="line">[[ <span class="number">0.61854458</span>  <span class="number">0.97045314</span> <span class="number">-0.75675184</span>]</span><br><span class="line"> [ <span class="number">0.39336792</span>  <span class="number">0.64061081</span> <span class="number">-0.26375616</span>]</span><br><span class="line"> [<span class="number">-0.27992743</span> <span class="number">-0.12586813</span> <span class="number">-0.40246865</span>]</span><br><span class="line"> [ <span class="number">0.54720771</span>  <span class="number">0.61397451</span>  <span class="number">0.66880828</span>]]</span><br><span class="line"><span class="comment"># 打印张量A乘以张量B</span></span><br><span class="line">[[ <span class="number">0.7251178</span>   <span class="number">0.7453838</span> ]</span><br><span class="line"> [ <span class="number">0.40002131</span>  <span class="number">0.39979655</span>]</span><br><span class="line"> [ <span class="number">0.18994027</span>  <span class="number">0.32228675</span>]</span><br><span class="line"> [ <span class="number">0.96875006</span>  <span class="number">1.10299659</span>]]</span><br><span class="line"><span class="comment"># 打印placeholder_A 乘以 placeholder_B</span></span><br><span class="line">[[<span class="number">-0.46408546</span> <span class="number">-2.08722115</span>]</span><br><span class="line"> [<span class="number">-0.02081895</span> <span class="number">-1.0122509</span> ]</span><br><span class="line"> [<span class="number">-0.20734093</span> <span class="number">-0.50428039</span>]</span><br><span class="line"> [ <span class="number">0.76150823</span>  <span class="number">0.45751405</span>]]</span><br><span class="line"><span class="comment"># 打印张量A的四舍五入的结果</span></span><br><span class="line">[[ <span class="number">0.</span>  <span class="number">1.</span>  <span class="number">1.</span>]</span><br><span class="line"> [ <span class="number">0.</span>  <span class="number">1.</span>  <span class="number">1.</span>]</span><br><span class="line"> [ <span class="number">1.</span>  <span class="number">1.</span>  <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">1.</span>  <span class="number">1.</span>  <span class="number">1.</span>]]</span><br><span class="line"><span class="comment"># 调用函数打印placeholder_A 乘以 placeholder_B,结果应该是一样的</span></span><br><span class="line">[[<span class="number">-0.46408546</span> <span class="number">-2.08722115</span>]</span><br><span class="line"> [<span class="number">-0.02081895</span> <span class="number">-1.0122509</span> ]</span><br><span class="line"> [<span class="number">-0.20734093</span> <span class="number">-0.50428039</span>]</span><br><span class="line"> [ <span class="number">0.76150823</span>  <span class="number">0.45751405</span>]]</span><br></pre></td></tr></table></figure></p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p><a href="https://blog.csdn.net/hanging_gardens/article/details/72784392" target="_blank" rel="noopener">hukai7190</a></p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;Tensorflow基础，张量，变量，计算图&lt;/center&gt;
    
    </summary>
    
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow-可视化</title>
    <link href="http://yoursite.com/2018/04/24/tensorboard/"/>
    <id>http://yoursite.com/2018/04/24/tensorboard/</id>
    <published>2018-04-24T03:20:37.000Z</published>
    <updated>2018-04-24T07:50:03.507Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Tensorboard"><a href="#Tensorboard" class="headerlink" title="Tensorboard"></a>Tensorboard</h1><p>Tensorboard是tensorflow中的可视化模块，可以对训练过程中的统计指标，曲线图，计算图，损失，准确度等进行可视化</p><p><a href="https://github.com/tensorflow/tensorboard/blob/master/README.md" target="_blank" rel="noopener">TensorBoard官方文档</a><br>使用起来非常简单，这里先使用之前的例子基础上进行可视化<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义张量 4x3 and 3x2</span></span><br><span class="line">A = tf.random_uniform([<span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line">C = tf.random_uniform([<span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line">B = tf.random_uniform([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line"><span class="comment"># 生成2行4列服从标准正态分布的随机数</span></span><br><span class="line">A_np = np.random.randn(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">B_np = np.random.randn(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 定义占位符</span></span><br><span class="line">A_placeholder = tf.placeholder(<span class="string">'float32'</span>, shape=[<span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line">B_placeholder = tf.placeholder(<span class="string">'float32'</span>, shape=[<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">mul_1 = tf.matmul(A, B)</span><br><span class="line">mul_2 = tf.matmul(A_placeholder, B_placeholder)</span><br><span class="line"><span class="comment">#定义自定义函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mulTwoMatrix</span><span class="params">(M_a,M_b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.matmul(M_a,M_b)</span><br><span class="line"><span class="comment"># 定义session</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 创建可视化对象</span></span><br><span class="line">    writer = tf.summary.FileWriter(<span class="string">"/"</span>, sess.graph)</span><br><span class="line">    <span class="comment"># 直接打印张量A</span></span><br><span class="line">    print(sess.run(A))</span><br><span class="line">    <span class="comment"># 打印placeholder_A,由A_np赋值</span></span><br><span class="line">    print(sess.run(A_placeholder, feed_dict=&#123;A_placeholder: A_np&#125;))</span><br><span class="line">    <span class="comment"># 打印张量A乘以张量B</span></span><br><span class="line">    print(sess.run(mul_1))</span><br><span class="line">    <span class="comment"># 打印placeholder_A 乘以 placeholder_B</span></span><br><span class="line">    print(sess.run(mul_2, feed_dict=&#123;A_placeholder: A_np, B_placeholder: B_np&#125;))</span><br><span class="line">    <span class="comment"># 打印张量A的四舍五入的结果</span></span><br><span class="line">    print(sess.run(tf.round(A)))</span><br><span class="line">    <span class="comment"># 调用函数打印placeholder_A 乘以 placeholder_B,结果应该是一样的</span></span><br><span class="line">    print(sess.run(mulTwoMatrix(A_placeholder,B_placeholder),feed_dict=&#123;A_placeholder: A_np, B_placeholder: B_np&#125;))</span><br><span class="line">    <span class="comment"># 卧槽，发现结果不一样，再次调用看看</span></span><br><span class="line">    print(sess.run(mulTwoMatrix(A_placeholder, B_placeholder), feed_dict=&#123;A_placeholder: A_np, B_placeholder: B_np&#125;))</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure></p><p>在原来的基础上添加了一个writter，这里我直接把指定的log保存位置为当前文件夹下(将会和我的.py文件在同一目录下):<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">writer = tf.summary.FileWriter(<span class="string">"/"</span>, sess.graph)</span><br></pre></td></tr></table></figure></p><p>最后别忘记关掉writter:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">writer.close()</span><br></pre></td></tr></table></figure></p><p>然后运行程序，会生成这样的一个log文件，保存你运行过程中的信息：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">events.out.tfevents<span class="number">.1524554706</span>.yangyiqingdeMacBook-Pro.local</span><br></pre></td></tr></table></figure></p><p>然后到终端，进入该文件上一级文件夹位置，执行<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我的log文件位于 cookBook 文件夹中</span></span><br><span class="line">tensorboard --logdir cookBook</span><br></pre></td></tr></table></figure></p><p>如果出现如下提示信息，说明成功<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir cookBook</span><br><span class="line">TensorBoard <span class="number">0.1</span><span class="number">.7</span> at http://yangyiqingdeMacBook-Pro.local:<span class="number">6006</span> (Press CTRL+C to quit)</span><br></pre></td></tr></table></figure></p><p>在浏览器打开TensorBoard即可<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">localhost:<span class="number">6006</span></span><br></pre></td></tr></table></figure></p><p>如下图所示，对应博客[Tensorflow基础]中最后的例子：<br><img src="/2018/04/24/tensorboard/1.png" alt="pic"></p><h1 id="主要概念"><a href="#主要概念" class="headerlink" title="主要概念"></a>主要概念</h1><p>由于先前的例子太简单，等后续关于Tensorboard再补充。</p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;Tensorflow Tensorboard 可视化&lt;/center&gt;
    
    </summary>
    
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>梯度提升树GBDT</title>
    <link href="http://yoursite.com/2018/03/20/gbdt/"/>
    <id>http://yoursite.com/2018/03/20/gbdt/</id>
    <published>2018-03-20T13:00:17.000Z</published>
    <updated>2018-03-21T12:49:13.333Z</updated>
    
    <content type="html"><![CDATA[<h1 id="提升树"><a href="#提升树" class="headerlink" title="提升树"></a>提升树</h1><p>提升树正是一种前向分步的加法模型，但是其基分类器是树模型(二叉树)，分为二叉分类树和二叉回归树，提升树往往在实践中表现非常好。提升树的模型如下:<br>$$f_M(x)=\sum_{m=1}^MT(x;\Theta)$$<br>$M$为树的个数，$\Theta$表示数的参数(在加法模型中的重要性)</p><h1 id="提升树算法"><a href="#提升树算法" class="headerlink" title="提升树算法"></a>提升树算法</h1><p>由于提升树是一种前向分步的加法模型，所以其算法步骤就是前向分布加法模型的算—<br>title: 梯度提升树GBDT<br>date: 2018-03-15 21:00:17<br>tags: 机器学习</p><h2 id="description-提升树算法，GBDT原理，损失函数"><a href="#description-提升树算法，GBDT原理，损失函数" class="headerlink" title="description: 提升树算法，GBDT原理，损失函数"></a>description: <center>提升树算法，GBDT原理，损失函数</center></h2><h1 id="提升树-1"><a href="#提升树-1" class="headerlink" title="提升树"></a>提升树</h1><p>提升树正是一种前向分步的加法模型，但是其基分类器是树模型(二叉树)，分为二叉分类树和二叉回归树，提升树往往在实践中表现非常好。提升树的模型如下:<br>$$f_M(x)=\sum_{m=1}^MT(x;\Theta)$$<br>$M$为树的个数，$\Theta$表示数的参数(在加法模型中的重要性)</p><h1 id="提升树算法-1"><a href="#提升树算法-1" class="headerlink" title="提升树算法"></a>提升树算法</h1><p>由于提升树是一种前向分步的加法模型，所以其算法步骤就是前向分布加法模型的算法步骤。<br>解决分类问题的时候，其基学习器为二叉分类树，其损失函数为指数函数，<strong>这个时候就相当于Adaboost将基分类器替换为二叉分类树。</strong><br>解决回归问题的时候，其基学习器为二叉回归树，其损失函数为平方损失函数，下面介绍一下<strong>回归提升树的算法。</strong><br>第m颗树的模型可以表示为：<br>$$f_m(x)=f_{m-1}(x)+T(x; \theta_m)$$<br>其中，$\Theta_m$是这颗树的参数，它是通过上一颗树 $f_{m-1}(x)$计算出来的<br>$$\theta_{m+1} = arg\min_{\theta_m}\sum_{i=1}^N L(y_i, f_{m-1}(x_i)+T(x_i; \theta_m)$$<br>不难发现，这里就是前向分步算法中求当前学习器的参数的过程，<strong>使得当前的损失函数最小，也就是经验风险最小化策略</strong>。<br>其中，二叉回归树模型为,就是(每个小区域的平均值*$Indicator函数$)，可以点击<a href="http://yangyiqing.cn/2018/03/09/%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">这里</a>查看关于二叉树的生成过程:<br>$$T(x; \theta)=\sum_{j=1}^J C_j I(x \in R_j)$$<br>于是,第m颗树的平方损失函数可以表示为：</p><p><center>$Loss(y,f_m(x)) = (y-f_m(x))^2$</center></p><p><center>$=(y-f_{m-1}(x)-T(x;\Theta_m))^2$</center></p><p><center>$=((y-f_{m-1}(x))-T(x;\Theta_m))^2$</center><br>不难发现，式中前两项就是当前模型拟合数据的残差，所以，对于回归提升树来说，只需要拟合当前残差即可。<strong>当前模型的残差公式:</strong><br>$$r_m = y - f_{m-1}(x)$$<br>不理解为什么$Min((y-f_{m-1}(x))-T(x;\Theta_m))^2$相当于拟合残差的可以自己去看一下《统计学习方法》中提升树相关例题。<br>所以其实这也是一种<strong>前向分步加法模型</strong>，其基分类器为<strong>二叉回归树</strong>，其损失函数为<strong>平方损失函数</strong>，求参数的过程采用了使<strong>平方损失函数最小化</strong>的策略，相当于<strong>拟合当前模型的残差</strong>。<br><strong>提升树与Adaboost的区别:</strong></p><blockquote><p>Adaboost的策略是在每一轮迭代的过程中根据误差率最小化策略来更新样本的权重，使得权重更大的样本在再次被错分的时候损失代价更大。而提升树的策略是为了减少当前模型的”残差”，<u>这里说的残差值得是当前得到的加法模型对训练集的预测和其真正label之间的差异大小</u></p></blockquote><h1 id="GBDT-梯度提升树"><a href="#GBDT-梯度提升树" class="headerlink" title="GBDT(梯度提升树)"></a>GBDT(梯度提升树)</h1><p>梯度提升树(Gradient Boosting Decison Tree)，GBDT的基分类器是CART树(简单回顾：二叉树，分类损失函数为基尼系数，回归损失函数为平方损失)。</p><p><strong>为什么要引入梯度这个概念？</strong><br>因为之前我们用到的损失函数是指数损失函数或者是平方损失函数，这个时候，对于每一轮的弱学习器来说，其损失函数最小化的策略是比较简单的。但是如果损失函数是一般函数呢？其优化策略可能非常复杂，所以引入了梯度提升算法，其利用损失函数的负梯度在当前模型的值作为”残差”的近似值:</p><p><strong>第m轮第i个样本的损失函数的负梯度(每个样本可以计算一个梯度):</strong><br>$$r_{mi} = -\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]<em>{f(x) = f</em>{m-1}\;\; (x)}$$<br>然后将负梯度作为残差的近似值，利用和提升树相同的方法，可以拟合得到一颗CART树(当前轮的弱学习器)，拟合的策略也是使当前模型残差最小，在此不再赘述。</p><p><strong>最终得到本轮的弱学习器为(就回归树而言):</strong><br>$$T(x; \theta)=\sum_{j=1}^J C_j I(x \in R_j)$$<br><strong>当前模型(强分类器)为：</strong><br>$$f(x) = f_{m-1}(x)+\sum_{j=1}^J C_j I(x \in R_j)$$<br><strong>所以，</strong>引入负梯度让我们对损失函数的处理变得统一化，算法只需要基于由负梯度得来的残差进行展开，而这个<strong>残差的具体由来，就看具体的损失函数是什么了</strong>。</p><h1 id="GBDT与LR比较"><a href="#GBDT与LR比较" class="headerlink" title="GBDT与LR比较"></a>GBDT与LR比较</h1><blockquote><p>1.由于GBDT是迭代的学习方法，且下一模型的学习是在前一模型的基础上，因而只能在训练样本的层面上进行并行化处理。关于并行化相关内容等结合Xgboost学习再详细了解。<br>2.GBDT几乎可用于所有回归问题（线性/非线性），相对logistic regression仅能用于线性回归，GBDT的适用面非常广。亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例。<br>3.GBDT 是一个加性回归模型，通过 boosting 迭代的构造一组弱学习器，相对LR的优势如不需要做特征的归一化，自动进行特征选择，模型可解释性较好，可以适应多种损失函数如 SquareLoss，LogLoss 等等。但作为非线性模型，其相对线性模型的缺点也是显然的：boosting 是个串行的过程，不能并行化，计算复杂度较高，同时其不太适合高维稀疏特征，通常采用稠密的数值特征如点击率预估中的 COEC。—引自xgboost导读与实战</p></blockquote><h1 id="GBDT损失函数"><a href="#GBDT损失函数" class="headerlink" title="GBDT损失函数"></a>GBDT损失函数</h1><p>上面我们讨论了提升树和梯度提升树，了解了引入负梯度的概念是为了解决一般损失函数最优化难的问题。还给出了为什么当损失函数是平方损失函数的时候提升树的损失函数最小化策略相当于拟合模型的当前残差。</p><p><strong>那么当损失函数是指数函数的时候，其最小化损失函数的策略是什么呢？</strong><br>其实，当Adaboost的基分类器是二叉分类树的时候，其等价于二叉提升分类树。也就是指数损失函数最小化策略和Adaboost的分类误差率最小化策略是等价的，具体推导请参见<a href="http://blog.csdn.net/google19890102/article/details/50522945" target="_blank" rel="noopener">这篇博客</a></p><h2 id="GBDT分类常用损失函数"><a href="#GBDT分类常用损失函数" class="headerlink" title="GBDT分类常用损失函数"></a>GBDT分类常用损失函数</h2><p><strong>(1) 指数损失函数</strong><br>$$L(y, f(x)) = exp(-yf(x))$$<br>其等价于Adaboost<br><strong>(2)对数损失函数</strong><br>对数损失函数在进行二分类和多分类的时候又有区别，下面会讲</p><h2 id="GBDT回归常用损失函数"><a href="#GBDT回归常用损失函数" class="headerlink" title="GBDT回归常用损失函数"></a>GBDT回归常用损失函数</h2><p><strong>(1) 均方误差</strong><br>$$L(y, f(x)) =(y-f(x))^2$$<br>最为常见的回归损失函数了<br><strong>(2) 绝对损失函数</strong><br>$$L(y, f(x)) =|y-f(x)|$$<br><strong>(3) 分位损失函数 (4) Huber损失函数</strong> 较为少见，这里不予解释。</p><h1 id="GBDT二分类"><a href="#GBDT二分类" class="headerlink" title="GBDT二分类"></a>GBDT二分类</h1><p>其损失函数类似于逻辑斯蒂回归的对数似然函数：<br>$$L(y, f(x)) = log(1+ exp(-yf(x)))$$<br>其中$y∈{1,-1},此时的残差计算公式为:<br>（我惊了！这里的公式就是显示不出来）?</p><p>$$r_{ti} = -\bigg[\frac{\partial L(y, f(x_i)))}{\partial f(x_i)}\bigg]<em>{f(x) = f</em>{t-1}\;\; (x)} = y_i/(1+exp(y_if(x_i)))$$<br>各个叶子节点的最佳残差拟合值:<br>$$c_{tj} = \underbrace{arg\; min}<em>{c}\sum\limits</em>{x_i \in R_{tj}} log(1+exp(-y_i(f_{t-1}(x_i) +c)))$$<br>简化为:<br>$$c_{tj} = \sum\limits_{x_i \in R_{tj}}r_{ti}\bigg /  \sum\limits_{x_i \in R_{tj}}|r_{ti}|(1-|r_{ti}|)$$</p><h1 id="GBDT多分类"><a href="#GBDT多分类" class="headerlink" title="GBDT多分类"></a>GBDT多分类</h1><p>其中，<strong>多分类的对数似然损失函数为:</strong><br>$$L(y, f(x)) = -  \sum\limits_{k=1}^{K}y_klog\;p_k(x)$$<br><strong>其在第m轮的第i个样本的负梯度为：</strong><br>$$r_{til} = -\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]_{f_k(x) = f_{l, t-1}\;\; (x)} = y_{il} - p_{l, t-1}(x_i)$$<br><strong>观察上式可以看出，其实这里的误差就是样本$i$对应类别l的真实概率和$t−1$轮预测概率的差值。</strong>(如果你理解不了的话可以记住这句话)<br>各个叶子节点的最佳残差拟合值:<br>$$c_{tjl} = \underbrace{arg\; min}_{c_{jl}}\sum\limits_{i=0}^{m}\sum\limits_{k=1}^{K} L(y_k, f_{t-1, l}(x) + \sum\limits_{j=0}^{J}c_{jl} I(x_i \in R_{tj}))$$<br>简化为:<br>$$c_{tjl} =  \frac{K-1}{K} \; \frac{\sum\limits_{x_i \in R_{tjl}}r_{til}}{\sum\limits_{x_i \in R_{til}}|r_{til}|(1-|r_{til}|)}$$</p><p>其实gbdt回归与分类损失函数最小化策略跟CART树利用平方损失和基尼系数找最优切分点的道理是一样的，毕竟boosting的前一颗树只是起到了提供残差的作用，每一轮的迭代其实就只有一颗CART树，这里可以回去翻看CART树的生成策略。</p><h1 id="GBDT的正则化"><a href="#GBDT的正则化" class="headerlink" title="GBDT的正则化"></a>GBDT的正则化</h1><p>为了防止过拟合，我们有三种正则化的策略。<br><strong>(1)</strong>第一种是和Adaboost类似的正则化项，即<strong>步长(learning rate)</strong>。定义为ν,对于前面的弱学习器的迭代：<br>$$f_{k}(x) = f_{k-1}(x) + h_k(x)$$<br>如果我们加上了正则化项，则有:<br>$$f_{k}(x) = f_{k-1}(x) + \nu h_k(x)$$<br>$ν$的取值范围为$0&lt;ν≤1$。对于同样的训练集学习效果，<strong>较小的ν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。</strong><br><strong>(2)</strong>第二种正则化的方式是通过<strong>子采样比例（subsample）</strong>。取值为(0,1]。注意这里的子采样和随机森林不一样，随<strong>机森林使用的是放回抽样，而这里是不放回抽样</strong>。如果取值为1，则全部样本都使用，等于没有使用子采样。<strong>如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间</strong>。<br>使用了子采样的GBDT有时也称作随机梯度提升树(Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。<br><strong>(3)</strong>第三种是对于弱学习器<strong>即CART回归树进行正则化剪枝</strong>。</p><h1 id="GBDT的优缺点"><a href="#GBDT的优缺点" class="headerlink" title="GBDT的优缺点"></a>GBDT的优缺点</h1><p><strong>优点:</strong></p><ul><li>可以<strong>灵活处理各种类型的数据</strong>，包括连续值和离散值。</li><li>在相对少的调参时间情况下，预测的<strong>准确率也可以比较高</strong>。这个是相对SVM来说的。</li><li><strong>使用一些健壮的损失函数，对异常值的鲁棒性非常强</strong>。比如 Huber损失函数和Quantile损失函数。<br><strong>缺点：</strong></li><li>由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。</li></ul><hr><p><strong>本博客内容引用了以下链接内容:</strong><br>1.<a href="https://www.cnblogs.com/pinard/p/6140514.html" target="_blank" rel="noopener">刘建平</a><br>2.<a href="http://blog.csdn.net/google19890102/article/details/50522945" target="_blank" rel="noopener">zhiyong_will</a><br>3<a href="http://blog.csdn.net/sb19931201/article/details/52506157" target="_blank" rel="noopener">我曾被山河大海跨过</a></p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;提升树算法，GBDT原理，损失函数，与LR比较&lt;/center&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>浅谈boosting与Adaboost</title>
    <link href="http://yoursite.com/2018/03/19/adaboost/"/>
    <id>http://yoursite.com/2018/03/19/adaboost/</id>
    <published>2018-03-19T10:16:44.000Z</published>
    <updated>2018-03-21T12:49:29.663Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h1><p>Boosting算法是一种<strong>将多个弱学习器组合为强学习器</strong>的方法，<strong>这种算法的思想</strong>是：首先训练一个基学习器，然后根据基学习器的表现对<strong>样本的分布进行调整</strong>，使得先前学习错的样本在后面占更大的比重，然后基于调整后的样本分布来进行下一个基学习器的学习，最后将所有的学习器<strong>加权结合</strong>得到一个强学习器。</p><h1 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>Adaboost算法是boosting算法中比较有代表性的，它是一个**前向分步加法模型。</p><h2 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h2><p>1.<strong>在Adaboost中，对样本分布进行调整是调整</strong>样本分布的权重<strong>，对于分错的样本提高其权重，对于分对的样本降低其权重。<br>2.对于</strong>每个弱分类器的权值<strong>，根据其的</strong>分类误差率**来分配，对于分类误差率比较小的，给予一个高权值让它起更大的作用，反之给一个低权值。</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p><strong>基学习器以二分类模型为例：</strong></p><ol><li>初始化训练集样本的权值,最开始权值相等，和为1：<br>$$D1 = (w_{11},w_{1i},….,w_{1n}),w_{1i}=\frac{1}{N}$$</li><li>对$m=1,2,3,4….(第m轮)$，使用带有权值分布的训练数据集$D_m$得到二分类器$G_m(x)$,<strong>其中$G_m(x)$的生成策略是找到一个决策函数使得当前分类误差率最小，所以其实得到$G_m(x)$的同时就已经得到了分类误差率。</strong><br>$$min(e_m)$$</li><li>使用$G_m(x)$在训练集上计算分类误差率<br>$$e_m = P(G(x_i)≠y_i) = \sum_{i=1}^Nw_{mi}I(G(x_i)≠y_i)$$<br>就是说如果预测错了的话，则该样本的分类分类误差率为: $w_{mi}$x1,所以<strong>数据集整体的分类误差率其实就是分类错误的样本对应的样本权值之和</strong>(函数$I$中分类正确的样本为$I(x_i)=0$)</li><li>计算当前弱学习器的的系数(对数是自然对数),由下式可以看出，当分类误差率$e_m$&gt;$\frac{1}{2}$的时候，其<strong>分类器对应的系数为$0$</strong>(说明效果太差了)<br>$$\alpha_m = \frac{1}{2}log\frac{1-e_m}{e_m}$$</li><li>根据计算得到的$\alpha_m$和当前的弱分类器，计算<strong>下一轮的数据集样本权重</strong>,其中第$m$轮的第$i$个样本的权重为：<br>$$w_{mi}=\frac{w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i))$$<br>其中，<strong>$Z_m$是规范化因子</strong>，它是D_{m+1}称为一个概率分布,其实就是上式分子$i$从$1$到$N$的一个<strong>累加和</strong>。<br>$$Z_m=\sum_{i=1}^Nw_{mi}exp(-\alpha_my_iG_m(x_i))$$</li><li>循环算法步骤$(3)-(5)$之后，可以得到<strong>m个弱分类器的加权线性组合函数(</strong>一共$m$轮):<br>$$f(x)=\sum_{m=1}^M\alpha_mG_m(x)$$<br>即最后得到的强分类器:<br>$$G(x)=sign(f(x))$$</li></ol><h1 id="算法关键点"><a href="#算法关键点" class="headerlink" title="算法关键点"></a>算法关键点</h1><ul><li>注意算法每一轮计算的步骤，首先是学习这一轮的基本分类器，然后是计算分类误差率，然后是计算当前分类器的最终系数$\alpha_m$,最终计算下一轮的样本权重$w_{m+1,i}$,更新样本权重可以写为:<br>$$\begin{cases}<br>\frac{w_{mi}}{Z_{m}}e^{-\alpha_m} \ , \ G_m(x_i)=y_i\<br>\<br>\frac{w_{mi}}{Z_{m}}e^{\alpha_m} \ , \ G_m(x_i) ≠y_i<br>\end{cases}<br>$$</li><li>从分类误差率的计算可以看出，当前分类误差率依赖于上一轮的样本权重，<strong>由于上一轮对于分错的样本给予了比较大权重，如果这一轮分类继续出错，那么这一轮的分类误差率肯定也会比较大，为了降低分类误差率，模型会朝着使得这些权重大的样本分类正确的方向拟合</strong>，从而达到了我们”提高分类错误的样本，使得其被后面的分类器更加关注”的目的。</li></ul><h1 id="前向分步模型"><a href="#前向分步模型" class="headerlink" title="前向分步模型"></a>前向分步模型</h1><p>那既然说Adaboost是一个前向分步加法模型，那什么是前向分步模型呢，为什么叫这个名字？<br>在加法模型中:<br>$$G(x)=\sum_{m=1}^M\alpha_mG_m(x)$$<br>在给定数据集D和损失函数L的情况下，学习策略变成了使损失函数最小的最优化问题:<br>$$Min\sum_{i=1}^NLoss(y_i,G(x_i))$$<br>如果<strong>把这个损失函数当成一个整体来求最优化是个很复杂的问题</strong>，为了简化这种问题，将其<strong>分解为从前往后，每一步只学习一个分类器和其系数的问题</strong>。所以，每一次只需要优化如下的损失函数:<br>$$Min(Loss(y_i,\alpha_mG_m(x)))$$<br>而前向分布的算法步骤正如Adaboost一样，这里不再写一遍了。</p><h1 id="前向分步与Adaboost关系"><a href="#前向分步与Adaboost关系" class="headerlink" title="前向分步与Adaboost关系"></a>前向分步与Adaboost关系</h1><p>AdaBoost算法是前向分步加法算法的特例，这时，模型是由<strong>基本分类器</strong>组成的加法模型，损失函数是<strong>指数函数</strong>。</p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;Adaboost原理与算法，前向分步加法模型&lt;/center&gt;&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>浅谈SVM与感知机</title>
    <link href="http://yoursite.com/2018/03/16/svm/"/>
    <id>http://yoursite.com/2018/03/16/svm/</id>
    <published>2018-03-16T11:36:46.000Z</published>
    <updated>2018-03-20T12:39:23.646Z</updated>
    
    <content type="html"><![CDATA[<h1 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>感知机是SVM和神经网络的基础，所以在介绍SVM之前，先谈谈感知机。</p><p><strong>感知机是一个二分类的线性分类模型。</strong>其表示从输入空间到输出空间的如下函数，输出为-1,1:<br>$$f(x)=sign(w·x+b)$$</p><p>其中，w是权值向量，b为偏置bias,sign是符号函数</p><h2 id="几何意义"><a href="#几何意义" class="headerlink" title="几何意义"></a>几何意义</h2><p><strong>其几何意义为:</strong><br>$$w·x+b=0$$<br>对应于特征空间中的一个<strong>分离超平面</strong>，其中<strong>w表示超平面的法向量</strong>，而<strong>b对应超平面的截距</strong>。</p><h2 id="感知机-线性可分"><a href="#感知机-线性可分" class="headerlink" title="感知机-线性可分"></a>感知机-线性可分</h2><p><strong>线性可分</strong>的意思是存在一个超平面可以将正负样本点完全分开。</p><blockquote><p>在线性可分的情况下，感知机的<strong>学习策略</strong>是：<br>“<strong>使得误分类点到超平面的总距离最小</strong>”</p></blockquote><p>所有误分类点到超平面S的总距离为:<br>$$-\frac{1}{||w||}\sum_{x_i∈M}y_i(w·x_i+b)$$<br>所以可以得到其损失函数，它是关于w,b的连续可导函数:<br>$$L(w,b)=-\sum_{x_i∈M}y_i(w·x_i+b)$$</p><h2 id="感知机学习算法"><a href="#感知机学习算法" class="headerlink" title="感知机学习算法"></a>感知机学习算法</h2><p>感知机的学习算法即上式(损失函数)的最优化问题，其方法为<strong>随机梯度下降法</strong>。<br>$$minL(w,b)=-\sum_{x_i∈M}y_i(w·x_i+b)$$<br>随机梯度下降一次只选取一个误分类点使其梯度下降,其梯度计算为:<br>$$L_w(w,b)=-\sum_{x_i∈M}y_ix_i$$<br>$$L_b(w,b)=-\sum_{x_i∈M}y_i$$</p><p>所以下面给出感知机学习算法步骤(原始形式):<br>给定训练集和学习率$\eta$<br>(1) 选取初始值$w_0,b_0$<br>(2) 随机选取一个误分类点$y_i(w·w_i+b)≤0$更新w,b:<br>$$w←w+\eta y_ix_i$$<br>$$b←b+\eta y_i$$<br>(3)重复(2)直到没有误分类点</p><h2 id="感知机学习算法几何解释"><a href="#感知机学习算法几何解释" class="headerlink" title="感知机学习算法几何解释"></a>感知机学习算法几何解释</h2><p>当一个样本点被误分到分离超平面的另一端时，调整w,b的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，如此循环直到该误分类点被正确分类。由于<strong>其结束的条件是所有样本点被正确分类，所以，在线性可分数据集中，感知机有无数个解。</strong></p><h1 id="线性支持向量机"><a href="#线性支持向量机" class="headerlink" title="线性支持向量机"></a>线性支持向量机</h1><p>SVM(support vector machines,SVM)是一种二分类模型。其核心是<strong>核技巧</strong>，可以用来解决非线性问题，当<strong>使用非线性核</strong>的时候，SVM是非线性模型，其相当于把<strong>隐式地在高维的特征空间中学习线性支持向量机</strong>。</p><h2 id="SVM与感知机的区别"><a href="#SVM与感知机的区别" class="headerlink" title="SVM与感知机的区别"></a>SVM与感知机的区别</h2><p>1.学习策略不同，感知机的学习策略为使得误分类点的距离最小，而SVM的学习策略是间隔最大化，这样使得在同样解决线性可分的问题时，感知机可以有无数个解，而svm只有一个最优解。</p><ol><li>感知机拥有核技巧，可以解决非线性问题<h2 id="间隔最大化"><a href="#间隔最大化" class="headerlink" title="间隔最大化"></a>间隔最大化</h2>我们知道SVM和感知机的最大区别之一就是SVM采用的是<strong>间隔最大化</strong>的策略，使得其存在<strong>唯一最优解</strong>下面介绍函数间隔和几何间隔：<br><strong>(1) 函数间隔</strong><br>$|w·x+b|$表示一个点离超平面的远近，而加上$y_i$可以判断其是否被正确分类，所以<strong>函数间隔就是组合两者：表示一个样本点被分类的正确性和确信度(离得越远越确定):</strong><br>$$\gamma_i = y_i(w·x_i+b)$$<br>整个函数集的函数间隔就是<strong>所有样本点函数间隔最小值</strong>。<br><strong>(2) 几何间隔</strong><br>使用函数间隔会有缺点：<br><strong>成倍缩放w和b，超平面位置不变</strong>，但是函数间隔为会改变。<br>所以引入了几何间隔的概念(做了一个L2归一化)使得超平面确定了，其间隔也确定，几何间隔为:<br>$$\gamma = -(\frac{w}{||w||}·x_i+\frac{b}{||w||})$$</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;感知机，线性可分，非线性内核(未写完待续)&lt;/center&gt;&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>广义线性模型</title>
    <link href="http://yoursite.com/2018/03/16/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2018/03/16/广义线性模型/</id>
    <published>2018-03-16T06:54:22.000Z</published>
    <updated>2018-03-16T07:59:23.631Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是广义线性模型？"><a href="#什么是广义线性模型？" class="headerlink" title="什么是广义线性模型？"></a><strong>什么是广义线性模型？</strong></h1><blockquote><p>广义线性模型是线性模型的扩展，其特点是不强行改变数据的自然度量，数据可以具有非线性和非恒定方差结构，主要是通过<u>联结函数g()(link function)，建立响应变量Y的数学期望值与线性组合的预测变量P之间的关系。</u></p></blockquote><h1 id="指数分布族"><a href="#指数分布族" class="headerlink" title="指数分布族"></a>指数分布族</h1><p>指数分布族具有以下形式:<br>$$p(y;\eta)=b(y)exp(\eta^TT(y)-a(\eta))$$<br>其中的参数意义为:</p><ul><li>$\eta$ 是自然参数</li><li>$T(y)$是充分统计量(一般$T(y)=y$)</li><li>$a(\eta)$是log partition function($e^{-a(\eta)}$充当正规化常量的角色，保证$\sum_{p(y;\eta)}=1$)<br>所以，<strong>T,a,b确定了一种分布，而$\eta$是该分布的参数</strong></li></ul><p>符合指数族分布的模型我们都可以用把它当成广义线性模型来求解:<br><img src="/2018/03/16/广义线性模型/1.jpg" alt=""></p><h1 id="广义线性模型的三个假设"><a href="#广义线性模型的三个假设" class="headerlink" title="广义线性模型的三个假设"></a>广义线性模型的三个假设</h1><p>GLM(Generalized Linear Models)的三个假设是推导普通模型的基础:<br>1.$y | x ; \theta ~ ExponentialFamily(\eta)$固定参数$\theta$,在给定x的情况下，$y$服从指数分布族中以$\eta$为参数的某个分布<br>2.给定一个x，我们需要的目标函数为$h_\theta(x)=E[T(y)|x;\theta]$,后者为该分布的期望<br>3.令$\eta=\theta^Tx$</p><h1 id="GLM和线性回归与LR的联系"><a href="#GLM和线性回归与LR的联系" class="headerlink" title="GLM和线性回归与LR的联系"></a>GLM和线性回归与LR的联系</h1><p><strong>GLM和线性回归</strong><br>其中，线性回归是在选择合适的T,a,b使其为高斯分布的时候推导出来的:<br><strong>看起是否满足三点假设:</strong><br>假设1:能否将高斯分布写成指数分布族的形式</p><center>$p(y;\mu)=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y-\mu)^2}{2\sigma^2})$</center><br><center>$=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{1}{2}y^2)\cdot \exp(\mu y-\frac{1}{2}\mu ^2)$</center><p>假设2：假设函数是否等于高斯分布的期望</p><center>$h_\theta(x)=E[T(y)\mid x;\theta]$</center><br><center>$=E\left[y\mid x;\theta\right]$</center><br><center>$=\mu$</center><br>假设3：自然参数$\eta$和x是否是线性关系<br>$$h_\theta(x)=\mu=\eta=\theta^Tx$$<br><strong>满足三个以上假设，而假设三满足的线性关系正是线性回归。</strong><br><br><strong>广义线性模型与LR</strong><br>同理，当把<strong>伯努利分布</strong>放在广义线性模型中推导的时候，可以得到逻辑斯蒂回归的$h_\theta(x)$<br><center>$h_\theta(x)=E\left[T(y)\mid x;\theta\right]$</center><br><center>$=E\left[y\mid x;\theta\right]$</center><br><center>$=\phi$</center><br><center>$=\frac{1}{1+e^{-\eta}}$</center><br><center>$=\frac{1}{1+e^{-\theta^Tx}}$</center><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>除了高斯分布与伯努利分布，大多数的概率分布都能表示成指数分布族的形式，如多项式分布（Multinomial），对有K个离散结果的事件建模；泊松分布（Poisson），对计数过程进行建模，如网站访问量的计数问题；指数分布（Exponential），对有间隔的证书进行建模，如预测公交车的到站时间的问题；等等，然后通过进一步的推导，就能得到各自的线性模型，这大大扩展了线性模型可解决问题的范围。</p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p>1.<a href="https://zhuanlan.zhihu.com/p/22876460" target="_blank" rel="noopener">晓雷</a><br>2.<a href="http://blog.csdn.net/sinat_37965706/article/details/69204397" target="_blank" rel="noopener">NJiaHe</a></p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;LR、线性回归和广义线性模型的联系&lt;/center&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>逻辑斯蒂回归</title>
    <link href="http://yoursite.com/2018/03/14/%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2018/03/14/逻辑斯蒂回归/</id>
    <published>2018-03-14T06:59:54.000Z</published>
    <updated>2018-03-14T08:31:40.182Z</updated>
    
    <content type="html"><![CDATA[<h1 id="是什么？"><a href="#是什么？" class="headerlink" title="是什么？"></a>是什么？</h1><p>逻辑斯蒂回归，又称对数纪律回归，是在线性回归的基础上，使用sigmoid函数将线性模型$w^TX$的输出压缩到0-1之间，使其具有预测概率的特性。<strong>是一种广义上的线性模型</strong>。<br><strong>sigmoid函数:</strong><br>$$\sigma(a)=\frac{1}{1+e^{-a}}$$<br>解决了阶跃函数不是单调可微(因为它不连续)的问题，使得输出能够映射为0-1的连续区间。</p><h1 id="逻辑斯蒂分布？"><a href="#逻辑斯蒂分布？" class="headerlink" title="逻辑斯蒂分布？"></a>逻辑斯蒂分布？</h1><p>逻辑斯蒂分布的<strong>概率密度函数</strong>和<strong>分布函数</strong>如下：<br>$$F(x)=P(X\le x)= {1 \over 1+e^{-(x-\mu )/\gamma}}$$<br>$$f(x)=F’(x)={e^{-(x-\mu)/y} \over \gamma(1+e^{-(x-\mu)}/\gamma)^2}$$<br>其中,$μ$为位置参数,$γ$为形状参数，其分别对应的图像如下：<br><img src="/2018/03/14/逻辑斯蒂回归/1.jpg" alt=""></p><h1 id="逻辑斯蒂分布与逻辑斯蒂回归的关系？"><a href="#逻辑斯蒂分布与逻辑斯蒂回归的关系？" class="headerlink" title="逻辑斯蒂分布与逻辑斯蒂回归的关系？"></a>逻辑斯蒂分布与逻辑斯蒂回归的关系？</h1><p>逻辑斯蒂回归中的sigmoid函数就是逻辑斯蒂分布在$μ=0;$$γ=1$时的情况</p><h1 id="对数几率回归？"><a href="#对数几率回归？" class="headerlink" title="对数几率回归？"></a>对数几率回归？</h1><p>首先由之前给出的sigmoid函数:<br>$$\sigma(a)=\frac{1}{1+e^{-a}}$$,代入一个广义线性模型$W^TX+b$,得到:<br>$$y = \frac{1}{1+e^{-(\omega^Tx+b)}}$$<br>通过等式变换得到:<br>$$\ln(\frac{y}{1-y}) = \omega^Tx+b$$<br><strong>等式左边：</strong>一个事件的<strong>几率</strong>是该事件发生的概率与一个事件不发生的概率的比值，也就是<strong>对数几率</strong>。<strong>等式右边</strong>是线性模型。所以<strong>对数几率回归</strong>实际上就是用右边的<strong>线性模型</strong>去逼近这个<strong>对数几率。</strong></p><h1 id="参数估计？"><a href="#参数估计？" class="headerlink" title="参数估计？"></a>参数估计？</h1><p>既然可以写成对数几率回归的形式，那么怎样来估计等式右边线性模型的$w$和$b$呢？<br><strong>极大似然估计</strong><br>引用知乎上对极大似然的一种比较直观的解释    <a href="https://www.zhihu.com/question/20447622" target="_blank" rel="noopener">作者：稻花香</a></p><blockquote><p>现在已经拿到了很多个样本（你的数据集中所有因变量），这些样本值已经实现，最大似然估计就是去找到<u><strong>那个（组）参数估计值，使得前面已经实现的样本值发生概率最大</strong>。</u>因为你手头上的样本已经实现了，其发生概率最大才符合逻辑。<u><strong>这时是求样本所有观测的联合概率最大化，是个连乘积，只要取对数，就变成了线性加总</strong></u>。此时通过对参数求导数，并令一阶导数为零，就可以通过解方程（组），得到最大似然估计值。</p></blockquote><p>用极大似然估计来求参数的算法步骤为：<br>1.<strong>写出似然函数</strong>，在这里，其似然函数为：<br>$$\Pi^N_{i=1}[\pi(x_i)^{y_i}][1-\pi(x)]^{1-y_i},\pi(x)为x为1的概率$$<br>2.对似然函数<strong>取对数</strong>并化简:</p><center>$L(w)=\sum^N_{i=1}[y_i\log \pi(x_i)+(1-y_i)\log (1-\pi(x_i))]$</center><br><center>$=\sum^N_{i=1}[y_i\log{\pi(x) \over 1-\pi(x_i)}+\log(1-\pi(x_i))]$</center><br><center>$= \sum^N_{i=1}[y_i(wx)-\log(1+exp(wx))]$</center><p>3.<strong>求导数</strong>:<br>(下式$y_n$对应上式$y_i$，$t_n$对应$x_i$)</p><p>$$\frac{\partial lnP(t|w)}{\partial w}=\sum_{n=1}^N{t_nlny_n+(1-t_n)ln(1-y_n)}$$</p><p><strong>求导过程：</strong></p><center>$\frac{\partial lnP(t|w)}{\partial w} =\sum_{n=1}^N{t_n\frac{1}{y_n}\partial y_n-(1-t_n)\frac{1}{1-y_n}\partial y_n}$</center><br><center>$=\sum_{n=1}^N{(t_n\frac{1}{y_n}-(1-t_n)\frac{1}{1-y_n})y_n(1-y_n)\partial(-w^T\phi_n)}$</center><br><center>$=\sum_{n=1}^N{(y_n-t_n)\phi_n}$</center><p><strong>4.估计参数</strong><br>梯度下降法，拟牛顿法等。</p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;逻辑斯蒂分布，对数几率回归，极大似然估计&lt;/center&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>对生成模型与判别模型的理解</title>
    <link href="http://yoursite.com/2018/03/11/%E7%94%9F%E6%88%90%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2018/03/11/生成判别模型/</id>
    <published>2018-03-11T15:18:21.000Z</published>
    <updated>2018-03-11T17:36:59.685Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>[该博客为本网站作者: “yangyiqing”原创，转载请注明出处]</p></blockquote><h1 id="书中介绍"><a href="#书中介绍" class="headerlink" title="书中介绍"></a>书中介绍</h1><p>在《统计学习方法》中，对生成模型和判别模型的介绍篇幅比较少：</p><blockquote><p><strong>1.生成方法</strong><br>监督学习方法可以分为生成方法和判别方法，生成的模型分别对应为<strong>生成模型</strong>和<strong>判别模型</strong><br><strong>生成方法</strong>由<strong>数据联合概率分布$P(X,Y)$,</strong>然后求出<strong>条件概率分布$P(Y|X)$</strong>作为预测的模型，即生成模型：<br>$$P(Y|X)=\frac{P(X,Y)}{P(X)}$$<br><strong>称之为生成方法的原因是：模型表示了给定输入X产生输Y的生成关系</strong>。典型的生成方法有：</p><ul><li>朴素贝叶斯</li><li>隐马尔科夫模型</li></ul><hr><p> <strong>2.判别方法</strong><br> 判别方法由数据直接学习决策函数$f(x)$或者条件概率分布$P(Y|X)$作为预测的模型，即判别模型。<strong>判别模型关心的是对于给定的输入X，应该预测什么样的Y</strong>。典型的判别模型有：</p><ul><li>k近邻(knn)</li><li>感知机</li><li>决策树</li><li>逻辑斯蒂回归</li><li>最大熵模型</li><li>支持向量机</li><li>提升方法</li><li>条件随机场</li></ul><hr><p> <strong>生成方法和判别方法的区别：</strong><br> 1.生成方法可以还原出<strong>联合概率分布</strong>，而判别方法不可以<br> 2.通常情况下，<strong>生成方法</strong>的学习<strong>收敛速度快</strong>，而<strong>判别方法的准确率更高</strong><br> 3.当存在隐变量的时候，仍然可以用生成方法，但是不能用判别方法<br> 4.由于直接学习$P(Y|X)$或$f(X)$，可以对数据进行各种程度上的抽象，定义特征并使用特征，因此可以简化学习问题。</p></blockquote><h1 id="个人补充"><a href="#个人补充" class="headerlink" title="个人补充"></a>个人补充</h1><h2 id="决策函数和条件概率函数"><a href="#决策函数和条件概率函数" class="headerlink" title="决策函数和条件概率函数"></a>决策函数和条件概率函数</h2><p>先说说<strong>决策函数$f(x)$和条件概率分布$P(Y|X)$:</strong><br>举个例子来说，有一个分类问题，要求判断给定特征下判断是篮球，足球，还是乒乓球。<br>1.在<strong>决策函数</strong>中：<br>对于每一个特征有一个阈值(分类边界)，对于输入的特征每个去判断属于边界的哪一边，最后<strong>直接得到一个对应的输出$Y$就是对应的分类结果</strong>。<br>2.在<strong>条件概率分布</strong>中：<br>会去计算在对应特征条件下是每个类的概率，也就是依次计算：$$P(篮球|X),P(足球|X),P(乒乓球|X)$$<br>然后<strong>从中选择输出概率最大的那个最为对应的分类结果。</strong><br>再说的详细一点儿，每个条件概率分布式是怎么求出来的呢：<br>$$P(Y|X)=\frac{P(X,Y)}{P(X)}=\frac{P(X|Y)P(Y)}{P(X)}$$<br>从以上的例子应该可以看出决策函数和条件概率分布的不同，一个是给定X直接产生一个对应输出Y，而另一个是首先计算其可能为每个Y的概率，然后再输出一个概率最大的Y。</p><p><strong>那决策函数$f(x)$和条件概率分布$P(Y|X)$有什么联系呢？</strong></p><blockquote><p>实际上<strong>通过条件概率分布P(Y|X)进行预测也是隐含着表达成决策函数Y=f(X)的形式</strong>的。例如也是两类w1和w2，那么我们求得了P(w1|X)和P(w2|X)，那么实际上判别函数就可以表示为Y= P(w1|X)/P(w2|X)，如果Y大于1或者某个阈值，那么X就属于类w1，如果小于阈值就属于类w2。<u><strong>而同样，很神奇的一件事是，实际上决策函数Y=f(X)也是隐含着使用P(Y|X)的</strong></u>。因为一般决策函数Y=f(X)是通过学习算法使你的预测和训练数据之间的误差平方最小化，而贝叶斯告诉我们，虽然它没有显式的运用贝叶斯或者以某种形式计算概率，但它实际上也是在隐含的输出极大似然假设（MAP假设）。也就是说学习器的任务是在所有假设模型有相等的先验概率条件下，输出极大似然假设。</p></blockquote><h2 id="简化学习问题"><a href="#简化学习问题" class="headerlink" title="简化学习问题"></a>简化学习问题</h2><p><strong>如何理解判别模型可以简化学习问题？</strong></p><blockquote><p>分类器的设计就是在给定训练数据的基础上估计其概率模型P(Y|X)。如果可以估计出来，那么就可以分类了。<u><strong>但是一般来说，概率模型是比较难估计的。给一堆数给你，特别是数不多的时候，你一般很难找到这些数满足什么规律吧。</strong></u>那能否不依赖概率模型直接设计分类器呢？事实上，分类器就是一个决策函数（或决策面），<u><strong>如果能够从要解决的问题和训练样本出发直接求出判别函数，就不用估计概率模型了</strong></u>，这就是决策函数Y=f(X)的伟大使命了。<u><strong>例如支持向量机，我已经知道它的决策函数（分类面）是线性的了，也就是可以表示成Y=f(X)=WX+b的形式，那么我们通过训练样本来学习得到W和b的值就可以得到Y=f(X)了</strong></u>。还有一种<strong>更直接的分类方法</strong>，它不用事先设计分类器，而是只确定分类原则，根据已知样本（训练样本）直接对未知样本进行分类。包括<u><strong>近邻法，它不会在进行具体的预测之前求出概率模型P(Y|X)或者决策函数Y=f(X)</strong></u>，而是在真正预测的时候，将X与训练数据的各类的Xi比较，和哪些比较相似，就判断它X也属于Xi对应的类。</p></blockquote><p>所以判别模型的可以简化学习问题的优势就在于，我不一定硬要去找数据中的规律或计算条件概率，我只要找到一种判别规则就可以了，而且由于<strong>不需要计算类别的条件概率，可以对数据进行降维等抽象操作</strong>。</p><h2 id="其它联系"><a href="#其它联系" class="headerlink" title="其它联系"></a>其它联系</h2><p><strong>生成模型和判别模型还有什么联系？</strong><br>由生成模型可以得到判别模型，但由判别模型得不到生成模型。生成模型可以反应数据本身的特性，而判别模型不可以。<br>来源于同一个原因：</p><blockquote><p><u><strong>生成模型可以求出数据的联合概率分布，而判别模型不可以。</strong></u></p></blockquote><h2 id="区别和优缺点"><a href="#区别和优缺点" class="headerlink" title="区别和优缺点"></a>区别和优缺点</h2><p><strong>再通俗的解释一下两者的区别？</strong></p><p>生成算法尝试去找到底这个数据是怎么生成的（产生的），然后再对一个信号进行分类。<u>基于你的生成假设，那么那个类别最有可能产生这个信号，这个信号就属于那个类别</u>。判别模型不关心数据是怎么生成的，<u>它只关心信号之间的差别</u>，然后用差别来简单对给定的一个信号进行分类。</p><p><strong>生成模型的优点和缺点？</strong><br><strong>优点：</strong><br>1.可以还原联合概率分布$P(X,Y)$，得到数据集更多的特性。<br>2.生成模型收敛速度比较快，即当样本数量较多时，生成模型能更快地收敛于真实模型。<br>3.生成模型能够应付存在隐变量的情况<br>4.研究单类问题比判别模型灵活性强<br><strong>缺点：</strong><br>1.准确率一般没有判别模型高</p><p><strong>判别模型的优点和缺点？</strong><br><strong>优点：</strong><br>1.需要的样本数量少，计算量小<br>2.直接面对预测，准确率高<br>3.直接学习$P(Y|X)$，不需要求解条件概率，所以允许我们对输入进行抽象（比如降维、构造等），从而能够简化学习问题。<br><strong>缺点：</strong><br>1.无法反映数据集的特性，无法求解联合概率分布</p><h2 id="过拟合？"><a href="#过拟合？" class="headerlink" title="过拟合？"></a>过拟合？</h2><p><strong>过拟合问题</strong><br>直接摘自<a href="http://helloyuan.com/2018/03/09/%E8%AF%B4%E8%AF%B4%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">大鼻子的博客</a></p><h3 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a><strong>生成模型</strong></h3><p>‘‘没有考虑正则化很简单，因为他们很少过拟合’’。生成模型学习$X,Y$的联合概率分布$P(X,Y)$，直接学习的就是数据的分布，从整个数据的整体着手，很少会出现过拟合。<br>基本上属于高偏差/低方差分类器，当样本数量小于特征数量或样本数量不足时，应选用这种模型</p><h3 id="判别模型"><a href="#判别模型" class="headerlink" title="判别模型"></a><strong>判别模型</strong></h3><p>判别模型应当有正则化过程，因为是直接生成$f(x)$或者$p(y|x)$，所以很容易比较$y$跟$f(x)$的关系，按照现有数据照葫芦画瓢来判别，容易过拟合，所以正则化便有存在的意义。<br>基本上属于低偏差/高方差分类器，容易过拟合，需要正则项。数据量充足时选用判别模型</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a><strong>结论</strong></h3><p>随着训练集的增大，低偏差/高方差分类器（判别模型）相对于高偏差/低方差分类器（生成模型）准确率高，因为随着数据量的增大，现有训练集数据的分布更接近于真实分布，此时生成模型优势变小，同时生成模型不能提高足够的准确率，此时，判别模型优势更大。</p><h3 id="举个例子："><a href="#举个例子：" class="headerlink" title="举个例子："></a><strong>举个例子：</strong></h3><p>当一份分类数据的特征维度大于样本数量时。如果采用判别模型，极端情况下每条样本都有唯一的特征（或特征组合），此时如果正则化不够给力，那么该判别模型将极大限度拟合当前数据，训练集AUC可能将近1，那么就可能得到训练数据上准确率100%，测试数据准确率不如XJB猜的模型。</p><hr><p><strong>感觉知乎这个答案也比较赞</strong><br><img src="/2018/03/11/生成判别模型/3.png" width="800" height="750" alt="图片名称" align="center"><br><strong>一张图总结生成模型与判别模型</strong><br><img src="/2018/03/11/生成判别模型/2.png" width="600" height="500" alt="图片名称" align="center"></p><hr><p><strong>仍有以下问题没有完全搞懂：</strong></p><ol><li>关于隐变量的问题<br>贴一张知乎的图作为初步理解。<br><img src="/2018/03/11/生成判别模型/1.png" width="639" height="985" alt="图片名称" align="center"><br>2.”而贝叶斯告诉我们，虽然它没有显式的运用贝叶斯或者以某种形式计算概率，但它实际上也是在隐含的输出极大似然假设（MAP假设）”</li></ol><hr><p>参考链接:<br>1.<a href="http://blog.csdn.net/zouxy09/article/details/8195017" target="_blank" rel="noopener">CSDN</a><br>2.<a href="https://www.zhihu.com/search?type=content&amp;q=%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B" target="_blank" rel="noopener">知乎1</a><br>3.<a href="https://zhuanlan.zhihu.com/p/30941701" target="_blank" rel="noopener">知乎2</a></p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;生成模型与判别模型的区别和各自的应用场景&lt;/center&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
