<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[目录导航]]></title>
    <url>%2F2028%2F04%2F24%2Fnavigation%2F</url>
    <content type="text"><![CDATA[机器学习理论 [1.梯度提升树GBDT] [ 2.浅谈boosting与Adaboost] [ 3.浅谈SVM与感知机] [ 4.广义线性模型] [ 5.逻辑斯蒂回归] [ 6.对生成模型与判别模型的理解] [ 7.决策树原理] [ 8.决策树生成代码] [ 9.稀疏矩阵实践及意义] [ 10.FM和FFM模型原理及实践] [ 11.Softmax|skip-gram|CBOW|negative sampling] [ 12.零碎知识点] [ 13.AUC、加权AUC原理] [14.Batch Normalization] 数据挖掘比赛 [1.2017 CCF 大数据竞赛top4%] [2.2017-JDD京东金融算法大赛15th解决方案] 算法题 [1.剑指offer] [2.Leetcode] 论文 [1.Deep Neural Network for Youtube Recommendation] [2.xDeepFM:Combining Explicit and Implicit Feature Interactions for Recommender Systems]]]></content>
      <tags>
        <tag>导航</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xDeepFM:Combining Explicit and Implicit Feature Interactions for Recommender Systems]]></title>
    <url>%2F2018%2F07%2F18%2FxDeepFM%2F</url>
    <content type="text"><![CDATA[ABSTRACT1.现在诸如DeepFM和Deep&amp;Wide等模型都可以自动学习隐式的高维交互特征，并结合了低维特征，但是有一个缺点就是它们的高维特征都是在bite-wise的层面上进行交互的。本片论文提出了一种压缩交互网络(Compressed Interaction Network(CIN))，能够学习显式的交互特征并且是在vector-wise的级别，CIN带有一些CNN和RNN的特点，最终作者将整个模型命名为”eXtreme Deep Factorization Machine(xDeepFM)”。 2.本文提出的模型有两个优点： 能够显式的学习有明确边界的高维交互特征 能够学习隐式的低维和高维特征 个人理解这里作者对implicit和explicit的理解是交互特征的维度是否明确，在这里翻译为隐式和显式。 INTRODUCTION1.简单介绍了单值离散特征和多值离散特征，然后介绍三个手动提取交互特征的缺点： 挖掘出高质量的交互特征需要非常专业的领域知识并且需要做大量的尝试，很耗时间。 在大型的推荐系统中，原生特征是海量的，手动挖掘交叉特征几乎不可能。 挖掘不出肉眼不可见的交叉特征 2.然后介绍了经典的FM模型，用提取隐向量然后做内积的形式来提取交叉特征，扩展的FM模型可以提取随机的高维特征，但是主要的缺陷是： 会学习所有的交叉特征，其中肯定会包含无用的交叉组合，另外一篇论文指出引入无用的交叉特征会引入噪音并降低模型的表现。 3.介绍了引入了DNN的组合模型，”Factorisation-machine supported Neural Network (FNN)“，它在DNN之前使用了预训练的field embedding。 4.介绍了PNN(Product-based Neural Network),在embedding layer和DNN Input之间插入了一层product layer,不依赖于pre-trained FM。 5.FNN和PNN的缺点都是忽略了低维交互特征，Wide&amp;Deep和DeepFM模型通过混合架构解决了这种问题，但是它们同样存在缺点： 它们学习到的高维特征是一种implicit fasion,没有一种公式可以明确推论出最终学习出来的交叉特征到底是多少维的 另一方面，其DNN部分是在bit-wise的层面下进行学习的，而经典的FM架构是在vetor-wise层面学习的 6.本文提出的方法基于DCN(Deep &amp; Cross Network)模型，其目标是有效率的捕捉到边界明确的交叉特征。 PRELEMINARIESEmbedding Layer介绍一些基于”univalent”,”multivalent”进行embedding的基础知识，这里不介绍了: Implicit High-order Interactions前向传播过程： 这种架构是bit-wise层面的，意思是说，即使是同一个filed embedding，不同的element之间也会互相影响。 PNN和DeepFM基于上面的缺点进行了改进，除了DNN component,还添加了two-way interation layer到架构中，这样就既有vector-wise也有bit-wise的component了。PNN和DeepFM的区别就是DeepFM是把product layer直接作为结果连到输出层，而PNN是把product layer放在DNN和embedding layer之间 Explicit High-order Interactions这里主要介绍了Cross Network(cross net)也是本文主要借鉴的一种模型，下面是该模型的架构： 该模型的主要目标是显示的构建高维交互特征，不像DNN前向传播的全连接层那样，每个隐藏层是通过如下公式计算出来的： 通过推导可以看出其实每一个隐含层都是x0的一个scalar multiple,这当然不是代表隐含层是x0的线性表达，只是说因为每一层原生x0都会参与计算，因此对x0非常敏感。但是其缺点为： crossnet的输出是一种特殊形式，即x0的scalar multiple 交互特征仍然是bit-wise层面的 OUR PROPSED MODELCompressed Interation Network本论文设计了一种新的cross network, 称为Compressed Interaction Network (CIN), 设计的时候主要考虑了下面三个方面： 交互特征是在vector-wise层面的(主要基于crossnet改进了这点) 高维交互特征是显式的 网络的复杂度不会因为交互层级的增加而增加 下面介绍了一些在CIN的中的概念： 既然在CIN中是vector-wise层级的，那么每一个unit是一个vector，因此field embedding的输出是一个mxD的矩阵(D:embedding size,m:filed size)，CIN的第k层是一个Hk x D的矩阵(Hk代表的是CIN中每一层的向量数量，H0=m),下面是第CIN第k层的h-emb的计算公式： 还是比较直观的，其中○代表Hadamard product: 可以发现k-th layer的计算也是和crossnet一样依赖于(k-1)-th layer和 0-th layer,因此交互特征是显式的，而且交互的层级随着网络结构的加深而增加(在这点上和crossnet是一样的)，同时通过公式也可以很明显的看出，模型是vector-wise的: 如果公式不好理解的话，可以通过如下图示来理解： 图(a)和(b)表示了如何从这一层的隐藏层（Hk x D）和X^0层（m X D）来产生下一层隐藏层的（Hk+1 x D）,图示所示计算方法是为了更好的展现为什么模型有CNN的思想，先通过X0和Xk的第i列做一个outer product(matrix multiplication)得到一个Hk x m的矩阵(0&lt;=i&lt;D), 然后W就像是CNN中的filter,来过滤产生每个feature map的第i列，这样CNN中的”compressed”在CIN中就指代 Hk x D矩阵压缩为Hk+1 x D矩阵。 需要注意的是，CIN的输出是除了X0以外每一层的feature map的sum pooling横向拼接的结果。然后根据所需要进行的任务套一个激活函数就行了，比如sigmoid: CIN Analysis本文从空间复杂度，时间复杂度和多项式逼近等方面进行了分析，这里只介绍参数： 从公式也可以看出: 计算第k-th layer的第h(0&lt;h&lt;=Hk)个emb需要(Hk-1 x m)个参数，而k-th layer共有Hk个emb vetor，因此计算k-th layer在第k-1 th需要(Hk x Hk-1 x m)个参数 。 Combination with Implicit Networks由于DeepFM可以对低维交叉特征和隐式的高维交叉特征有一个比较好的支持了，因此直接将CIN加入到DeepFM中以补足其缺少有明确边界的vetor-wise层级的部分，最终的公式如下： 最终的模型结构如下： EXPRIMENTS这部分就不说了，反正就是好多实验，自己的模型就是吊吊吊，有用的信息是： 对于这种高维稀疏特征来说，基于FM思想的模型例如DeepFM,Deep&amp;Wide,PNN等比LR不知道高到哪里去了 并不是混合模型就一定好，但是单用DNN component一般效果比较差 这种用于高维稀疏特征的混合模型一般在比较浅层的比如2-3层的网络结构下会取得最好的效果]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNN for Youtube Recommendation]]></title>
    <url>%2F2018%2F07%2F10%2Fyoutube%2F</url>
    <content type="text"><![CDATA[Abstract本文主要介绍两种模型： 深度候选集生成模型 “Deep candidate generation model” 深度排序模型 关键词 推荐系统 深度学习 海量数据 介绍Youtube是目前世界上最大的视频网站，其服务的用户超10亿，由于Youtube的推荐系统主要面临着以下三个挑战： 规模：Youtube每秒钟就有总长几万小时的视频上传，那些在小数据上表现好的算法放在这里效果很差 新鲜度：传统的E&amp;E问题 噪音：用户历史行为稀疏不完整，没有非常明确的满意度反馈，视频信息非结构化 系统总览系统总体架构如下： 首先是召回模型根据简单的用户历史和上下文信息从几百万的视频库中召回几百个候选视频，然后排序模型基于大量的特征对每个视频进行打分，将最终排序的结果呈现给用户，最终效果通过线上A/B test来验证。 CANDIDATE GENERATION在深度候选集召回模型中，需要从海量的视频中进行召回，以前用的非常多的方法是矩阵分解，本文的方法其实是一种非线性的矩阵分解(其实很好理解，一会看到模型结构你们就懂了) Recommendation as Classfication这里是将召回问题转化为了一个多分类问题，因为类别如此之大，可以说是极限多分类了，最终的score由如下公式输出: 就是一个很正经的softmax而已，这里需要注意的是u即我们要学习的user向量，v代表的是video向量。 Efficient Extreme Multiclass类别如此之多的多分类模型，肯定是需要采取一些trick来避免庞大的计算量的。其中最主要的是“Sampled softmax”,感兴趣的可以自己去研究一下(其实这些sampled方法差不多，都是用极大似然的方法，通过一个人工设定的分布Q，来近似的表达真实的大数据集分布来避免全量计算)。 在线上阶段的时候需要在几十毫秒内计算出需要找回的TopN，因此问题可以简化为点积空间中的最近邻搜索问题。(这里论文里没展开说，我觉得从局部敏感哈希和用户向量的KNN入手都是可行的) Model Architecture最关键的部分： 借鉴了CBOW,输入是最近看过的N个视频id,还有搜索过的N个词，将不定长(个数不定)的稀疏向量通过加和或平均等手段转为定长向量，作为DNN的输入，其他特征如人口统计特征等不经emb直接作为输入。其中，emb参数矩阵作为整个网络的一部分参与训练，在神经网络反向传播过程中更新参数。在训练阶段，最后一层经过softmax输出概率(使用sampled softmax)，最终训练得到user vector和video vector矩阵，在线上阶段，通过最近邻搜索来进行有效率的打分和召回。 Heterogeneous Signals使用DNN来作为矩阵分解的一种泛化的好处就是可以任意加入连续特征或类别特征。例如ID类特征做embedding处理，像简单的二值特征、性别、登录状态、年龄等可以直接作为DNN的输入，实数特征做normalization处理。 “example age” Feature 关于如何将新鲜度作为特征，论文这里给出了一种方法，将视频已上传时长作为特征，训练集末尾的视频该特征置为0，从而取得了很不错的效果： 从上图可以看出，新上传的视频召回的概率大大增加，而这也与用户喜欢新视频的意愿相符。 Label and Context Selection训练集的选取是从整个Youtube视频库中选取的而不仅仅是展示(推荐)给用户的那些视频，否则新内容将很难有机会露脸，这样，如果用户是观看了其他的source而不是我们推荐的视频，那我们就可以协同过滤很快的把这种特性传播给其他用户。 还有一个需要注意的问题是要注意搜索词的顺序问题。例如一个用户刚刚搜索完”泰勒斯威夫特”，那么马上展示给她的肯定全部都是“泰勒斯威夫特”的内容，而我们当然不希望直接给用户推荐全部都是用户马上搜索过的内容(为了多样性的考虑)，因此在实际的应用中，需要打乱搜索词的顺序。 另外，用户观看存在一种模式，例如对于一些电视剧，往往会一直追着看，因此未来时间的消费(观看)就和过去的消费产生了关联，因此在选取训练集的时候，不能随机选取hold out来训练，而是选取训练选取的当前时间点之前的视频来训练，避免造成future information leak。 Expriments with Features and Depth在实验特征和网络结构的时候，选取了1M视频和1M的搜索词来构建vocabulary，隐向量的长度为256，作为特征的最近观看视频和搜索词条数为50。整体的网络是一个塔型的DNN。 RANKING在召回一批视频以后，最后一步要对每个视频进行打分然后按照排序顺序呈现给用户，在深度排序模型中，与召回模型不同，为了综合考虑各方面的因素，需要更精细的特征例如点赞数，评论数，转发数等。同时，每个视频的召回来源也是非常重要的。 深度排序的架构和深度召回模型很相似，最后用修改过的logistic regression来对每个视频打分，最终的排序依据是一个针对观看时长的简单函数。采取观看时长而不是点击率来进行排序是因为有些“骗子视频”的点击率非常高，但是内容十分糟糕，用户往往点击进去以后很快就退出了。 模型如下图： Feature Representation类别特征主要分为”univalent”和”Multivalent”特征，其中univalent就是单值离散特征，而Multivalent就是多值离散特征，比如最近观看过的视频ID，在这里的处理是过embedding后采取均值处理。 “Feature Engineering” 尽管DNN有这种“自动学习特征”的特性，实际上还是需要大量的人工来提取特征的工作，关于特征工程部分本文不做过多的介绍： Main challenge : 如何将用户的近期行为与候选视频的打分联系起来？ Some hints： 候选集来自哪个channel 用户历史上从这个channel观看了多少个视频 用户上一次观看和候选视频相同topic的视频是什么时间 这个候选视频的来源是什么(pass infromation from candidate generation) Others: 描述与视频被推荐的频次相关的特征也非常重要，例如某视频多次被推荐但是无人观看 “Embedding Categorical Features” 关于高维离散特征，除了上述的处理办法，还有两个trick： 对于出现次数较少的视频不被编码到vocabulary中，以此来减少vocabulary的长度 共享底层的embedding “Normalizing Continuous Featuers” 连续特征归一化这里使用了累积分布(估计跟min-max等差别不大)，也就是概率密度函数的积分： 同时，将归一化结果的根号和平方也作为特征输入。 Model Expected Watch Time使用了”weighted logistic regression”，正样本(标有观看时长的视频)，负样本(观看时长小于某一threshold)，在训练的时候使用观看时长进行加权，对正样本使用其观看时长进行加权，对负样本进行单位加权，这样，最终学习到的目标为: 其中N为训练样本总数，k是正样本的数量，Ti是第i个视频的观看时长，经过推导后，其近似于e的指数，用来近似代替预估观看时长。 Expriments with Hidden Layers CONCLUSIONS模型很简单就不复现了，本文除了讲模型外其他的一些点比较关键吧，比如训练集的选取，每个用户等量训练集来避免倾向活跃用户等，以及”example age”这种指代不明，耐人寻味的东西(因为请教过公司一个之前在youtube做推荐的大佬，说这个东西也就是说说而已。。)等。]]></content>
      <tags>
        <tag>推荐算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Batch Normalization]]></title>
    <url>%2F2018%2F06%2F22%2Fbn%2F</url>
    <content type="text"><![CDATA[背景批标准化（Batch Normalization ）简称BN算法，是为了克服神经网络层数加深导致难以训练而诞生的一个算法。根据ICS理论，当训练集的样本数据和目标样本集分布不一致的时候，训练得到的模型无法很好的泛化。 而在神经网络中，每一层的输入在经过层内操作之后必然会导致与原来对应的输入信号分布不同,,并且前层神经网络的增加会被后面的神经网络不对的累积放大。这个问题的一个解决思路就是根据训练样本与目标样本的比例对训练样本进行一个矫正，而BN算法（批标准化）则可以用来规范化某些层或者所有层的输入，从而固定每层输入信号的均值与方差。 原因我们知道在神经网络训练开始前，都要对输入数据做一个归一化处理，那么具体为什么需要归一化呢？归一化后有什么好处呢？ 原因在于神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因。 对于深度网络的训练是一个复杂的过程，只要网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，训练数据的分布一直在发生变化，那么将会影响网络的训练速度。 我们知道网络一旦train起来，那么参数就要发生更新(每一层的参数更新变化导致分布变化)，除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。以网络第二层为例：网络的第二层输入，是由第一层的参数和input计算得到的，而第一层的参数在整个训练过程中一直在变化，因此必然会引起后面每一层输入数据分布的改变。我们把网络中间层在训练过程中，数据分布的改变称之为：“Internal Covariate Shift”。Paper所提出的算法，就是要解决在训练过程中，中间层数据分布发生改变的情况，于是就有了Batch Normalization，这个牛逼算法的诞生。 原理批标准化一般用在非线性映射（激活函数）之前，对y= Wx + b进行规范化，是结果(输出信号的各个维度)的均值都为0,方差为1,让每一层的输入有一个稳定的分布会有利于网络的训练 在神经网络收敛过慢或者梯度爆炸时的那个无法训练的情况下都可以尝试。 归一化公式： E(xk)指的是每一批训练数据神经元xk的平均值；然后分母就是每一批数据神经元xk激活度的一个标准差了。 这么简单的想法，为什么之前没人用呢？然而其实实现起来并不是那么简单的。其实如果是仅仅使用上面的归一化公式，对网络某一层A的输出数据做归一化，然后送入网络下一层B，这样是会影响到本层网络A所学习到的特征的。打个比方，比如我网络中间某一层学习到特征数据本身就分布在S型激活函数的两侧，你强制把它给我归一化处理、标准差也限制在了1，把数据变换成分布于s函数的中间部分，这样就相当于我这一层网络所学习到的特征分布被你搞坏了，这可怎么办？ 训练 关于DNN中的normalization，大家都知道白化（whitening），只是在模型训练过程中进行白化操作会带来过高的计算代价和运算时间。因此本文提出两种简化方式：1）直接对输入信号的每个维度做规范化（“normalize each scalar feature independently”）；2）在每个mini-batch中计算得到mini-batch mean和variance来替代整体训练集的mean和variance. 论文中提到的方法：变换重构，引入了可学习参数γ、β，这就是算法关键之处： 每一个神经元xk都会有一对这样的参数γ、β。这样其实当： 是可以恢复出原始的某一层所学到的特征的。因此我们引入了这个可学习重构参数γ、β，让我们的网络可以学习恢复出原始网络所要学习的特征分布。最后Batch Normalization网络层的前向传导过程公式就是： 上面的公式中m指的是mini-batch size。 源码实现 1234m = K.mean(X, axis=-1, keepdims=True)#计算均值 std = K.std(X, axis=-1, keepdims=True)#计算标准差 X_normed = (X - m) / (std + self.epsilon)#归一化 out = self.gamma * X_normed + self.beta#重构变换 预测 (1) 一个网络一旦训练完了，就没有了min-batch这个概念了。测试阶段我们一般只输入一个测试样本，看看结果而已。因此测试样本，前向传导的时候，上面的均值u、标准差σ 要哪里来？其实网络一旦训练完毕，参数都是固定的，这个时候即使是每批训练样本进入网络，那么BN层计算的均值u、和标准差都是固定不变的。我们可以采用这些数值来作为测试样本所需要的均值、标准差，于是最后测试阶段的u和σ 计算公式如下： 上面简单理解就是：对于均值来说直接计算所有batch u值的平均值；然后对于标准偏差采用每个batch σB的无偏估计。最后测试阶段，BN的使用公式就是： (2) 根据文献说，BN可以应用于一个神经网络的任何神经元上。文献主要是把BN变换，置于网络激活函数层的前面。在没有采用BN的时候，激活函数层是这样的： 也就是我们希望一个激活函数，比如s型函数s(x)的自变量x是经过BN处理后的结果。因此前向传导的计算公式就应该是： 其实因为偏置参数b经过BN层后其实是没有用的，最后也会被均值归一化，当然BN层后面还有个β参数作为偏置项，所以b这个参数就可以不用了。因此最后把BN层+激活函数层就变成了： 我们引入一些 batch normalization 的公式. 这三步就是我们在刚刚一直说的 normalization 工序, 但是公式的后面还有一个反向操作, 将 normalize 后的数据再扩展和平移. 原来这是为了让神经网络自己去学着使用和修改这个扩展参数 gamma, 和 平移参数 β, 这样神经网络就能自己慢慢琢磨出前面的 normalization 操作到底有没有起到优化的作用, 如果没有起到作用, 我就使用 gamma 和 belt 来抵消一些 normalization 的操作. 优点 减少了参数的人为选择,可以取消dropout和L2正则项参数,或者采取更小的L2正则项约束参数 减少了对学习率的要求，你可以选择比较大的初始学习率，让你的训练速度飙涨。以前还需要慢慢调整学习率，甚至在网络训练到一半的时候，还需要想着学习率进一步调小的比例选择多少比较合适，现在我们可以采用初始很大的学习率，然后学习率的衰减速度也很大，因为这个算法收敛很快。当然这个算法即使你选择了较小的学习率，也比以前的收敛速度快，因为它具有快速训练收敛的特性； 可以不再使用局部响应归一化了,BN本身就是归一化网络(局部响应归一化-AlexNet) 更破坏原来的数据分布,一定程度上缓解过拟合 使用BN在TensorFlow中主要有两个函数:tf.nn.moments以及tf.nn.batch_normalization,两者需要配合使用,前者用来返回均值和方差,后者用来进行批处理(BN) tf.nn.moments TensorFlow中的函数 123456789moments( x, axes, shift=None, name=None, keep_dims=False) Returns: Two `Tensor` objects: `mean` and `variance`. 其中参数 x 为要传递的tensor, axes是个int数组,传递要进行计算的维度,返回值是两个张量: mean and variance,我们需要利用这个函数计算出BN算法需要的前两项,公式见前面的原理部分 参考代码如下: 12345# 计算Wx_plus_b 的均值与方差,其中axis = [0] 表示想要标准化的维度img_shape= [128, 32, 32, 64]Wx_plus_b = tf.Variable(tf.random_normal(img_shape))axis = list(range(len(img_shape)-1)) # [0,1,2] wb_mean, wb_var = tf.nn.moments(Wx_plus_b, axis) 我们已经假设图片的shape[128, 32, 32, 64],它的运算方式如图: tf.nn.batch_normalizationTensorFlow中的函数 123456789batch_normalization( x, mean, variance, offset, scale, variance_epsilon, name=None) 其中x为输入的tensor,mean,variance由moments()求出,而offset,scale一般分别初始化为0和1,variance_epsilon一般设为比较小的数字即可,参考代码如下: 123456789scale = tf.Variable(tf.ones([64]))offset = tf.Variable(tf.zeros([64]))variance_epsilon = 0.001Wx_plus_b = tf.nn.batch_normalization(Wx_plus_b, wb_mean, wb_var, offset, scale, variance_epsilon)# 根据公式我们也可以自己写一个Wx_plus_b1 = (Wx_plus_b - wb_mean) / tf.sqrt(wb_var + variance_epsilon)Wx_plus_b1 = Wx_plus_b1 * scale + offset# 因为底层运算方式不同,实际上自己写的最后的结果与直接调用tf.nn.batch_normalization获取的结果并不一致 使用BN的深层原因说到底，BN的提出还是为了克服深度神经网络难以训练的弊病。其实BN背后的insight非常简单，只是在文章中被Google复杂化了。首先来说说“Internal Covariate Shift”。文章的title除了BN这样一个关键词，还有一个便是“ICS”。大家都知道在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如，transfer learning/domain adaptation等。而covariate shift就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同，即：对所有,，但是. 大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。那么好，为什么前面我说Google将其复杂化了。其实如果严格按照解决covariate shift的路子来做的话，大概就是上“importance weight”（ref）之类的机器学习方法。可是这里Google仅仅说“通过mini-batch来规范化某些层/所有层的输入，从而可以固定每层输入信号的均值与方差”就可以解决问题。如果covariate shift可以用这么简单的方法解决，那前人对其的研究也真真是白做了。此外，试想，均值方差一致的分布就是同样的分布吗？当然不是。显然，ICS只是这个问题的“包装纸”嘛，仅仅是一种high-level demonstration。那BN到底是什么原理呢？说到底还是为了防止“梯度弥散”。关于梯度弥散，大家都知道一个简单的栗子：。在BN中，是通过将activation规范为均值和方差一致的手段使得原本会减小的activation的scale变大。可以说是一种更有效的local response normalization方法 引用hjmce tianFont]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FFM与FM模型原理与实践]]></title>
    <url>%2F2018%2F05%2F04%2Fffm%2F</url>
    <content type="text"><![CDATA[什么是FM模型？因子分解机(Factorization Machine, FM)是由Steffen Rendle提出的一种基于矩阵分解的机器学习算法，其开源库为libFM，FM模型有以下优势：1.FM模型对稀疏矩阵有非常好的适应性，这点比SVM强很多。2.FM拥有线性的复杂度3.FM具有非常好的适应性所以，其突出的特点就是对高维度稀疏矩阵效果比较好。 FM模型原理FM模型表示为：$$y:=w_0+\sum_{i=1}^n{w_ix_i}+\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}&lt;v_i,v_j&gt;x_ix_j$$前半部分为传统的线性模型，后半部分表示两个互异特征分量之间的关系,$&lt;v_i,v_j&gt;$的表达式如下：$$&lt;v_i,v_j&gt;:=\sum_{f=1}^kv_{i,f}·v_{j,f}$$表示长度为k的两个向量的点积。$v_i$表示的是系数矩阵$V$的第$i$维隐向量。k是定义factorization维度的超参数，是正整数因子分解机FM也可以推广到高阶的形式，即将更多互异特征分量之间的相互关系考虑进来。 先从多项式模型开始看起：$$y:=w_0+\sum_{i=1}^n{w_ix_i}+\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}w_{ij}x_ix_j$$从上述公式可以看出来，组合特征的参数一共有$\frac{n(n-1)}{2}$个，然而，在数据稀疏性普遍存在的实际应用场景中，二次项参数的训练是很困难的。其原因是，每个参数 $w_{ij}$ 的训练需要大量 $x_i$ 和 $x_j$ 都非零的样本；由于样本数据本来就比较稀疏，满足“$x_i$ 和 $x_j$ 都非零”的样本将会非常少。训练样本的不足，很容易导致参数 $w_{ij}$ 不准确，最终将严重影响模型的性能。 那么，如何解决二次项参数的训练问题呢？矩阵分解提供了一种解决思路。在model-based的协同过滤中，一个rating矩阵可以分解为user矩阵和item矩阵，每个user和item都可以采用一个隐向量表示。比如在下图中的例子中，我们把每个user表示成一个二维向量，同时把每个item表示成一个二维向量，两个向量的点积就是矩阵中user对item的打分。类似地，所有二次项参数 $w_{ij}$ 可以组成一个对称阵 $W$（为了方便说明FM的由来，对角元素可以设置为正实数），那么这个矩阵就可以分解为 $W=V^TV$，$V$ 的第 $j$ 列便是第 $j$ 维特征的隐向量。换句话说，每个参数 $w_{ij}=⟨v_i,v_j⟩$，这就是FM模型的核心思想，因此可以得到FM的模型:$$y:=w_0+\sum_{i=1}^n{w_ix_i}+\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}&lt;v_i,v_j&gt;x_ix_j$$其中，$v_i$ 是第 $i$ 维特征的隐向量，$⟨⋅,⋅⟩$ 代表向量点积。隐向量的长度为 k,包含 k 个描述特征的因子，根据上述公式，二次项的参数数量减少为 $kn$个，远少于多项式模型的参数数量。另外，参数因子化使得 $x_hx_i$ 的参数和 $x_ix_j$ 的参数不再是相互独立的，因此我们可以在样本稀疏的情况下相对合理地估计FM的二次项参数。具体来说，$x_hx_i$ 和 $x_ix_j$ 的系数分别为 $⟨v_h,v_i⟩$ 和 $⟨v_i,v_j⟩$，它们之间有共同项 $v_i$。也就是说，所有包含“$x_i$ 的非零组合特征”（存在某个 $j≠i$，使得 $x_ix_j≠0$）的样本都可以用来学习隐向量 vi，这很大程度上避免了数据稀疏性造成的影响。而在多项式模型中，$w_{hi}$ 和 $w_{ij}$ 是相互独立的。 FM复杂度分析直观上看，FM的复杂度是$ O(kn^2)$的，但是通过下式可以化简二次项:可以将复杂度优化到$O(kn)$，所以FM可以在线性时间内对样本做出预测。 FM的训练复杂度：，使用SGD训练模型，各参数的梯度如下： 其中，$v_{j,f}$ 是隐向量 $v_j$ 的第 $f$ 个元素。由于 $\sum_{j=1}^n{v_{j,f}x_j}$ 只与 $f$ 有关，而与$ i $无关，在每次迭代过程中，只需计算一次所有$ f$ 的 $\sum_{j=1}^n{v_{j,f}x_j}$，就能够方便地得到所有 $v_{i,f}$ 的梯度。显然，计算所有 $f$ 的 $\sum_{j=1}^n{v_{j,f}x_j}$ 的复杂度是 $O(kn)$；已知 $\sum_{j=1}^n{v_{j,f}x_j}$时，计算每个参数梯度的复杂度是 $O(1)$；得到梯度后，更新每个参数的复杂度是 $O(1)$；模型参数一共有 $nk+n+1$ 个。因此，FM参数训练的复杂度也是 $O(kn)$。综上可知，FM可以在线性时间训练和预测，是一种非常高效的模型。 FFM模型FFM模型是FM模型的一种升级版，具体体现在引入了”域”field这个概念，相同性质的特征同属于一个field。简单来讲，同一个类别特征生成的one-hot特征都可以放在同一个field中，在FFM中，每一维特征 $x_i，$针对其它特征的每一种field $f_j$，都会学习一个隐向量 $v_i$,$f_j$。因此，隐向量不仅与特征相关，也与field相关。举个例子来说，用户ID One-hot以后产生了500个特征，那么其中“id00001”这个特征就会和Age，gender这两个field去产生不同的隐向量，这与“Age”，“gender”的内在差异相符，这也是FFM种“field-aware”的由来。 FM和FFM的联系：假设样本的 $n $个特征属于 $f$ 个field，那么FFM的二次项有$ nf$个隐向量。而在FM模型中，每一维特征的隐向量只有一个。FM可以看作FFM的特例，是把所有特征都归属到一个field时的FFM模型，即通过引入field的概念，来对产生的隐向量进行细化，从而达到更好的挖掘交叉特征的目的。 根据其field特性，可以得到FFM模型的公式：$$y:=w_0+\sum_{i=1}^n{w_ix_i}+\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}&lt;v_if_j,v_jf_i&gt;x_ix_j$$其中，$f_j$ 是第$ j$ 个特征所属的field。如果隐向量的长度为 $k$，那么FFM的二次参数有 $nfk$ 个，远多于FM模型的 $nk$ 个。此外，由于隐向量与field相关，FFM二次项并不能够化简，其预测复杂度是 $O(kn^2)$。 下面举一个例子： 这条记录可以编码成5个特征，其中“Genre=Comedy”和“Genre=Drama”属于同一个field，“Price”是数值型，不用One-Hot编码转换。为了方便说明FFM的样本格式，我们将所有的特征和对应的field映射成整数编号。那么，FFM的组合特征有10项，如下图所示。其中，红色是field编号，蓝色是特征编号，绿色是此样本的特征取值。二次项的系数是通过与特征field相关的隐向量点积得到的，二次项共有 $\frac{n(n−1)}{2}$ 个。 适用场景主要用于CTR/CVR预估。总体来讲可以分为这几类：(1) 回归问题损失函数为平方损失函数。(2) 分类问题使用logitloss作为优化标准，使用sigmoid函数(3) 排序 FFM训练和参数同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个field。所有的特征必须转换成“field_id:feat_id:value”格式，field_id代表特征所属field的编号，feat_id是特征编号，value是特征的值。数值型的特征比较容易处理，只需分配单独的field编号，如用户评论得分、商品的历史CTR/CVR等。categorical特征需要经过One-Hot编码成数值型，编码产生的所有特征同属于一个field，而特征的值只能是0或1，如用户的性别、年龄段，商品的品类id等。除此之外，还有第三类特征，如用户浏览/购买品类，有多个品类id且用一个数值衡量用户浏览或购买每个品类商品的数量。这类特征按照categorical特征处理，不同的只是特征的值不是0或1，而是代表用户浏览或购买数量的数值。按前述方法得到field_id之后，再对转换后特征顺序编号，得到feat_id，特征的值也可以按照之前的方法获得。 在训练FFM的过程中，有许多小细节值得特别关注。1.样本层面的数据是推荐进行归一化的。2.特征归一化。尤其是数值型特征归一化。CTR/CVR模型采用了多种类型的源特征，包括数值型和categorical类型等。但是，categorical类编码后的特征取值只有0或1，较大的数值型特征会造成样本归一化后categorical类生成特征的值非常小，没有区分性。例如，一条用户-商品记录，用户为“男”性，商品的销量是5000个（假设其它特征的值为零），那么归一化后特征“sex=male”（性别为男）的值略小于0.0002，而“volume”（销量）的值近似为1。特征“sex=male”在这个样本中的作用几乎可以忽略不计，这是相当不合理的。因此，将源数值型特征的值归一化到 [0，1] 是非常必要的。3.省略零值特征。零值特征对模型完全没有贡献。包含零值特征的一次项和组合项均为零，对于训练模型参数或者目标值预估是没有作用的。因此，可以省去零值特征，提高FFM模型训练和预测的速度，这也是稀疏样本采用FFM的显著优势。 FM模型，将参数$w_{ij}$对应的矩阵$W$，利用矩阵分解表示为$W=V^TV$, 矩阵$V∈R^{k×n}$, 可以通过调节$k$来调节模型的泛化能力。 FFM使用这里使用了xlearn库：1234567891011121314151617181920212223242526import xlearn as xl# Training taskffm_model = xl.create_ffm() # Use field-aware factorization machineffm_model.setTrain("./small_train.txt") # Training dataffm_model.setValidate("./small_test.txt") # Validation data# param:# 0. binary classification# 1. learning rate: 0.2# 2. regular lambda: 0.002# 3. evaluation metric: accuracyparam = &#123;'task':'binary', 'lr':0.2, 'lambda':0.002, 'metric':'acc'&#125;# Start to train# The trained model will be stored in model.outffm_model.fit(param, './model.out')# Prediction taskffm_model.setTest("./small_test.txt") # Test dataffm_model.setSigmoid() # Convert output to 0-1# Start to predict# The output result will be stored in output.txtffm_model.predict("./model.out", "./output.txt") 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768Output:---------------------------------------------------------------------------------------------- _ | | __ _| | ___ __ _ _ __ _ __ \ \/ / | / _ \/ _` | '__| '_ \ &gt; &lt;| |___| __/ (_| | | | | | | /_/\_\_____/\___|\__,_|_| |_| |_| xLearn -- 0.31 Version ------------------------------------------------------------------------------------------------[ ACTION ] Read Problem ...[------------] First check if the text file has been already converted to binary format.[------------] Binary file (./small_train.txt.bin) NOT found. Convert text file to binary file.[------------] First check if the text file has been already converted to binary format.[------------] Binary file (./small_test.txt.bin) NOT found. Convert text file to binary file.[------------] Number of Feature: 9991[------------] Number of Field: 18[------------] Time cost for reading problem: 0.00 (sec)[ ACTION ] Initialize model ...[------------] Model size: 5.56 MB[------------] Time cost for model initial: 0.00 (sec)[ ACTION ] Start to train ...[------------] Epoch Train log_loss Test log_loss Test Accuarcy Time cost (sec)[ 10% ] 1 0.594795 0.535269 0.770000 0.00[ 20% ] 2 0.537182 0.551915 0.770000 0.00[ 30% ] 3 0.519317 0.532339 0.770000 0.00[ 40% ] 4 0.507896 0.535436 0.770000 0.00[ 50% ] 5 0.493365 0.534694 0.775000 0.00[ 60% ] 6 0.481961 0.534718 0.775000 0.00[ 70% ] 7 0.470308 0.528452 0.775000 0.00[ 80% ] 8 0.464966 0.534071 0.770000 0.00[ 90% ] 9 0.456130 0.534956 0.770000 0.00[ ACTION ] Early-stopping at epoch 7[ ACTION ] Start to save model ...[------------] Model file: ./model.out[------------] Time cost for saving model: 0.00 (sec)[ ACTION ] Finish training[ ACTION ] Clear the xLearn environment ...[------------] Total time cost: 0.02 (sec)---------------------------------------------------------------------------------------------- _ | | __ _| | ___ __ _ _ __ _ __ \ \/ / | / _ \/ _` | '__| '_ \ &gt; &lt;| |___| __/ (_| | | | | | | /_/\_\_____/\___|\__,_|_| |_| |_| xLearn -- 0.31 Version ------------------------------------------------------------------------------------------------[ ACTION ] Load model ...[------------] Load model from ./model.out[------------] Loss function: cross-entropy[------------] Score function: ffm[------------] Number of Feature: 9991[------------] Number of K: 4[------------] Number of field: 18[------------] Time cost for loading model: 0.00 (sec)[ ACTION ] Read Problem ...[------------] First check if the text file has been already converted to binary format.[------------] Binary file (./small_test.txt.bin) found. Skip converting text to binary.[------------] Time cost for reading problem: 0.00 (sec)[ ACTION ] Start to predict ...[------------] The test loss is: 0.528452[ ACTION ] Clear the xLearn environment ...[------------] Total time cost: 0.00 (sec) 引用美团技术团队zhiyong_willLeo000000001简书]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[稀疏矩阵-使用scipy]]></title>
    <url>%2F2018%2F05%2F01%2FsparseMatrix%2F</url>
    <content type="text"><![CDATA[什么是稀疏矩阵对于那些零元素的数目远远多于非零元素，并且非零元素的分布没有规律的矩阵称为稀疏矩阵(sparse Matrix)，相反的，如果大部分是非0元素，那么为稠密矩阵。 稀疏矩阵的存储格式由于稀疏矩阵中非零元素较少，零元素较多，因此可以采用只存储非零元素的方式来进行压缩存储，采用的一般方法就是三元组，分别来保存非0元素的行号，列号和对应的非0值，常用的存储格式有以下几种： COO(Coordinate Format)-坐标格式COO采用三个数组row、col和data来保存非零元素的信息，这三个数组的长度相同。 优点： 灵活，简单，仅存储坐标和值。 缺点： 不支持增删改，一旦创建之后转换为其他格式的矩阵才能做更多的操作。 图示： 上图示例：row=0,column=1,values=7,则对应于稀疏矩阵中0行1列的位置值为7，所以稀疏矩阵有多少个非0值，三元组的长度就有多长。 CSR(Compressed Sparse Row Format)-压缩稀疏行格式这种格式是把行的信息压缩存储了，CSR由三个数组决定:values,columns,row_ptr 数组values：保存非0实数，按行优先保存(即从0行到最后一行，每一行从左到右的顺序) Columns: 和values长度相同，给出每个非0值所在的列标 Row_ptr: 长度为稀疏矩阵的行数，保存稀疏矩阵的每行第一个非零元素在values的索引。 图示： 如上图所示，val和col_ind很好理解，row_ptr第一个值1代表第一行的第一个非0元素在val中的位置为1，或者说是截止到这一行的第一个非0元素，一共出现了n个非0元素，以此类推，第二行的3出现时，一共有3个非零元素，这样就知道每一行有多少个元素了，根据列号可以还原出稀疏矩阵来，以此来压缩行信息。 BSR(Block Compressed Sparse Row Format)-分块压缩分块压缩其实是基于CSR的，可以分块压缩的稀疏矩阵要求能分解成小的矩阵块，然后再按照CSR的原理进行压缩 图示： 上面的稀疏矩阵的BSR格式为：values = (1 0 2 1 6 7 8 2 1 4 5 1 4 3 0 0 7 2 0 0)columns = (0 1 1 1 2)pointerB = (0 2 3)pointerE = (2 3 5) 其他格式DIA(Diagonal Storage Format)-对角线格式ELLPACK (ELL)Hybrid (HYB)dok_matrixlil_matrix 不同压缩存储格式的优缺点1.BSR-分块压缩矩阵BSR更适合于有密集子矩阵的稀疏矩阵，分块矩阵通常出现在向量值有限的离散元中，在这种情景下，比CSR和CSC算术操作更有效。 2.CSC-列压缩格式高效的CSC +CSC,CSC*CSC算术运算；高效的列切片操作。但是矩阵内积操作没有CSR,BSR快；行切片操作慢（相比CSR）；稀疏结构的变化代价高（相比LIL 或者 DOK）。 3.CSR-行压缩格式高效的CSR + CSR, CSR *CSR算术运算；高效的行切片操作；高效的矩阵内积内积操作。但是列切片操作慢（相比CSC）；稀疏结构的变化代价高（相比LIL 或者 DOK）。CSR格式在存储稀疏矩阵时非零元素平均使用的字节数(Bytes per Nonzero Entry)最为稳定（float类型约为8.5，double类型约为12.5）。CSR格式常用于读入数据后进行稀疏矩阵计算。 4.COO-坐标压缩便利快捷的在不同稀疏格式间转换；允许重复录入，允许重复的元素；从CSR\CSC格式转换非常快速，COO格式常用于从文件中进行稀疏矩阵的读写，如matrix market即采用COO格式。例如某个CSV文件中可能有这样三列：“用户ID，商品ID，评价值”。采用numpy.loadtxt或pandas.read_csv将数据读入之后，可以通过coo_matrix快速将其转换成稀疏矩阵：矩阵的每行对应一位用户，每列对应一件商品，而元素值为用户对商品的评价。但是coo_matrix不支持元素的存取和增删，一旦创建之后，除了将之转换成其它格式的矩阵，几乎无法对其做任何操作和矩阵运算。 5.DIA-对角压缩对角存储格式(DIA)和ELL格式在进行稀疏矩阵-矢量乘积(sparse matrix-vector products)时效率最高，所以它们是应用迭代法(如共轭梯度法)解稀疏线性系统最快的格式；DIA格式存储数据的非零元素平均使用的字节数与矩阵类型有较大关系，适合于StructuredMesh结构的稀疏矩阵（float类型约为4.05，double类型约为8.10）。对于Unstructured Mesh以及Random Matrix，DIA格式使用的字节数是CSR格式的十几倍。 综合来讲1.COO和CSR格式比起DIA和ELL来，更加灵活，易于操作；2.ELL的优点是快速，而COO优点是灵活，二者结合后的HYB格式是一种不错的稀疏矩阵表示格式；3.CSR格式在存储稀疏矩阵时非零元素平均使用的字节数(Bytes per Nonzero Entry)最为稳定（float类型约为8.5，double类型约为12.5），而DIA格式存储数据的非零元素平均使用的字节数与矩阵类型有较大关系，适合于StructuredMesh结构的稀疏矩阵（float类型约为4.05，double类型约为8.10，对于Unstructured Mesh以及Random Matrix,DIA格式使用的字节数是CSR格式的十几倍；4.一些线性代数计算库：COO格式常用于从文件中进行稀疏矩阵的读写，如matrix market即采用COO格式，而CSR格式常用于读入数据后进行稀疏矩阵计算。 Scipy实践官方文档 稀疏矩阵类 1234567891011121314# 新建一个稀疏矩阵import pandas as pdimport numpy as npfrom scipy import sparsesparse_array = np.array([ [1,0,0,0,8,11], [0,0,4,3,1,0], [3,0,0,0,0,0], [18,0,0,1,0,0]])# 转换为coo存储格式sparse_coo = sparse.coo_matrix(sparse_array)# 查看类型type(sparse_coo) 1Output:scipy.sparse.coo.coo_matrix 1234# scipy.sparse.spmatrix 是所有存储格式类的基类# 它有两个属性，'nnz'非0元素的个数，'shape'矩阵的shapeprint(sparse_coo.nnz)print(sparse_coo.shape) 12Output: 9(4, 6) 比较常用的method：tocoo(),tocsc(),tocsr()…. 构建稀疏矩阵 eye(m[, n, k, dtype, format]) - 构建一个对角线值1为1的矩阵 1234567sparse.eye(4,4).toarray()Output:array([[ 1., 0., 0., 0.], [ 0., 1., 0., 0.], [ 0., 0., 1., 0.], [ 0., 0., 0., 1.]]) 2.identity(n[, dtype, format]) - 返回一个nxn的对角线矩阵，跟上面那个差不多1234567sparse.identity(4, dtype='float32', format='csc').toarray()Output:array([[ 1., 0., 0., 0.], [ 0., 1., 0., 0.], [ 0., 0., 1., 0.], [ 0., 0., 0., 1.]], dtype=float32) 3.kron(A, B[, format]) - 计算稀疏矩阵A和B的克罗内克积12# format指定返回的压缩格式sparse.kron(sparse_coo,sparse_coo,format='csc').toarray() 4.kronsum(A, B[, format]) - 计算矩阵的克罗内克和 5.diags(diagonals[, offsets, shape, format, dtype]) - 从对角线构建一个稀疏矩阵12345678910# 使用起来比较灵活，有很多种定义的方法，但是都要使用diagnols和offset来定义（offset是离对角线的距离）offset = [-1,0,1]diagnols = [[1,2,1],[-2,1,-2,1],[2,3,3]]sparse.diags(diagnols,offset).toarray()Output:array([[-2., 2., 0., 0.], [ 1., 1., 3., 0.], [ 0., 2., -2., 3.], [ 0., 0., 1., 1.]]) 6.spdiags(data, diags, m, n[, format]) - 利用原矩阵和offset构建对角矩阵 12345678910# offset长度要和行数对应，把每一行放到了第i个offset的对角线上。sparse.spdiags(sparse_array,[0,-1,1,2],6,6).toarray()Output:array([[ 1, 0, 0, 0, 0, 0], [ 0, 0, 0, 1, 0, 0], [ 0, 0, 0, 0, 0, 0], [ 0, 0, 4, 0, 0, 0], [ 0, 0, 0, 3, 8, 0], [ 0, 0, 0, 0, 1, 11]]) 7.scipy.sparse.block_diag - 根据提供的子矩阵构建块矩阵1234567891011A = coo_matrix([[1, 2], [3, 4]])B = coo_matrix([[5], [6]])C = coo_matrix([[7]])block_diag((A, B, C)).toarray()Output:array([[1, 2, 0, 0], [3, 4, 0, 0], [0, 0, 5, 0], [0, 0, 6, 0], [0, 0, 0, 7]]) 8.scipy.sparse.tril - 得到距离中心对角线距离为k的对角线以下的矩阵9.scipy.sparse.triu - 得到距离中心对角线距离为k的对角线以上的矩阵10.scipy.sparse.bmat - 根据子矩阵建立稀疏块矩阵11.scipy.sparse.hstack - 在水平方向连接两个稀疏矩阵123456789101112131415161718192021222324sparse_coo.toarray()Output:array([[ 1, 0, 0, 0, 8, 11], [ 0, 0, 4, 3, 1, 0], [ 3, 0, 0, 0, 0, 0], [18, 0, 0, 1, 0, 0]])sparse_coo_2.toarray()Output:array([[2, 2], [0, 0], [0, 1], [1, 2]])sparse.hstack([sparse_coo,sparse_coo_2]).toarray()Output:# 可以看到是水平方向上的拼接,所以两个稀疏矩阵的行数要相等array([[ 1, 0, 0, 0, 8, 11, 2, 2], [ 0, 0, 4, 3, 1, 0, 0, 0], [ 3, 0, 0, 0, 0, 0, 0, 1], [18, 0, 0, 1, 0, 0, 1, 2]], dtype=int64) 11.scipy.sparse.vstack - 在竖直方向上拼接两个稀疏矩阵，与上面的同理，vstack要求两个矩阵的列数(特征数)相等。12.scipy.sparse.rand - 生成具有均匀分布值的给定形状和密度的稀疏矩阵。 123456789101112131415161718192021# 前两个参数是shape,density是密度，密度为0意味着没有非0项(全为0)，密度为1意味着全矩阵(没有0)。sparse.rand(3,3,0.3).toarray()Output:array([[ 0. , 0. , 0.03750317], [ 0. , 0. , 0.16852654], [ 0. , 0. , 0. ]])sparse.rand(3,3,1).toarray()Output:array([[ 0.22234732, 0.37775149, 0.70501589], [ 0.87329993, 0.97699362, 0.62989565], [ 0.23061425, 0.58412748, 0.3593626 ]])sparse.rand(3,3,0).toarray()Output:array([[ 0., 0., 0.], [ 0., 0., 0.], [ 0., 0., 0.]]) 13.find() - 得到保存的存储数组(非0值和索引)123456sparse.find(sparse_coo)Output：(array([0, 2, 3, 1, 1, 3, 0, 1, 0], dtype=int32), array([0, 0, 0, 2, 3, 3, 4, 4, 5], dtype=int32), array([ 1, 3, 18, 4, 3, 1, 8, 1, 11])) 稀疏矩阵的存储和加载1.scipy.sparse.save_npz - 将稀疏矩阵存储为.npz格式 12# 使用起来很简单sparse.save_npz('YourFileName.npz',YourSparseMatrix) 2.scipy.sparse.load_npz12# 直接load保存的.npz文件即可sparse.load_npz('YourFileName.npz') 在使用稀疏矩阵格式如coo,csc等格式的时候，根据官方建议，最好不要使用numpy库里的方法，而是使用sparse库自带的方法，如果要使用numpy的方法，先转换格式。 稀疏表达的意义根据周志华《机器学习》中所说：当样本数据是一个稀疏矩阵时，对学习任务来说会有不少的好处，例如很多问题变得线性可分，储存更为高效等。这便是稀疏表示与字典学习的基本出发点。稀疏矩阵即矩阵的每一行列中都包含了大量的零元素，且这些零元素没有出现在同一行/列，对于一个给定的稠密矩阵，若我们能通过某种方法找到其合适的稀疏表示，则可以使得学习任务更加简单高效，我们称之为稀疏编码（sparse coding）或字典学习（dictionary learning）。 稀疏矩阵的场景1.例如CTR预估中会有大量的category特征和id特征，one-hot以后维度会非常高2.例如对文本特征的提取过程3.图像在小波变换，梯度算子下是（近似）稀疏的；4.分类过程中需要输入在不同的基下面表达不同，这是稀疏性；5.Deep Learning在不断地提出feature的过程也是稀疏性；6.推荐系统背后是因为用户产品评价是一个低秩矩阵； 引用孙华强 某小皮 知乎]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow-regression]]></title>
    <url>%2F2018%2F04%2F24%2Ftensorflow-regression%2F</url>
    <content type="text"><![CDATA[线性回归这里使用天池糖尿病比赛的数据,数据量为6633条数据,90维。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import numpy as npimport pandas as pdimport tensorflow as tffrom sklearn.preprocessing import MinMaxScaler# 导入数据train = pd.read_csv('trainSet.csv')y = train['血糖']train = train.drop('血糖',axis=1)# 填补空值train = train.fillna(method = 'bfill')train = train.fillna(method = 'pad')# 归一化scaler = MinMaxScaler()train = scaler.fit_transform(train)# 定义占位符trainSet = tf.placeholder(tf.float32,shape=[None,90])train_y = tf.placeholder(tf.float32,shape=[None])# 定义线性回归的变量 W and bW = tf.Variable(tf.random_normal(shape=[90,1]))b = tf.Variable(tf.random_normal(shape=[1]))# 定义相关参数learning_rate = 0.05batch_size = 50iterations = 200# 定义目标函数f = tf.add(tf.matmul(trainSet,W),b)# 声明损失函数，这里用平均方差作为损失函数loss = tf.reduce_mean(tf.square(train_y-f))# 声明优化函数,梯度下降opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)train_step = opt.minimize(loss)# 开始迭代训练with tf.Session() as sess: sess.run(tf.global_variables_initializer()) # 可视化 writer = tf.summary.FileWriter("", sess.graph) for i in range(iterations): sess.run(train_step,feed_dict=&#123;train_y:y,trainSet:train&#125;) # 打印当前的损失 temp_loss = sess.run(loss,feed_dict=&#123;train_y:y,trainSet:train&#125;) print(temp_loss)writer.close() 输出1234567891011121314151617181920217.053576.784936.573176.378996.191796.011435.837285.671365.514425.36657....2.137922.136942.135952.134952.134022.133032.13209 Tensorboard 计算图 Tensorflow命名空间可以看到TensorBoard比较混乱，可以使用name_scope“命名空间”来对代码进行整理12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import numpy as npimport pandas as pdimport tensorflow as tffrom sklearn.preprocessing import MinMaxScaler# 导入数据with tf.name_scope("input_preProcess"): train = pd.read_csv('trainSet.csv') y = train['血糖'] train = train.drop('血糖',axis=1) # 填补空值 train = train.fillna(method = 'bfill') train = train.fillna(method = 'pad') # 归一化 scaler = MinMaxScaler() train = scaler.fit_transform(train) # 定义占位符 trainSet = tf.placeholder(tf.float32, shape=[None, 90]) train_y = tf.placeholder(tf.float32, shape=[None])with tf.name_scope("inference"): # 定义线性回归的变量 W and b W = tf.Variable(tf.random_normal(shape=[90,1])) b = tf.Variable(tf.random_normal(shape=[1])) # 定义相关参数 learning_rate = 0.05 batch_size = 50 iterations = 200with tf.name_scope("object_function"): # 定义目标函数 f = tf.add(tf.matmul(trainSet,W),b)with tf.name_scope("Loss"): # 声明损失函数，这里用平均方差作为损失函数 loss = tf.reduce_mean(tf.square(train_y-f))with tf.name_scope("optimizer"): # 声明优化函数,梯度下降 opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate) train_step = opt.minimize(loss)# 开始迭代训练with tf.Session() as sess: sess.run(tf.global_variables_initializer()) # 可视化 writer = tf.summary.FileWriter("", sess.graph) for i in range(iterations): sess.run(train_step,feed_dict=&#123;train_y:y,trainSet:train&#125;) # 打印当前的损失 temp_loss = sess.run(loss,feed_dict=&#123;train_y:y,trainSet:train&#125;) print(temp_loss)writer.close() 更新后的TensorBoard视图: TF实现MNIST手写数字识别1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import tensorflow as tfimport numpy as npfrom tensorflow.examples.tutorials.mnist import input_data# 读入数据MNIST = input_data.read_data_sets("data", one_hot=True)# 定义相关超参eta = 0.01batch_size = 128n_epochs = 25# 为输入和输出定义placeholder# 因为每个在MNIST中的像素是28*28 = 784# 所以每一个图像都有784个特征，是一个1x784的张量X = tf.placeholder('float32',[batch_size,784])y = tf.placeholder('float32',[batch_size,10])# 创建要调整的参数变量 weights and bias# 784，10 是因为要输出一个长度为10的结果向量w = tf.Variable(tf.random_normal(shape=[784,10],stddev=0.1), name='weights')b = tf.Variable(tf.zeros([1,10]), name='bias')# 定义假设函数logits = tf.matmul(X,w) + b# 定义损失函数entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y)loss = tf.reduce_mean(entropy)# 梯度下降optimizer = tf.train.GradientDescentOptimizer(learning_rate=eta).minimize(loss)# 初始化变量sess = tf.Session()sess.run(tf.global_variables_initializer())# 这里batch_size是每一次训练多少个样本，这里要算每一轮要训练多少次n_batches = int(MNIST.train.num_examples/batch_size)# 训练多少轮for i in range(n_epochs): for _ in range(n_batches): # 该循环的每个步骤中，我们都会随机抓取训练数据中的batch_size个批处理数据点，然后我们用这些数据点作为参数替换之前的占位符来运行train_step X_batch,Y_batch = MNIST.train.next_batch(batch_size) sess.run([optimizer,loss],&#123;X:X_batch,y:Y_batch&#125;)# 测试模型corrects = 0n_batches = int(MNIST.test.num_examples/batch_size)total_correct_preds = 0for i in range(n_batches): X_batch, Y_batch = MNIST.test.next_batch(batch_size) _,loss_batch,logits_batch = sess.run([optimizer,loss,logits],feed_dict=&#123;X:X_batch,y:Y_batch&#125;) preds = tf.nn.softmax(logits_batch) corrects_preds = tf.equal(tf.argmax(preds,1),tf.argmax(Y_batch,1)) print(sess.run(corrects_preds)) accuracy = tf.reduce_sum(tf.cast(corrects_preds,tf.float32)) print(sess.run(accuracy)) total_correct_preds += sess.run(accuracy)print('Accuracy',total_correct_preds/MNIST.test.num_examples) 1Output:Accuracy 0.9076 (github挂了看不了文档，未完待续) 引用GuanghaoChen_NEU]]></content>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow-可视化]]></title>
    <url>%2F2018%2F04%2F24%2Ftensorboard%2F</url>
    <content type="text"><![CDATA[TensorboardTensorboard是tensorflow中的可视化模块，可以对训练过程中的统计指标，曲线图，计算图，损失，准确度等进行可视化 TensorBoard官方文档使用起来非常简单，这里先使用之前的例子基础上进行可视化1234567891011121314151617181920212223242526272829303132333435363738import tensorflow as tfimport numpy as np# 定义张量 4x3 and 3x2A = tf.random_uniform([4, 3])C = tf.random_uniform([4, 3])B = tf.random_uniform([3, 2])# 生成2行4列服从标准正态分布的随机数A_np = np.random.randn(4, 3)B_np = np.random.randn(3, 2)# 定义占位符A_placeholder = tf.placeholder('float32', shape=[4, 3])B_placeholder = tf.placeholder('float32', shape=[3, 2])mul_1 = tf.matmul(A, B)mul_2 = tf.matmul(A_placeholder, B_placeholder)#定义自定义函数def mulTwoMatrix(M_a,M_b): return tf.matmul(M_a,M_b)# 定义sessionwith tf.Session() as sess: # 创建可视化对象 writer = tf.summary.FileWriter("/", sess.graph) # 直接打印张量A print(sess.run(A)) # 打印placeholder_A,由A_np赋值 print(sess.run(A_placeholder, feed_dict=&#123;A_placeholder: A_np&#125;)) # 打印张量A乘以张量B print(sess.run(mul_1)) # 打印placeholder_A 乘以 placeholder_B print(sess.run(mul_2, feed_dict=&#123;A_placeholder: A_np, B_placeholder: B_np&#125;)) # 打印张量A的四舍五入的结果 print(sess.run(tf.round(A))) # 调用函数打印placeholder_A 乘以 placeholder_B,结果应该是一样的 print(sess.run(mulTwoMatrix(A_placeholder,B_placeholder),feed_dict=&#123;A_placeholder: A_np, B_placeholder: B_np&#125;)) # 卧槽，发现结果不一样，再次调用看看 print(sess.run(mulTwoMatrix(A_placeholder, B_placeholder), feed_dict=&#123;A_placeholder: A_np, B_placeholder: B_np&#125;))writer.close() 在原来的基础上添加了一个writter，这里我直接把指定的log保存位置为当前文件夹下(将会和我的.py文件在同一目录下):1writer = tf.summary.FileWriter("/", sess.graph) 最后别忘记关掉writter:1writer.close() 然后运行程序，会生成这样的一个log文件，保存你运行过程中的信息：1events.out.tfevents.1524554706.yangyiqingdeMacBook-Pro.local 然后到终端，进入该文件上一级文件夹位置，执行12# 我的log文件位于 cookBook 文件夹中tensorboard --logdir cookBook 如果出现如下提示信息，说明成功12tensorboard --logdir cookBookTensorBoard 0.1.7 at http://yangyiqingdeMacBook-Pro.local:6006 (Press CTRL+C to quit) 在浏览器打开TensorBoard即可1localhost:6006 如下图所示，对应博客[Tensorflow基础]中最后的例子： 主要概念由于先前的例子太简单，等后续关于Tensorboard再补充。]]></content>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TF_CookBook-Chapter1]]></title>
    <url>%2F2018%2F04%2F24%2Fcookbook%2F</url>
    <content type="text"><![CDATA[Tensorflow 基础首先要明确tensorflow的工作原理 Tensorflow 工作原理首先准备数据集，变量，占位符，选定机器学习模型，损失函数，优化函数，然后模型训练，调参。Tensorflow通过计算图来实现上述过程，这些计算图是有向无环图，并且支持并行运算 Tensor 张量Tensorflow从字面看就是基于tensor，也是TF中一个非常基础和重要的概念“张量”来构建的，张量包含了0到任意维度的量，其中，0维的叫做常数，1维的叫做向量，二维叫做矩阵，多维度的就直接叫张量，而tensorflow的整个流程“计算图”也是基于张量来操作的。注意，仅仅创建一个张量，其不会被添加到计算图中，只有把张量赋值给一个变量或者占位符1234567891011121314151617181920212223242526272829# 将任意维度的numpy数组转为张量converted = tf.convert_to_sensor(numpy数组)# 创建指定维度的0张量zero_tar = tf.zeros([行数，列数])# 创建指定维度单位张量ones_tar = tf.ones([行数，列数])# 创建指定维度的常数填充的张量filled_tar = tf.fill([行数，列数]，要填充的值)# 创建一个常数张量constant_tar = tf.constant([1,2,3,4,5])# 创建一个与给定张量形状一样的张良zeros_similar = tf.zeros_like(给定张量)ones_similar = tf.ones_like(给定张量)# 创建序列张量 其中前两个值是起始和结束值（包含在内），最后一个值生成的张量长度linear_tsr = tf.linspace(0.,1.,4)&gt;&gt;&gt;&gt;[ 0. 0.33333334 0.66666669 1. ]# 类似用法 range,包含起始值，不包含结束值，最后一个参数是间隔integer_seq_tsr = tf.range(6,15,3)&gt;&gt;&gt;&gt;[ 6 9 12]# 创建随机张量randunif_tsr = tf.random_uniform([行数，列数],minval=0,maxval=1)#eg.# randomTsr = tf.random_uniform([4,5],0,2)#[[ 1.95798659 1.14590573 0.71785665 1.73604393 0.59747076]# [ 1.58351803 0.64294863 1.01357007 0.17978501 0.50106215]# [ 1.66894627 1.26074719 0.00528693 1.53832388 0.03066635]# [ 0.84991193 1.98108554 1.35822701 1.57819223 1.25322342]]# 创建正太分布的随机数randnorm = tf.random_normal([row_dim,col_dim],mean=0.,stddev=1.) 对张量的每个元素进行操作主要有四个函数：add(),sub(),mul(),div(),参数为张量1234567891011121314151617# div() and truediv()#这两个函数的主要区别是div()返回的是跟原数据类型一致的结果，本来是整数型那么返回的结果也会转为整数型print(tf.truediv(张量A))#下面还有一些比较重要的函数tf.abs() # 张量的绝对值tf.ceil() # 向上取整tf.cos() # 余弦值tf.exp() # e的指数tf.floor() # 向下取整tf.log() # 自然对数tf.maximum() # 比较两个张量的大小，必须shape相同tf.minimum() # 同理tf.neg() # 负数tf.pow() # (张量，次方)tf.round() # 四舍五入tf.sign() # 返回符号 -1,0,1,返回同维度，可以来判断张量中正负值个数tf.square() # 平方 Placeholder 占位符常常和feed_dict一起用，它表示tensorflow计算图中的输入和输出，要和Variable区分开,PlaceHolder可以接受的数据类型有:Python scalars, strings, lists, numpy ndarrays, or TensorHandles,注意，没有Tensor类型！1234567x = tf.placeholder(tf.string)y = tf.placeholder(tf.int32)z = tf.placeholder(tf.float32)with tf.Session() as sess: # feed_dict指明喂给各个placeholder的数据 output = sess.run(x, feed_dict=&#123;x: 'Test String', y: 123, z: 45.67&#125;) Variable 变量需要注意的地方是变量一定要初始化才行12345my_var = tf.Variable(tf.zeros[2,3])sess = tf.Session()# 变量需要初始化initialize_op = tf.global_variables_initializer()sess.run(initialize_op) Session 图会话在启动图会话之前，tensorflow的所有操作都是在往计算图中添砖加瓦，并没有产生实际的运算，只有当启动一个会话以后(session -&gt; run),才开始执行整个计算图。有两种调用会话的方式1. 明确的调用会话的生成函数和关闭会话函数12345678# create a sessionsess = tf.Session()# use this session to run a resultsess.run(...)# close this session, release memeorysess.close() 这种方式需要手动的调用关闭sess的函数来释放资源。2. 上下文管理机制自动释放所有资源12345# 创建会话，并通过上下文机制管理器管理该会话with tf.Session() as sess: sess.run(...)# 不需要再调用"Session.close()"# 在退出with statement时，会话关闭和资源释放已自动完成 3.默认计算图1234sess = tf.Session()with sess.as_default(): # result为某个张量 print(result.eval()) 以上，最常用的还是方式二，但这三种方式都可以通过ConfigProto Protocol Buffer来配置需要生成的会话，如并行线程数、GPU分配策略、运算超时时间 等参数，最常用的两个是allow_soft_placement和log_device_placement.ConfigProto配置方法：12345678config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)sess1 = tf.InteractiveSession(config=config)sess2 = tf.Session(config=config)#allow_soft_placement，布尔型，一般设置为True，很好的支持多GPU#或者不支持GPU时自动将运算放到CPU上。#log_device_placement，布尔型，为True时日志将会记录每个节点被#安排在了哪个设备上以方便调试。在生产环境下，通常设置为False可以减少日志量。 矩阵操作1234567891011121314# 矩阵加减法sess.run(A+B)# 矩阵乘法sess.run(tf.matmul(A,B))# 矩阵转置sess.run(tf.ranspose(A))# 矩阵行列式sess.run(tf.matrix_determinant(D))# 矩阵的逆矩阵sess.run(tf.matrix_inverse(D))# 矩阵分解,矩阵需要为对称正定矩阵或者可进行LU分解sess.run(tf.cholesky(identity_matrix))# 矩阵的特征分解（分解为特征值和特征向量），第一行为特征值，剩下的向量是对应的向量sess.run(tf.self_adjoint_eig(D)) 自定义函数例如:123def addTwo(A,B) return tf.add(A,B)print(sess.run(addTwo(2,3))) 输出15 综合例子占位符和tf.Variable可以直接run，但是涉及到placeholder就要给出相应的feed_dict1234567891011121314151617181920212223242526272829303132import tensorflow as tfimport numpy as np# 定义张量 4x3 and 3x2A = tf.random_uniform([4, 3])B = tf.random_uniform([3, 2])# 生成2行4列服从标准正态分布的随机数A_np = np.random.randn(4, 3)B_np = np.random.randn(3, 2)# 定义占位符A_placeholder = tf.placeholder('float32', shape=[4, 3])B_placeholder = tf.placeholder('float32', shape=[3, 2])# 注意tf中的矩阵乘法是tf.matmul不是tf.multiplymul_1 = tf.matmul(A, B)mul_2 = tf.matmul(A_placeholder, B_placeholder)#定义自定义函数def mulTwoMatrix(M_a,M_b): return tf.matmul(M_a,M_b)# 定义sessionwith tf.Session() as sess: # 直接打印张量A print(sess.run(A)) # 打印placeholder_A,由A_np赋值 print(sess.run(A_placeholder, feed_dict=&#123;A_placeholder: A_np&#125;)) # 打印张量A乘以张量B print(sess.run(mul_1)) # 打印placeholder_A 乘以 placeholder_B print(sess.run(mul_2, feed_dict=&#123;A_placeholder: A_np, B_placeholder: B_np&#125;)) # 打印张量A的四舍五入的结果 print(sess.run(tf.round(A))) # 调用函数打印placeholder_A 乘以 placeholder_B,结果应该是一样的 print(sess.run(mulTwoMatrix(A_placeholder,B_placeholder),feed_dict=&#123;A_placeholder: A_np, B_placeholder: B_np&#125;)) 输出123456789101112131415161718192021222324252627282930# 直接打印张量A[[ 0.45307124 0.62509346 0.25148225] [ 0.41434121 0.58660126 0.9465214 ] [ 0.37218738 0.91929305 0.71940351] [ 0.84926152 0.63980663 0.27016711]]# 打印placeholder_A,由A_np赋值[[ 0.61854458 0.97045314 -0.75675184] [ 0.39336792 0.64061081 -0.26375616] [-0.27992743 -0.12586813 -0.40246865] [ 0.54720771 0.61397451 0.66880828]]# 打印张量A乘以张量B[[ 0.7251178 0.7453838 ] [ 0.40002131 0.39979655] [ 0.18994027 0.32228675] [ 0.96875006 1.10299659]]# 打印placeholder_A 乘以 placeholder_B[[-0.46408546 -2.08722115] [-0.02081895 -1.0122509 ] [-0.20734093 -0.50428039] [ 0.76150823 0.45751405]]# 打印张量A的四舍五入的结果[[ 0. 1. 1.] [ 0. 1. 1.] [ 1. 1. 0.] [ 1. 1. 1.]]# 调用函数打印placeholder_A 乘以 placeholder_B,结果应该是一样的[[-0.46408546 -2.08722115] [-0.02081895 -1.0122509 ] [-0.20734093 -0.50428039] [ 0.76150823 0.45751405]] 引用hukai7190]]></content>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梯度提升树GBDT]]></title>
    <url>%2F2018%2F03%2F20%2Fgbdt%2F</url>
    <content type="text"><![CDATA[提升树提升树正是一种前向分步的加法模型，但是其基分类器是树模型(二叉树)，分为二叉分类树和二叉回归树，提升树往往在实践中表现非常好。提升树的模型如下:$$f_M(x)=\sum_{m=1}^MT(x;\Theta)$$$M$为树的个数，$\Theta$表示数的参数(在加法模型中的重要性) 提升树算法由于提升树是一种前向分步的加法模型，所以其算法步骤就是前向分布加法模型的算—title: 梯度提升树GBDTdate: 2018-03-15 21:00:17tags: 机器学习 description: 提升树算法，GBDT原理，损失函数提升树提升树正是一种前向分步的加法模型，但是其基分类器是树模型(二叉树)，分为二叉分类树和二叉回归树，提升树往往在实践中表现非常好。提升树的模型如下:$$f_M(x)=\sum_{m=1}^MT(x;\Theta)$$$M$为树的个数，$\Theta$表示数的参数(在加法模型中的重要性) 提升树算法由于提升树是一种前向分步的加法模型，所以其算法步骤就是前向分布加法模型的算法步骤。解决分类问题的时候，其基学习器为二叉分类树，其损失函数为指数函数，这个时候就相当于Adaboost将基分类器替换为二叉分类树。解决回归问题的时候，其基学习器为二叉回归树，其损失函数为平方损失函数，下面介绍一下回归提升树的算法。第m颗树的模型可以表示为：$$f_m(x)=f_{m-1}(x)+T(x; \theta_m)$$其中，$\Theta_m$是这颗树的参数，它是通过上一颗树 $f_{m-1}(x)$计算出来的$$\theta_{m+1} = arg\min_{\theta_m}\sum_{i=1}^N L(y_i, f_{m-1}(x_i)+T(x_i; \theta_m)$$不难发现，这里就是前向分步算法中求当前学习器的参数的过程，使得当前的损失函数最小，也就是经验风险最小化策略。其中，二叉回归树模型为,就是(每个小区域的平均值*$Indicator函数$)，可以点击这里查看关于二叉树的生成过程:$$T(x; \theta)=\sum_{j=1}^J C_j I(x \in R_j)$$于是,第m颗树的平方损失函数可以表示为： $Loss(y,f_m(x)) = (y-f_m(x))^2$ $=(y-f_{m-1}(x)-T(x;\Theta_m))^2$ $=((y-f_{m-1}(x))-T(x;\Theta_m))^2$不难发现，式中前两项就是当前模型拟合数据的残差，所以，对于回归提升树来说，只需要拟合当前残差即可。当前模型的残差公式:$$r_m = y - f_{m-1}(x)$$不理解为什么$Min((y-f_{m-1}(x))-T(x;\Theta_m))^2$相当于拟合残差的可以自己去看一下《统计学习方法》中提升树相关例题。所以其实这也是一种前向分步加法模型，其基分类器为二叉回归树，其损失函数为平方损失函数，求参数的过程采用了使平方损失函数最小化的策略，相当于拟合当前模型的残差。提升树与Adaboost的区别: Adaboost的策略是在每一轮迭代的过程中根据误差率最小化策略来更新样本的权重，使得权重更大的样本在再次被错分的时候损失代价更大。而提升树的策略是为了减少当前模型的”残差”，这里说的残差值得是当前得到的加法模型对训练集的预测和其真正label之间的差异大小 GBDT(梯度提升树)梯度提升树(Gradient Boosting Decison Tree)，GBDT的基分类器是CART树(简单回顾：二叉树，分类损失函数为基尼系数，回归损失函数为平方损失)。 为什么要引入梯度这个概念？因为之前我们用到的损失函数是指数损失函数或者是平方损失函数，这个时候，对于每一轮的弱学习器来说，其损失函数最小化的策略是比较简单的。但是如果损失函数是一般函数呢？其优化策略可能非常复杂，所以引入了梯度提升算法，其利用损失函数的负梯度在当前模型的值作为”残差”的近似值: 第m轮第i个样本的损失函数的负梯度(每个样本可以计算一个梯度):$$r_{mi} = -\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]{f(x) = f{m-1}\;\; (x)}$$然后将负梯度作为残差的近似值，利用和提升树相同的方法，可以拟合得到一颗CART树(当前轮的弱学习器)，拟合的策略也是使当前模型残差最小，在此不再赘述。 最终得到本轮的弱学习器为(就回归树而言):$$T(x; \theta)=\sum_{j=1}^J C_j I(x \in R_j)$$当前模型(强分类器)为：$$f(x) = f_{m-1}(x)+\sum_{j=1}^J C_j I(x \in R_j)$$所以，引入负梯度让我们对损失函数的处理变得统一化，算法只需要基于由负梯度得来的残差进行展开，而这个残差的具体由来，就看具体的损失函数是什么了。 GBDT与LR比较 1.由于GBDT是迭代的学习方法，且下一模型的学习是在前一模型的基础上，因而只能在训练样本的层面上进行并行化处理。关于并行化相关内容等结合Xgboost学习再详细了解。2.GBDT几乎可用于所有回归问题（线性/非线性），相对logistic regression仅能用于线性回归，GBDT的适用面非常广。亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例。3.GBDT 是一个加性回归模型，通过 boosting 迭代的构造一组弱学习器，相对LR的优势如不需要做特征的归一化，自动进行特征选择，模型可解释性较好，可以适应多种损失函数如 SquareLoss，LogLoss 等等。但作为非线性模型，其相对线性模型的缺点也是显然的：boosting 是个串行的过程，不能并行化，计算复杂度较高，同时其不太适合高维稀疏特征，通常采用稠密的数值特征如点击率预估中的 COEC。—引自xgboost导读与实战 GBDT损失函数上面我们讨论了提升树和梯度提升树，了解了引入负梯度的概念是为了解决一般损失函数最优化难的问题。还给出了为什么当损失函数是平方损失函数的时候提升树的损失函数最小化策略相当于拟合模型的当前残差。 那么当损失函数是指数函数的时候，其最小化损失函数的策略是什么呢？其实，当Adaboost的基分类器是二叉分类树的时候，其等价于二叉提升分类树。也就是指数损失函数最小化策略和Adaboost的分类误差率最小化策略是等价的，具体推导请参见这篇博客 GBDT分类常用损失函数(1) 指数损失函数$$L(y, f(x)) = exp(-yf(x))$$其等价于Adaboost(2)对数损失函数对数损失函数在进行二分类和多分类的时候又有区别，下面会讲 GBDT回归常用损失函数(1) 均方误差$$L(y, f(x)) =(y-f(x))^2$$最为常见的回归损失函数了(2) 绝对损失函数$$L(y, f(x)) =|y-f(x)|$$(3) 分位损失函数 (4) Huber损失函数 较为少见，这里不予解释。 GBDT二分类其损失函数类似于逻辑斯蒂回归的对数似然函数：$$L(y, f(x)) = log(1+ exp(-yf(x)))$$其中$y∈{1,-1},此时的残差计算公式为:（我惊了！这里的公式就是显示不出来）? $$r_{ti} = -\bigg[\frac{\partial L(y, f(x_i)))}{\partial f(x_i)}\bigg]{f(x) = f{t-1}\;\; (x)} = y_i/(1+exp(y_if(x_i)))$$各个叶子节点的最佳残差拟合值:$$c_{tj} = \underbrace{arg\; min}{c}\sum\limits{x_i \in R_{tj}} log(1+exp(-y_i(f_{t-1}(x_i) +c)))$$简化为:$$c_{tj} = \sum\limits_{x_i \in R_{tj}}r_{ti}\bigg / \sum\limits_{x_i \in R_{tj}}|r_{ti}|(1-|r_{ti}|)$$ GBDT多分类其中，多分类的对数似然损失函数为:$$L(y, f(x)) = - \sum\limits_{k=1}^{K}y_klog\;p_k(x)$$其在第m轮的第i个样本的负梯度为：$$r_{til} = -\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]_{f_k(x) = f_{l, t-1}\;\; (x)} = y_{il} - p_{l, t-1}(x_i)$$观察上式可以看出，其实这里的误差就是样本$i$对应类别l的真实概率和$t−1$轮预测概率的差值。(如果你理解不了的话可以记住这句话)各个叶子节点的最佳残差拟合值:$$c_{tjl} = \underbrace{arg\; min}_{c_{jl}}\sum\limits_{i=0}^{m}\sum\limits_{k=1}^{K} L(y_k, f_{t-1, l}(x) + \sum\limits_{j=0}^{J}c_{jl} I(x_i \in R_{tj}))$$简化为:$$c_{tjl} = \frac{K-1}{K} \; \frac{\sum\limits_{x_i \in R_{tjl}}r_{til}}{\sum\limits_{x_i \in R_{til}}|r_{til}|(1-|r_{til}|)}$$ 其实gbdt回归与分类损失函数最小化策略跟CART树利用平方损失和基尼系数找最优切分点的道理是一样的，毕竟boosting的前一颗树只是起到了提供残差的作用，每一轮的迭代其实就只有一颗CART树，这里可以回去翻看CART树的生成策略。 GBDT的正则化为了防止过拟合，我们有三种正则化的策略。(1)第一种是和Adaboost类似的正则化项，即步长(learning rate)。定义为ν,对于前面的弱学习器的迭代：$$f_{k}(x) = f_{k-1}(x) + h_k(x)$$如果我们加上了正则化项，则有:$$f_{k}(x) = f_{k-1}(x) + \nu h_k(x)$$$ν$的取值范围为$0&lt;ν≤1$。对于同样的训练集学习效果，较小的ν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。(2)第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。使用了子采样的GBDT有时也称作随机梯度提升树(Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。(3)第三种是对于弱学习器即CART回归树进行正则化剪枝。 GBDT的优缺点优点: 可以灵活处理各种类型的数据，包括连续值和离散值。 在相对少的调参时间情况下，预测的准确率也可以比较高。这个是相对SVM来说的。 使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。缺点： 由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。 本博客内容引用了以下链接内容:1.刘建平2.zhiyong_will3我曾被山河大海跨过]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈boosting与Adaboost]]></title>
    <url>%2F2018%2F03%2F19%2Fadaboost%2F</url>
    <content type="text"><![CDATA[BoostingBoosting算法是一种将多个弱学习器组合为强学习器的方法，这种算法的思想是：首先训练一个基学习器，然后根据基学习器的表现对样本的分布进行调整，使得先前学习错的样本在后面占更大的比重，然后基于调整后的样本分布来进行下一个基学习器的学习，最后将所有的学习器加权结合得到一个强学习器。 Adaboost介绍Adaboost算法是boosting算法中比较有代表性的，它是一个**前向分步加法模型。 思想1.在Adaboost中，对样本分布进行调整是调整样本分布的权重，对于分错的样本提高其权重，对于分对的样本降低其权重。2.对于每个弱分类器的权值，根据其的分类误差率**来分配，对于分类误差率比较小的，给予一个高权值让它起更大的作用，反之给一个低权值。 算法基学习器以二分类模型为例： 初始化训练集样本的权值,最开始权值相等，和为1：$$D1 = (w_{11},w_{1i},….,w_{1n}),w_{1i}=\frac{1}{N}$$ 对$m=1,2,3,4….(第m轮)$，使用带有权值分布的训练数据集$D_m$得到二分类器$G_m(x)$,其中$G_m(x)$的生成策略是找到一个决策函数使得当前分类误差率最小，所以其实得到$G_m(x)$的同时就已经得到了分类误差率。$$min(e_m)$$ 使用$G_m(x)$在训练集上计算分类误差率$$e_m = P(G(x_i)≠y_i) = \sum_{i=1}^Nw_{mi}I(G(x_i)≠y_i)$$就是说如果预测错了的话，则该样本的分类分类误差率为: $w_{mi}$x1,所以数据集整体的分类误差率其实就是分类错误的样本对应的样本权值之和(函数$I$中分类正确的样本为$I(x_i)=0$) 计算当前弱学习器的的系数(对数是自然对数),由下式可以看出，当分类误差率$e_m$&gt;$\frac{1}{2}$的时候，其分类器对应的系数为$0$(说明效果太差了)$$\alpha_m = \frac{1}{2}log\frac{1-e_m}{e_m}$$ 根据计算得到的$\alpha_m$和当前的弱分类器，计算下一轮的数据集样本权重,其中第$m$轮的第$i$个样本的权重为：$$w_{mi}=\frac{w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i))$$其中，$Z_m$是规范化因子，它是D_{m+1}称为一个概率分布,其实就是上式分子$i$从$1$到$N$的一个累加和。$$Z_m=\sum_{i=1}^Nw_{mi}exp(-\alpha_my_iG_m(x_i))$$ 循环算法步骤$(3)-(5)$之后，可以得到m个弱分类器的加权线性组合函数(一共$m$轮):$$f(x)=\sum_{m=1}^M\alpha_mG_m(x)$$即最后得到的强分类器:$$G(x)=sign(f(x))$$ 算法关键点 注意算法每一轮计算的步骤，首先是学习这一轮的基本分类器，然后是计算分类误差率，然后是计算当前分类器的最终系数$\alpha_m$,最终计算下一轮的样本权重$w_{m+1,i}$,更新样本权重可以写为:$$\begin{cases}\frac{w_{mi}}{Z_{m}}e^{-\alpha_m} \ , \ G_m(x_i)=y_i\\\frac{w_{mi}}{Z_{m}}e^{\alpha_m} \ , \ G_m(x_i) ≠y_i\end{cases}$$ 从分类误差率的计算可以看出，当前分类误差率依赖于上一轮的样本权重，由于上一轮对于分错的样本给予了比较大权重，如果这一轮分类继续出错，那么这一轮的分类误差率肯定也会比较大，为了降低分类误差率，模型会朝着使得这些权重大的样本分类正确的方向拟合，从而达到了我们”提高分类错误的样本，使得其被后面的分类器更加关注”的目的。 前向分步模型那既然说Adaboost是一个前向分步加法模型，那什么是前向分步模型呢，为什么叫这个名字？在加法模型中:$$G(x)=\sum_{m=1}^M\alpha_mG_m(x)$$在给定数据集D和损失函数L的情况下，学习策略变成了使损失函数最小的最优化问题:$$Min\sum_{i=1}^NLoss(y_i,G(x_i))$$如果把这个损失函数当成一个整体来求最优化是个很复杂的问题，为了简化这种问题，将其分解为从前往后，每一步只学习一个分类器和其系数的问题。所以，每一次只需要优化如下的损失函数:$$Min(Loss(y_i,\alpha_mG_m(x)))$$而前向分布的算法步骤正如Adaboost一样，这里不再写一遍了。 前向分步与Adaboost关系AdaBoost算法是前向分步加法算法的特例，这时，模型是由基本分类器组成的加法模型，损失函数是指数函数。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈SVM与感知机]]></title>
    <url>%2F2018%2F03%2F16%2Fsvm%2F</url>
    <content type="text"><![CDATA[感知机定义感知机是SVM和神经网络的基础，所以在介绍SVM之前，先谈谈感知机。 感知机是一个二分类的线性分类模型。其表示从输入空间到输出空间的如下函数，输出为-1,1:$$f(x)=sign(w·x+b)$$ 其中，w是权值向量，b为偏置bias,sign是符号函数 几何意义其几何意义为:$$w·x+b=0$$对应于特征空间中的一个分离超平面，其中w表示超平面的法向量，而b对应超平面的截距。 感知机-线性可分线性可分的意思是存在一个超平面可以将正负样本点完全分开。 在线性可分的情况下，感知机的学习策略是：“使得误分类点到超平面的总距离最小” 所有误分类点到超平面S的总距离为:$$-\frac{1}{||w||}\sum_{x_i∈M}y_i(w·x_i+b)$$所以可以得到其损失函数，它是关于w,b的连续可导函数:$$L(w,b)=-\sum_{x_i∈M}y_i(w·x_i+b)$$ 感知机学习算法感知机的学习算法即上式(损失函数)的最优化问题，其方法为随机梯度下降法。$$minL(w,b)=-\sum_{x_i∈M}y_i(w·x_i+b)$$随机梯度下降一次只选取一个误分类点使其梯度下降,其梯度计算为:$$L_w(w,b)=-\sum_{x_i∈M}y_ix_i$$$$L_b(w,b)=-\sum_{x_i∈M}y_i$$ 所以下面给出感知机学习算法步骤(原始形式):给定训练集和学习率$\eta$(1) 选取初始值$w_0,b_0$(2) 随机选取一个误分类点$y_i(w·w_i+b)≤0$更新w,b:$$w←w+\eta y_ix_i$$$$b←b+\eta y_i$$(3)重复(2)直到没有误分类点 感知机学习算法几何解释当一个样本点被误分到分离超平面的另一端时，调整w,b的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，如此循环直到该误分类点被正确分类。由于其结束的条件是所有样本点被正确分类，所以，在线性可分数据集中，感知机有无数个解。 线性支持向量机SVM(support vector machines,SVM)是一种二分类模型。其核心是核技巧，可以用来解决非线性问题，当使用非线性核的时候，SVM是非线性模型，其相当于把隐式地在高维的特征空间中学习线性支持向量机。 SVM与感知机的区别1.学习策略不同，感知机的学习策略为使得误分类点的距离最小，而SVM的学习策略是间隔最大化，这样使得在同样解决线性可分的问题时，感知机可以有无数个解，而svm只有一个最优解。 感知机拥有核技巧，可以解决非线性问题间隔最大化我们知道SVM和感知机的最大区别之一就是SVM采用的是间隔最大化的策略，使得其存在唯一最优解下面介绍函数间隔和几何间隔：(1) 函数间隔$|w·x+b|$表示一个点离超平面的远近，而加上$y_i$可以判断其是否被正确分类，所以函数间隔就是组合两者：表示一个样本点被分类的正确性和确信度(离得越远越确定):$$\gamma_i = y_i(w·x_i+b)$$整个函数集的函数间隔就是所有样本点函数间隔最小值。(2) 几何间隔使用函数间隔会有缺点：成倍缩放w和b，超平面位置不变，但是函数间隔为会改变。所以引入了几何间隔的概念(做了一个L2归一化)使得超平面确定了，其间隔也确定，几何间隔为:$$\gamma = -(\frac{w}{||w||}·x_i+\frac{b}{||w||})$$]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[广义线性模型]]></title>
    <url>%2F2018%2F03%2F16%2F%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[什么是广义线性模型？ 广义线性模型是线性模型的扩展，其特点是不强行改变数据的自然度量，数据可以具有非线性和非恒定方差结构，主要是通过联结函数g()(link function)，建立响应变量Y的数学期望值与线性组合的预测变量P之间的关系。 指数分布族指数分布族具有以下形式:$$p(y;\eta)=b(y)exp(\eta^TT(y)-a(\eta))$$其中的参数意义为: $\eta$ 是自然参数 $T(y)$是充分统计量(一般$T(y)=y$) $a(\eta)$是log partition function($e^{-a(\eta)}$充当正规化常量的角色，保证$\sum_{p(y;\eta)}=1$)所以，T,a,b确定了一种分布，而$\eta$是该分布的参数 符合指数族分布的模型我们都可以用把它当成广义线性模型来求解: 广义线性模型的三个假设GLM(Generalized Linear Models)的三个假设是推导普通模型的基础:1.$y | x ; \theta ~ ExponentialFamily(\eta)$固定参数$\theta$,在给定x的情况下，$y$服从指数分布族中以$\eta$为参数的某个分布2.给定一个x，我们需要的目标函数为$h_\theta(x)=E[T(y)|x;\theta]$,后者为该分布的期望3.令$\eta=\theta^Tx$ GLM和线性回归与LR的联系GLM和线性回归其中，线性回归是在选择合适的T,a,b使其为高斯分布的时候推导出来的:看起是否满足三点假设:假设1:能否将高斯分布写成指数分布族的形式 $p(y;\mu)=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y-\mu)^2}{2\sigma^2})$$=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{1}{2}y^2)\cdot \exp(\mu y-\frac{1}{2}\mu ^2)$ 假设2：假设函数是否等于高斯分布的期望 $h_\theta(x)=E[T(y)\mid x;\theta]$$=E\left[y\mid x;\theta\right]$$=\mu$假设3：自然参数$\eta$和x是否是线性关系$$h_\theta(x)=\mu=\eta=\theta^Tx$$满足三个以上假设，而假设三满足的线性关系正是线性回归。广义线性模型与LR同理，当把伯努利分布放在广义线性模型中推导的时候，可以得到逻辑斯蒂回归的$h_\theta(x)$$h_\theta(x)=E\left[T(y)\mid x;\theta\right]$$=E\left[y\mid x;\theta\right]$$=\phi$$=\frac{1}{1+e^{-\eta}}$$=\frac{1}{1+e^{-\theta^Tx}}$ 结论除了高斯分布与伯努利分布，大多数的概率分布都能表示成指数分布族的形式，如多项式分布（Multinomial），对有K个离散结果的事件建模；泊松分布（Poisson），对计数过程进行建模，如网站访问量的计数问题；指数分布（Exponential），对有间隔的证书进行建模，如预测公交车的到站时间的问题；等等，然后通过进一步的推导，就能得到各自的线性模型，这大大扩展了线性模型可解决问题的范围。 参考链接1.晓雷2.NJiaHe]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑斯蒂回归]]></title>
    <url>%2F2018%2F03%2F14%2F%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[是什么？逻辑斯蒂回归，又称对数纪律回归，是在线性回归的基础上，使用sigmoid函数将线性模型$w^TX$的输出压缩到0-1之间，使其具有预测概率的特性。是一种广义上的线性模型。sigmoid函数:$$\sigma(a)=\frac{1}{1+e^{-a}}$$解决了阶跃函数不是单调可微(因为它不连续)的问题，使得输出能够映射为0-1的连续区间。 逻辑斯蒂分布？逻辑斯蒂分布的概率密度函数和分布函数如下：$$F(x)=P(X\le x)= {1 \over 1+e^{-(x-\mu )/\gamma}}$$$$f(x)=F’(x)={e^{-(x-\mu)/y} \over \gamma(1+e^{-(x-\mu)}/\gamma)^2}$$其中,$μ$为位置参数,$γ$为形状参数，其分别对应的图像如下： 逻辑斯蒂分布与逻辑斯蒂回归的关系？逻辑斯蒂回归中的sigmoid函数就是逻辑斯蒂分布在$μ=0;$$γ=1$时的情况 对数几率回归？首先由之前给出的sigmoid函数:$$\sigma(a)=\frac{1}{1+e^{-a}}$$,代入一个广义线性模型$W^TX+b$,得到:$$y = \frac{1}{1+e^{-(\omega^Tx+b)}}$$通过等式变换得到:$$\ln(\frac{y}{1-y}) = \omega^Tx+b$$等式左边：一个事件的几率是该事件发生的概率与一个事件不发生的概率的比值，也就是对数几率。等式右边是线性模型。所以对数几率回归实际上就是用右边的线性模型去逼近这个对数几率。 参数估计？既然可以写成对数几率回归的形式，那么怎样来估计等式右边线性模型的$w$和$b$呢？极大似然估计引用知乎上对极大似然的一种比较直观的解释 作者：稻花香 现在已经拿到了很多个样本（你的数据集中所有因变量），这些样本值已经实现，最大似然估计就是去找到那个（组）参数估计值，使得前面已经实现的样本值发生概率最大。因为你手头上的样本已经实现了，其发生概率最大才符合逻辑。这时是求样本所有观测的联合概率最大化，是个连乘积，只要取对数，就变成了线性加总。此时通过对参数求导数，并令一阶导数为零，就可以通过解方程（组），得到最大似然估计值。 用极大似然估计来求参数的算法步骤为：1.写出似然函数，在这里，其似然函数为：$$\Pi^N_{i=1}[\pi(x_i)^{y_i}][1-\pi(x)]^{1-y_i},\pi(x)为x为1的概率$$2.对似然函数取对数并化简: $L(w)=\sum^N_{i=1}[y_i\log \pi(x_i)+(1-y_i)\log (1-\pi(x_i))]$$=\sum^N_{i=1}[y_i\log{\pi(x) \over 1-\pi(x_i)}+\log(1-\pi(x_i))]$$= \sum^N_{i=1}[y_i(wx)-\log(1+exp(wx))]$ 3.求导数:(下式$y_n$对应上式$y_i$，$t_n$对应$x_i$) $$\frac{\partial lnP(t|w)}{\partial w}=\sum_{n=1}^N{t_nlny_n+(1-t_n)ln(1-y_n)}$$ 求导过程： $\frac{\partial lnP(t|w)}{\partial w} =\sum_{n=1}^N{t_n\frac{1}{y_n}\partial y_n-(1-t_n)\frac{1}{1-y_n}\partial y_n}$$=\sum_{n=1}^N{(t_n\frac{1}{y_n}-(1-t_n)\frac{1}{1-y_n})y_n(1-y_n)\partial(-w^T\phi_n)}$$=\sum_{n=1}^N{(y_n-t_n)\phi_n}$ 4.估计参数梯度下降法，拟牛顿法等。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对生成模型与判别模型的理解]]></title>
    <url>%2F2018%2F03%2F11%2F%E7%94%9F%E6%88%90%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[[该博客为本网站作者: “yangyiqing”原创，转载请注明出处] 书中介绍在《统计学习方法》中，对生成模型和判别模型的介绍篇幅比较少： 1.生成方法监督学习方法可以分为生成方法和判别方法，生成的模型分别对应为生成模型和判别模型生成方法由数据联合概率分布$P(X,Y)$,然后求出条件概率分布$P(Y|X)$作为预测的模型，即生成模型：$$P(Y|X)=\frac{P(X,Y)}{P(X)}$$称之为生成方法的原因是：模型表示了给定输入X产生输Y的生成关系。典型的生成方法有： 朴素贝叶斯 隐马尔科夫模型 2.判别方法 判别方法由数据直接学习决策函数$f(x)$或者条件概率分布$P(Y|X)$作为预测的模型，即判别模型。判别模型关心的是对于给定的输入X，应该预测什么样的Y。典型的判别模型有： k近邻(knn) 感知机 决策树 逻辑斯蒂回归 最大熵模型 支持向量机 提升方法 条件随机场 生成方法和判别方法的区别： 1.生成方法可以还原出联合概率分布，而判别方法不可以 2.通常情况下，生成方法的学习收敛速度快，而判别方法的准确率更高 3.当存在隐变量的时候，仍然可以用生成方法，但是不能用判别方法 4.由于直接学习$P(Y|X)$或$f(X)$，可以对数据进行各种程度上的抽象，定义特征并使用特征，因此可以简化学习问题。 个人补充决策函数和条件概率函数先说说决策函数$f(x)$和条件概率分布$P(Y|X)$:举个例子来说，有一个分类问题，要求判断给定特征下判断是篮球，足球，还是乒乓球。1.在决策函数中：对于每一个特征有一个阈值(分类边界)，对于输入的特征每个去判断属于边界的哪一边，最后直接得到一个对应的输出$Y$就是对应的分类结果。2.在条件概率分布中：会去计算在对应特征条件下是每个类的概率，也就是依次计算：$$P(篮球|X),P(足球|X),P(乒乓球|X)$$然后从中选择输出概率最大的那个最为对应的分类结果。再说的详细一点儿，每个条件概率分布式是怎么求出来的呢：$$P(Y|X)=\frac{P(X,Y)}{P(X)}=\frac{P(X|Y)P(Y)}{P(X)}$$从以上的例子应该可以看出决策函数和条件概率分布的不同，一个是给定X直接产生一个对应输出Y，而另一个是首先计算其可能为每个Y的概率，然后再输出一个概率最大的Y。 那决策函数$f(x)$和条件概率分布$P(Y|X)$有什么联系呢？ 实际上通过条件概率分布P(Y|X)进行预测也是隐含着表达成决策函数Y=f(X)的形式的。例如也是两类w1和w2，那么我们求得了P(w1|X)和P(w2|X)，那么实际上判别函数就可以表示为Y= P(w1|X)/P(w2|X)，如果Y大于1或者某个阈值，那么X就属于类w1，如果小于阈值就属于类w2。而同样，很神奇的一件事是，实际上决策函数Y=f(X)也是隐含着使用P(Y|X)的。因为一般决策函数Y=f(X)是通过学习算法使你的预测和训练数据之间的误差平方最小化，而贝叶斯告诉我们，虽然它没有显式的运用贝叶斯或者以某种形式计算概率，但它实际上也是在隐含的输出极大似然假设（MAP假设）。也就是说学习器的任务是在所有假设模型有相等的先验概率条件下，输出极大似然假设。 简化学习问题如何理解判别模型可以简化学习问题？ 分类器的设计就是在给定训练数据的基础上估计其概率模型P(Y|X)。如果可以估计出来，那么就可以分类了。但是一般来说，概率模型是比较难估计的。给一堆数给你，特别是数不多的时候，你一般很难找到这些数满足什么规律吧。那能否不依赖概率模型直接设计分类器呢？事实上，分类器就是一个决策函数（或决策面），如果能够从要解决的问题和训练样本出发直接求出判别函数，就不用估计概率模型了，这就是决策函数Y=f(X)的伟大使命了。例如支持向量机，我已经知道它的决策函数（分类面）是线性的了，也就是可以表示成Y=f(X)=WX+b的形式，那么我们通过训练样本来学习得到W和b的值就可以得到Y=f(X)了。还有一种更直接的分类方法，它不用事先设计分类器，而是只确定分类原则，根据已知样本（训练样本）直接对未知样本进行分类。包括近邻法，它不会在进行具体的预测之前求出概率模型P(Y|X)或者决策函数Y=f(X)，而是在真正预测的时候，将X与训练数据的各类的Xi比较，和哪些比较相似，就判断它X也属于Xi对应的类。 所以判别模型的可以简化学习问题的优势就在于，我不一定硬要去找数据中的规律或计算条件概率，我只要找到一种判别规则就可以了，而且由于不需要计算类别的条件概率，可以对数据进行降维等抽象操作。 其它联系生成模型和判别模型还有什么联系？由生成模型可以得到判别模型，但由判别模型得不到生成模型。生成模型可以反应数据本身的特性，而判别模型不可以。来源于同一个原因： 生成模型可以求出数据的联合概率分布，而判别模型不可以。 区别和优缺点再通俗的解释一下两者的区别？ 生成算法尝试去找到底这个数据是怎么生成的（产生的），然后再对一个信号进行分类。基于你的生成假设，那么那个类别最有可能产生这个信号，这个信号就属于那个类别。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。 生成模型的优点和缺点？优点：1.可以还原联合概率分布$P(X,Y)$，得到数据集更多的特性。2.生成模型收敛速度比较快，即当样本数量较多时，生成模型能更快地收敛于真实模型。3.生成模型能够应付存在隐变量的情况4.研究单类问题比判别模型灵活性强缺点：1.准确率一般没有判别模型高 判别模型的优点和缺点？优点：1.需要的样本数量少，计算量小2.直接面对预测，准确率高3.直接学习$P(Y|X)$，不需要求解条件概率，所以允许我们对输入进行抽象（比如降维、构造等），从而能够简化学习问题。缺点：1.无法反映数据集的特性，无法求解联合概率分布 过拟合？过拟合问题直接摘自大鼻子的博客 生成模型‘‘没有考虑正则化很简单，因为他们很少过拟合’’。生成模型学习$X,Y$的联合概率分布$P(X,Y)$，直接学习的就是数据的分布，从整个数据的整体着手，很少会出现过拟合。基本上属于高偏差/低方差分类器，当样本数量小于特征数量或样本数量不足时，应选用这种模型 判别模型判别模型应当有正则化过程，因为是直接生成$f(x)$或者$p(y|x)$，所以很容易比较$y$跟$f(x)$的关系，按照现有数据照葫芦画瓢来判别，容易过拟合，所以正则化便有存在的意义。基本上属于低偏差/高方差分类器，容易过拟合，需要正则项。数据量充足时选用判别模型 结论随着训练集的增大，低偏差/高方差分类器（判别模型）相对于高偏差/低方差分类器（生成模型）准确率高，因为随着数据量的增大，现有训练集数据的分布更接近于真实分布，此时生成模型优势变小，同时生成模型不能提高足够的准确率，此时，判别模型优势更大。 举个例子：当一份分类数据的特征维度大于样本数量时。如果采用判别模型，极端情况下每条样本都有唯一的特征（或特征组合），此时如果正则化不够给力，那么该判别模型将极大限度拟合当前数据，训练集AUC可能将近1，那么就可能得到训练数据上准确率100%，测试数据准确率不如XJB猜的模型。 感觉知乎这个答案也比较赞一张图总结生成模型与判别模型 仍有以下问题没有完全搞懂： 关于隐变量的问题贴一张知乎的图作为初步理解。2.”而贝叶斯告诉我们，虽然它没有显式的运用贝叶斯或者以某种形式计算概率，但它实际上也是在隐含的输出极大似然假设（MAP假设）” 参考链接:1.CSDN2.知乎13.知乎2]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树原理]]></title>
    <url>%2F2018%2F03%2F09%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[[该博客为本网站作者: yangyiqing 原创，转载请注明出处] 决策树概念1.决策树学习通常分为三个步骤： 特征选择 决策树生成 剪枝 2.可以将决策树看成一个if-else规则的集合，其特点是所有的路径互斥且完备，给出一个测试用例，有且只有一条路径满足条件。3.决策树还表示给定特征条件下的条件概率分布，这一条件概率分布定义特征空间的一个划分上，决策树的一条路径对应于划分中的一个单元，决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。4.决策树的学习的损失函数通常是正则化的极大似然函数，决策树学习的策略是以损失函数为目标函数的最小化5.“从所有可能的决策树种选择最优决策树是NP完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题，这样得到的决策数是‘次最优’的” – 《统计学习方法》我的理解是由于最优决策树是不可推导的(只能列举全部可能情况)，所以只能通过局部最优解来递归的解决，在特征选择的过程中每次选择信息增益最大的特征来作为当前划分区域的特征,也就是递归的解局部最优 特征选择特征选择是选择对数据具有良好区分度的特征，通常对决策树来说衡量特征对label区分度的能力用信息增益或信息增益比来表示。以书上的数据表作为例子：为了从中找出更有区分度的特征，这里引入信息增益的概念: 熵(entropy)是表示随机变量不确定性的度量，对熵的定义:在信息论与概率统计中，熵是表示随机变量不确定性的度量。设X是一个取有限个值得离散随机变量，其概率分布为$$P(X = x_i)=p_i,i=1,2,3..n$$则随机变量X的熵定义为$$H(x)=-\sum_{i=1}^np_ilog_2p_i$$熵越大，随机变量的不确定性就越大，举例当随机变量只有两个取值的时候:$$H(p)=-plog_2p-(1-p)log_2(1-p)$$这个时候，熵随着概率p的变化曲线如下图所示，当概率为0.5的时候，也就是取两个值得概率相等时，熵的值最大。这也很容易理解，因为当两个值取值相等时，不确定性最大，而熵又是衡量随机变量的不确定性的，所以此时熵最大。 条件熵随机变量X给定条件下随机变量Y的条件熵H(Y|X)，定义为X为给定条件下的Y的条件概率分布的熵对X的数学期望:$$H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)$$ 信息增益表示得到特征X的信息以后使得类Y的信息不确定性减少的程度。比如有个本来的经验熵为H(D),这个时候又加入了一个特征A，那么加入特征A以后的经验熵就是H(D|A),则信息增益：$g(D,A) = H(D) - H(D|A)$假设本来的经验熵为1，加入特征A以后的经验熵变为0.8,那么信息熵就减少了0.2,说明类别的不确定性减少了，那么对于分类来说就更准确了一般来说，熵与条件熵之差称为互信息，决策树学习中的信息增益等价于训练数据集中类与特征的互信息 信息增益算法(1) 计算数据集D的经验熵H(D)其中k是指一共多少个类别，Ck是该类别在数据集D中出现的次数 $$H(D)=-\sum_{k=1}^K\frac{|C_k|}{D}log_2\frac{|C_k|}{|D|}$$ (2) 计算特征A对数据集D的条件经验熵H(D|A)其中n是指特征A中的类别，k是指label的类别 $$H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_ik|}{|D_i|}log_2\frac{|D_ik|}{|D_i|}$$ (3) 计算信息增益 根据以上公式，可以计算上面例子数据集中每个特征的信息增益：$$-\frac{2}{5}log\frac{2}{5}-\frac{3}{5}log\frac{3}{5}=0.971$$然后计算每个特征的信息增益:以年龄特征为例：$$g(D,A_1)=H(D)-[\frac{5}{15}H(D_1)+\frac{5}{15}H(D_2)+\frac{5}{15}H(D_2)]$$其中以$H(D_1)$为例：$$H(D_1)=-\frac{2}{5}log\frac{2}{5}-\frac{3}{5}log\frac{3}{5}$$其余同理，这样最终算出的信息增益:$g(D,A1) = 0.083$$g(D,A2) = 0.324$$g(D,A3) = 0.420$$g(D,A4) = 0.363$这样，特征A3的信息增益最大，所以选择A3作为最优特征。 信息增益比引入信息增益比的原因只依赖信息增益来选择特征可能会出现问题，会存在偏向于选择取值较多的特征，就是哪个特征中的类别越多，可能其信息增益就会越大，原因是分类越多，那么在子数据集中lable全部相同的概率就越大（一种极端情况），在这种情况下，该子数据集的熵为0，如果这种子数据集多的话，那么其条件熵$H(D|A)$就会越小,那么$g(D,A) = H(D) - H(D|A)$就会越大。 概念$$信息增益比 = \frac{信息增益}{特征A的熵}$$一目了然，引入了特征A的熵最为分母，特征A的取值(类别数)越多，那么其熵值越大，这样就防止了因为取值多而造成的偏向问题 决策树生成ID3算法核心思想：从跟节点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，然后在其子结点上递归使用此方法，直到达不到信息增益阈值或没有特征可以选择为止。 使用ID3算法对例子进行二叉树的构建首先根据上面的计算结果选取信息增益最大的特征(有没有房子)，然后根据有无房子分为左右子节点，发现左子数据集所有的lable都是是，所以左子树结束。继续对右节点进行划分，剩下的特征有“年龄”，“工作”，“信贷情况”，这个时候需要重新计算$D_2$(没有房子数据集)的信息熵，以及各个特征的信息增益：$g(D_2,A_1)=H(D_2)-H(D_2|A_1)=0.251$$g(D_2,A_2)=0.918$$g(D_2,A_4)=0.474$可以发现$A_2$的信息增益最大，选择是否有工作作为当前最优特征对数据集进行划分，后面依次递归。这样就仅基于信息增益生成了一颗决策树，但是仅仅这样还不够，容易产生过拟合 C4.5算法与ID3算法类似，不同的地方是在特征选择的时候利用信息增益比来选择特征，这样可以在防止ID3算法的过拟合方面上有了改进。 查看ID3算法的决策树生成代码点击这里 决策树的剪枝按照ID3或C4.5算法生成的决策树，可能对训练数据集有比较好的准确度，但是对未知数据的预测能力却不一定比较好，因为在训练过程中为了拟合训练数据而构造了很复杂的决策树的话，就会产生过拟合。解决这一问题的方法就是对决策树进行剪枝。剪枝的目的是为了同时兼顾模型对训练集的拟合程度以及减少模型的复杂度来提高预测能力，那就不能仅仅使用信息增益来判别，还要加入带有模型复杂度的项对过于复杂的模型进行惩罚。在这里定义决策树的损失函数(代价函数)：设数的叶节点个数为$|T|$,$t$是$T$的叶节点，该叶节点有$N_t个$样本，其中$k$类的样本有$N_{tk}$个，$k=1,2,3,…K$,$H_t(T)$为叶节点上$t$的经验熵，$α&gt;=0$为参数,则决策树学习的损失函数可以定义为：$$C_α(T) = \sum_{t=1}^{|T|}N_tH_t(T)+α|T| $$其中经验熵为：$$H_t(T) = -\sum_k \frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}$$ 解释损失函数计算的是每个叶节点的样本数和每个叶节点的经验熵的乘积的累加和最终加上以叶节点个数为基础的惩罚项，经验熵就是之前介绍计算数据集D的经验熵的公式。为什么每个叶节点可以计算经验熵呢，叶节点不是都属于同一类吗？其实每个叶节点并不一定都属于同一个类，因为在树的生成过程中，生成叶子节点的条件并不仅仅是划分到子数据集只剩下一个类，如果小于某个信息增益(ID3)或信息增益比(C4.5)的话，也会直接生成叶节点，这个时候按照子数据集中占比最大的类作为叶子节点的类别 对于$C_α(T)$来说，左半边表示的训练数据的误差，也就是训练数据的拟合程度，而$|T|$表示模型复杂度，$α$来控制两者之间的关系，相当于一个惩罚系数。当$α$确定以后，剪枝的策略就是使得损失函数$C_α(T)$最小化。可以看出，决策树生成学习局部的模型，而决策树剪枝学习整体的模型。 决策树的损失函数最小化相当于正则化的极大似然估计决策树的剪枝算法：输入：生成算法产生的整个树T，参数α：输出：修建后的子树Tα（1） 计算每个结点的经验熵（2） 递归地从树的叶结点向上回缩设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_B$与$T_A$，其对应的损失函数值分别为$C_α(T_B)$与$C_α(T_A)$，如果$$C_{\alpha}(T_A)\le C_{\alpha}(T_B)$$则进行剪枝，即将父结点变为新的叶结点。(3)返回(2)，直至不能继续为止，得到损失函数最小的子树$T_α$ CART 算法CART(classification and regression tree)是指分类与回归树，同样特征选择，树的生成，树的剪枝构成，既可以用于分类也可以用于回归，CART是二叉树。得到D以后，CART算法递归的二分每个特征，将数据集划分为有限个单元，给定一个测试用例，则其预测依据为给定条件下的条件概率分布。(1) 决策树的生成CART算法在树的生成过程中递归的二分特征，要求构建的树要尽量大，关于特征选择的评估指标，在ID3算法中是信息增益，在C4.5算法中是信息增益比，而在CART算法中，特征选择的评估标准是：$$\begin{cases}分类树：基尼系数\回归树：平方误差\\end{cases}$$依据最小化这两个评估标准的原则进行特征选择，在选择完特征以后把特征划分为两部分。那么怎么样根据这个评估标准来选择最优特征呢，在ID3和C4.5算法中，可以根据每个特征计算其信息增益和信息增益比，平方误差和基尼系数的在CART的生成树算法中是这样运用的：1.1 最小二乘回归树算法选择最优切分特征j和最优切分点，根据以下公式：$$min_{j,s}[min_{c1}\sum_{x_i∈R_1(j,s)}(y_i-c_1)^2+min_{c2}\sum_{x_i∈R_2(j,s)}(y_i-c_2)^2]$$（$c_1,c_2$是切分点左右部分的y的均值）其含义是遍历每个特征j，在j确定的条件下，遍历切分点s，这样就找到了$minL(j,s)$,比较所有的局部最优平方误差，找到最优的特征及其切分点作为当前的特征和划分依据。然后对当前的数据集进行划分：$$R_1(j,s)={x|x^{(j)}≤s},R_2(j,s)={x|x^{(j)}≥s}$$然后每个单元的输出值为当前单元的子数据集的label的平均值$$C_m = \frac{1}{N_m}\sum_{x_i∈R_m(j,s)}y_i,x∈R_m,m=1,2$$重复上述步骤直到达到停止条件，最后划分树的叶节点个数为M，在最终的回归树模型为：$$f(x)=\sum_{m=1}^Mc_mI(x∈R_m)$$解释：这里的累加并不是真正意义上的累加，因为后面跟了一个Indicator函数，当输入落在哪个小区域(叶节点)上的时候，其对应的值为改区域的子数据集的label均值$c_m$。 1.2 分类树的生成首先引入基尼指数的概念：假设有K个类，样本点属于第k类的概率为$p_k$,则该数据集的基尼指数为：$$Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^K(\frac{|D_k|}{|D|})^2$$即：1-(各类别占比平方和累加) 基尼指数的意义：基尼指数表示数据集的不确定性，基尼指数越大，数据集的不确定性越大，和熵类似。下图是在二分类中基尼指数和熵，分类误差率的关系： CART分类树生成算法与回归树类似，也是递归的进行特征选择，然后分左右节点，因为都是二叉树嘛。下面着重讲讲不同的地方。第一个点当然是特征选择的依据不同，回归树在进行特征选择的时候是嵌套循环遍历每一个特征和每一个特征中可能的切分点。对于分类算法来说同样适用，不同点在于选取切分点的判断依据不同。在分类树算法中，遍历到的每一个特征中的每一个值，都按照该值将数据集分为取该值$D_1$和不取该值$D_2$,然后根据$D_1$和$D_2$计算在特征A的条件下，集合D的基尼指数：$$Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$$循环遍历每个特征的每个切分点以后，就可以找到最优的特征和最优的切分点(使得上式最小)，递归的进行上述过程直到满足停止条件(没有特征，子数据集特征的取值只有一个，没有特征和切分点满足GIni阈值),最终生成的CART分类树：$$f(x)=\sum_{m=1}^MV_mI(x∈R_m)$$其中$V_m$是投票函数。 CART剪枝算法之前在说CART数生成的时候，谈到要生成的树尽量完全，这样剪枝的时候就有比较大空间。CART剪枝主要分为两步： 从数的底端开始不断的向上剪枝，知道树的根节点，这期间生成了n个子树，{$T_0,T_1,T_2…$} 通过交叉验证在独立的验证集上验证每颗子树的性能，选择最优子树 那自下而上剪枝的时候，有这么多的节点，怎么确定先剪哪一个呢，或者说，怎么剪才是最优的呢(使得损失函数更小)。因此，确定减掉一个结点t以后，整体的损失函数变为了多少，或者减少了多少是衡量是否应该减掉某个结点的标准。当前的树模型是在原来的数据集$D_{train}$上构建而成的，它在原来的数据集上的损失函数已经足够小了，但是现在数据集更换为了$D_{test}$，这个时候仍能计算出当前树模型的损失函数，然而它已经不一定是小的损失函数了(预测能力不确定，如果过拟合，则预测能力很差)，所以需要用剪枝的手段来减小在测试集上的损失函数(或者说提高预测能力)。 CART剪枝算法步骤： 1.设$k=0,T=T_k,(k表示第几颗子树)$ 2.$α=+∞$ 3.自下而上的对每个内部结点$t$计算 $$C(T_t):子树的损失函数，|T_t|:子树的叶节点个数$$ $$g(t)=\frac{C(t)-C(T_t)}{|T_t|-1},g(t)表示损失函数较少的程度,对于单结点数,C_α(t)=C(t)+α,$$ $$α=min(α,g(t))$$ 4.对$g(t)=α$的结点进行剪枝(即在计算过程中得到的$g(t)&lt;原α$),并对结点t以多数表决决定其类别， 更新:$k=k+1,α_k=α,T_k=T$ 5.重复以上步骤直到成为一个只有两个叶节点的树，在形成的子树序列{${T_0,T_1,T_2…,T_n}$}用验证集选取最优子树 对公式$g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}$的解释：首先需要明确的一点是剪枝的策略，在减掉某个结点的时候，我们希望它尽量不对模型的损失函数造成坏的影响，也就是它单节点的损失函数最好是小于等于以它为根节点的子树的损失函数，这样我们就可以轻松的把它的子树剪掉，既减小了模型复杂度，又没影响损失函数是最好的，其中，单结点数的损失函数为：$$C_α(t)=C(t)+α,because |T_t|为1$$以它为根节点的子树的损失函数为:$$C_α(T_t)=C(T_t)+α|T_t|$$我们希望是：$$C_α(t)≤C_α(T_t)$$即:$$C(t)+α≤C(T_t)+α|T_t|$$两边稍微变换一下：$$\frac{C(t)-C(T_t)}{|T_t|-1}≤α$$这是我自己的理解的，和书上稍微有一点点出入，这样，在某种一般情况下，我们可以得到一个等号，这个时候该结点的单节点树损失函数和其子树损失函数相等，那么肯定是直接剪成单节点树，那如果左边更小的话就更好了，所以就要对于每一个当前树$T_k$的结点计算一下 $$\frac{C(t)-C(T_t)}{|T_t|-1}$$ 然后保存其值，如果它的值比α还小，那么就更新全局变量的$min_α$值，然后最后找到$g(t)=min_α$的那个结点(你也可以辅助理解为保存键值对，最后找到值最小的那个键，虽然这和它的操作有点偏差)，把它剪成一个子结点，这样就完成了一轮剪枝，得到一颗子树$T_{k+1}$,然后将此时的α作为下颗子树的起始值。 如果有后续延伸或更深入的问题再补充，还有下面的问题没有完全理解：1.如何理解决策树剪枝过程中损失函数最小化等价于正则化的极大似然估计]]></content>
      <categories>
        <category>理论</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017 CCF 大数据竞赛top4%]]></title>
    <url>%2F2018%2F02%2F08%2F2017CCF%2F</url>
    <content type="text"><![CDATA[2017 CCF 大数据竞赛思路及源码分享源码见我的Github 比赛：蚂蚁金服：精准室内定位**，线下赛最终经过作弊筛选后是前100，共有2845支队伍，因为当时和小伙伴都不太会MapReduce，加上还有两个周期末考试了然而我为了比赛完全没有复习，所以复赛就弃了。 1.题目给出用户在商场使用手机支付时所采集到的信息，包括用户信息，店铺信息，商场信息等，要求预测给出上述信息后精准预测用户所在店铺。具体给出的数据表可以点击这里来看。 2.大致分析与思路1.总体来看比赛是一个多分类问题，但是直接处理的话会有上千个类别，所以仔细分析一下赛题数据，其实可以不同商场分开来进行预测。2.虽然给出了比较多的信息，包括很多的用户信息和商店类别之类的看似有用的信息，但是做过这个比赛的都知道，其实是一个Wifi定位的问题，当然其他的信息经过正确的特征提取也会给模型带来增益，但是绝大程度上的精确度都是由wifi信息来提供的。所以，如何有效的提取wifi信息，去除其中的噪音，构造与wifi信息相关的特征，就是比赛的关键。3.就wifi信息来说，原始数据包含一个字符串，其中给出用户当前接收的所有wifi和强度，可能隐含的问题有，wifi是否稳定，用户个人热点，公共wifi，楼层问题，wifi值缺失。4.即使根据商场来进行预测，多的商场依然有近百个类别，可以分别用多分类和二分类实现多分类来尝试解决这个问题(在用二分类解决问题的时候，需要考虑样本不均衡的问题)。5.由于数据集非常大，如果直接用boosting模型的话耗费时间非常大，前期用LR来测试特征 3.具体做法1.数据预处理 删除公共wifi，因为本题中给出了mall信息，当两个mall距离较远的情况下，同一个wifiId在这两个mall或多个mall都出现过的话，那么就可以判定这个wifiId是公共wifi或者是个人热点。 训练集和测试集wifi取交集，因为对于wifi指纹或我们后来构造的wifi特征来说，只在train中出现过或只在test中出现过的wifi都是无用的，甚至可能是噪音。 离群值的去除，没发现有什么离群点，部分wifi强度值有缺失，进行删除 去掉经纬度离群点2.经纬度信息给了两种经纬度信息，一种是店铺经纬度（固定值），一种是买家付款时的经纬度，两种经纬度理论上差距应该很小，实际部分差距很大，单独利用经纬度进行预测的准确率大概在75%左右。 通过对精度的调整做了一个小的离散化处理 欧氏距离特征 曼哈顿距离特征 经纬度聚类（效果一般）3.时间特征的处理 提取饭点特征 提取早晨和深夜指示特征，因为这两种店可能比较固定4.用户特征 用户购买力 用户常去商店这里的用户特征是个坑，用户特征的提取会使得本地验证的分数提高不少，但是实际上可能是个噪音，因为测试集里的用户更换了绝大多数，记得好像只有不到1/5之一的旧用户吧，但是在训练集里用户特征会占很高的重要性。5.Wifi特征wifi特征是最主要的部分，这里我们主要构建了如下的wifi特征(1)当前用户连接到的最强wifi举个例子来说，当我能搜索到的最强wifi是wifi0的时候，在历史上最强wifi是wifi0的时候有80个人在A店铺，5个人在B店铺，10个人在C店铺，那么我最大可能当前在A店铺。这个特征算是一个比较强的特征了。(2)wifi出现的次数搜索到的wifi数，wifi历史计数(3)店铺wifi指纹根据每个店铺历史上出现过的wifi和强度建立wifi强度指纹库，取每个wifi出现过的所有值得中位数作为最终指纹值，比对当前强度wifi和指纹库(4)商场wifi原点统计所有的高频wifi，在数据集中出现频数超过20的wifi作为指纹，其wifi名称(id)作为特征，然后将强度作为wifi指纹的值，而后将wifi强度值离散化。(5)wifi评分遍历数据建立嵌套字典WiFiscore，一层key为WiFi id，二层key为一层WiFi出现过的店铺，二层val为历史上该WiFi在该店铺强度的中位数。打分，对于每条数据的稳定WiFi信息，遍历嵌套字典$WiFi_score$,用打分函数对每条WiFi出现过的店铺打分。分数结果最高的直接作为结果。打分函数如下：$$f=1-tanh({|power_{now}-power_{middle}|\over k})$$其中：$tanh={e^x-e^{-x}\over e^x+e^{-x}}$可以将当前wifi强度与中位数差映射到0,1之间。后处理除了特征以外，还用了一些简单的规则来对预测数据进行后处理，相对于模型预测来说，后处理可处理的数据很少，但是相对于模型来说更加准确。 wiif强度序列完全相同 当前连接的wifi在历史上所在店铺的极大似然其实在作比赛的时候远远试了比这些更多的特征，但是因为效果不好都去掉了，最后提交模型一共使用了以上特征。4.数据集划分因为这是个时间相关的预测问题，所以应该和大多数人一样，最终我选取了最后一个周进行训练，特征提取是在整个训练集上进行的5.模型最开始我们使用了xgb的多分类模型，分mall进行预测，效果一般，然后转而使用二分类实现多分类，依然分mall进行预测，提升显著。具体做法是使用N个二分类器，分别对每个mall的每个店铺进行二分类得到一个二分类器，然后对数据进行预测，这样对于每一个二分类器都可以得到一个预测概率值，选取其中预测最大的概率值对应的店铺作为最终分类结果。 样本 分类器1 分类器2 1 0.12 0.131 2 0.13 0.03 3 0.94 0.001 大概就是表格里这种（值是我瞎写的）这样做的好处还有一个是模型融合的时候会很快捷和高效，直接将模型概率加权相加即可模型融合:最后我们取了 0.65xgb + 0.35lgb 加权融合 欠缺的地方没有用时间窗 无法构造统计类特征，构造了会过拟合(会造成label leak,形成过拟合) 训练集过大训练时间太长 没有考虑时效性没有用候选集 没有构造联合特征，商店-wifi，商店-经纬度等等 无法添加商店特征 主要欠缺的点就是这两个，造成了我们早早达到了对于特征提取的天花板，无法添加商店特征。 样本不均衡由于在进行二分类的时候，正负样本之间的差距比较大，我们尝试用了规则(如经纬度)来构造负样本，没有用随机下采样和上采样，因为有些店铺的正样本非常少，而且随机下采样可能会破坏边界样本的分布。但是最终还是保留了全部的负样本。 就写这么多，比赛过去两个月了好多东西都忘了，以后想到再补充。]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>比赛</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DTree]]></title>
    <url>%2F2018%2F02%2F01%2FDTree%2F</url>
    <content type="text"><![CDATA[习题为课程作业: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990# coding=utf-8import numpy as npimport pandas as pdD = pd.DataFrame()D['A'] = ['H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'L', 'L', ]D['B'] = ['X', 'X', 'Y', 'Y', 'Y', 'Z', 'Z', 'Z', 'X', 'X', 'Y', 'Z', 'Z', 'Z']D['C'] = ['T', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'F', 'F', 'T', 'F', 'F']D['class'] = [0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1]def getEntropy(Dataset, fea, cate): print('$$ ----------------------------$$') print('Father Node:', fea, '，| category of FatherNode：', cate,'|') nowFeature = &#123;&#125; label_list = Dataset['class'].tolist() label = set(label_list) entropy = 0 for v in label: label_count = label_list.count(v) label_len = len(label_list) entropy += -(label_count / label_len) * np.log2((label_count / label_len)) print('H(D):', entropy) if entropy == 0.0: print('End,the label is', np.unique(Dataset['class'])[0]) return # 计算每个特征的信息增益 feature_list = Dataset.columns.tolist() feature_list.remove('class') if feature_list: for feature in feature_list: # 条件熵 conEntropy = 0 tempList = Dataset[feature].tolist() feature_cat = set(tempList) # 保存计算条件熵的字符串 mathCon = '' for cat in feature_cat: conEntropy += (tempList.count(cat) / len(tempList)) * Entropy(Dataset[Dataset[feature] == cat])[0] mathCon += '-' + '\\frac&#123;'+str(tempList.count(cat))+'&#125;&#123;'+str(len(tempList))+'&#125;' +'*'+Entropy(Dataset[Dataset[feature] == cat])[1] conEntropy = conEntropy increaseInfor = entropy - conEntropy mathCon = '$$'+str(increaseInfor) + mathCon+'='+ str(increaseInfor)+'$$' print('Feature', feature, 'Information gain：') print(mathCon) if float(increaseInfor) &gt;= 0.3: nowFeature[feature] = increaseInfor if not nowFeature: print('There is no feature g(D,A)&gt;0.3') if Dataset['class'].tolist().count(0) &gt;= Dataset['class'].tolist().count(1): print('Label：0') return else: print('Label:1') return return sorted(nowFeature.items(), key=lambda x: x[1], reverse=True) biggest_feature = '' biggest_gain = -1 for f, v in nowFeature.items(): if v &gt; biggest_gain: biggest_feature = f biggest_gain = v print('Feature has biggest information Gain:', biggest_feature) feature_list.remove(biggest_feature) if feature_list: feature_list.append('class') biggest_cat = set(Dataset[biggest_feature].tolist()) for cat in biggest_cat: getEntropy(Dataset[feature_list][Dataset[biggest_feature] == cat], biggest_feature, cat) else: return else: returndef Entropy(Dataset): label_list = Dataset['class'].tolist() label = set(label_list) entropy = 0 for v in label: label_count = label_list.count(v) label_len = len(label_list) entropy += -(label_count / label_len) * np.log2((label_count / label_len)) mathJax = '-\\frac&#123;'+str(label_count)+'&#125;&#123;'+str(label_len)+'&#125;' +'*log2\\frac&#123;'+str(label_count)+'&#125;&#123;'+str(label_len)+'&#125;' return [entropy,mathJax]getEntropy(D, 'no', 'no') 运行代码以后可以得到如下结果:` python$$ —————————-$$Father Node: no ，| category of FatherNode： no |H(D): 0.940285958671Feature A Information gain：$$0.00133974240444-\frac{6}{14}-\frac{2}{6}log2\frac{2}{6}-\frac{8}{14}-\frac{3}{8}log2\frac{3}{8}=0.00133974240444$$Feature B Information gain：$$0.314936851373-\frac{4}{14}-\frac{1}{4}log2\frac{1}{4}-\frac{6}{14}-\frac{4}{6}log2\frac{4}{6}-\frac{4}{14}-\frac{4}{4}log2\frac{4}{4}=0.314936851373$$Feature C Information gain：$$0.0902763493928-\frac{8}{14}-\frac{4}{8}log2\frac{4}{8}-\frac{6}{14}-\frac{1}{6}log2\frac{1}{6}=0.0902763493928$$Feature has biggest information Gain: B$$ —————————-$$Father Node: B ，| category of FatherNode： Y |H(D): 0.811278124459Feature A Information gain：$$0.122556248918-\frac{1}{4}-\frac{1}{1}log2\frac{1}{1}-\frac{3}{4}-\frac{1}{3}log2\frac{1}{3}=0.122556248918$$Feature C Information gain：$$0.122556248918-\frac{3}{4}-\frac{1}{3}log2\frac{1}{3}-\frac{1}{4}-\frac{1}{1}log2\frac{1}{1}=0.122556248918$$There is no feature g(D,A)&gt;0.3Label：0$$ —————————-$$Father Node: B ，| category of FatherNode： Z |H(D): 0.918295834054Feature A Information gain：$$0.0-\frac{3}{6}-\frac{2}{3}log2\frac{2}{3}-\frac{3}{6}-\frac{2}{3}log2\frac{2}{3}=0.0$$Feature C Information gain：$$0.459147917027-\frac{3}{6}-\frac{3}{3}log2\frac{3}{3}-\frac{3}{6}-\frac{1}{3}log2\frac{1}{3}=0.459147917027$$Feature has biggest information Gain: C$$ —————————-$$Father Node: C ，| category of FatherNode： F |H(D): 0.0End,the label is 1$$ —————————-$$Father Node: C ，| category of FatherNode： T |H(D): 0.918295834054Feature A Information gain：$$0.251629167388-\frac{1}{3}-\frac{1}{1}log2\frac{1}{1}-\frac{2}{3}-\frac{1}{2}log2\frac{1}{2}=0.251629167388$$There is no feature g(D,A)&gt;0.3Label：0$$ —————————-$$Father Node: B ，| category of FatherNode： X |H(D): 0.0End,the label is 0 使用markdown编辑器或者mathjax编辑器可以看到效果。]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer]]></title>
    <url>%2F2018%2F01%2F07%2F%E5%89%91%E6%8C%87offer-1%2F</url>
    <content type="text"><![CDATA[二位数组中的查找题目描述在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 My solution:用递归来解，从右下向左上对角线遍历，每次把大矩阵分为左下右上两个小矩阵进行递归，终止条件是target位于对角线或矩阵只剩一行或一列遍历target（矩阵可能行列数不相同）123456789101112131415161718192021222324252627282930313233343536# -*- coding:utf-8 -*-class Solution: # array 二维列表 def Find(self,target, array): def FindTarget(target, array): # 如果只有一行 if len(array) == 1: if target in array[0]: return True else: return False if len(array) &gt;= 2: # 如果只有一列 if len(array[0]) == 1 and len(array[1]) == 1: if target in [x[0] for x in array]: return True else: return False # 其它情况 else: height = len(array) width = len(array[0]) while (height &gt; 0 and width &gt; 0): v = array[height - 1][width - 1] height -= 1 width -= 1 if v == target: return True if v &gt; target and height!=0: else: remain1 = [x[width + 1:len(array[0])] for x in array[:height+1]] remain2 = [x[:width + 1] for x in array[height + 1:len(array)]] return (FindTarget(target, remain1)) or (FindTarget(target, remain2)) result = False result = FindTarget(target, array) return result 简单做法：从左下开始遍历，比target大就往上，比target小就往下（妈的，看完很气，还特么辛苦写了个递归，这个简单做法太简单就不写了，╭(╯^╰)╮） 从尾到头打印列表1234567891011class Solution: # 返回从尾部到头部的列表值序列，例如[1,2,3] def printListFromTailToHead(self, listNode): # write code here # write code here l = [] head = listNode while head: l.insert(0, head.val) head = head.next return l 很简单，注意是while head 不是 while head.next 旋转数组的最小数字题目描述：把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。思路就是一个数组，把从后面切片放到前面来，让找最小数字，最简单的做法是直接min()函数，但是耗时我的做法是使用递归，如果数组中间的数比开头的小，那么说明最小的数在这个切片里，不然就在后半切片里12345678910111213def minNumberInRotateArray(self, rotateArray): # write code here if len(rotateArray)==0: return 0 def findMin(array): if len(array)&lt;5: return min(array) else: if array[int(len(array)/2)]&gt;array[0]: return findMin(array[int(len(array)/2):]) if array[int(len(array)/2)]&lt;array[0]: return findMin(array[:int(len(array)/2)+1]) return findMin(rotateArray) 再优化一点就是从左到右找，找到下一个比当前的小了说明下一个就是最小值 输出斐波那契数列第n个数递归做法12345678910111213141516171819202122def Fibonacci(n): # write code here def getN(n): if n == 0: return 0 if n == 1 or n==2: return 1 else: return getN(n - 1) + getN(n - 2) return getN(n)``` **数组做法**``` pythondef Fibonacci(self, n): a = [0,1,1] if n&lt;3: return a[n] else: for i in range(n-2): a.append(a[-1]+a[-2]) return a[-1] 三个变量做法最快 12345678910111213public int Fibonacci(int n) &#123; int one = 0; int two = 1; if(n &lt;= 0) return one; if(n == 1) return two; int result = 0; for(int i = 2; i &lt;= n; i++)&#123; result = one + two; one = two; two = result; &#125; return result; &#125; 斐波那契数列扩展问题一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法。思路一个青蛙跳到第k个台阶，对于k-1个台阶来说只有一种跳法，就是跳1级，对k-2级台阶来说就是跳两级，也只有一种跳法，所以跳到第k个台阶的做法就是跳到第k-1个台阶的做法+跳到第k-2个台阶的做法也就是 斐波那契数列！精彩 变态跳台阶问题：一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。思路：跟上题类似，但是青蛙上次的位置不限于k-1和k-2了，可以在任何位置，用递归来实现，核心思想是：**递归的加上一次所有可能的位置（1到k-1）加上从0直接到当前位置 12345678910def jumpFloorII(number): # write code here def findAlljumps(n): if n==1: return 1 if n==2: return 2 else: return sum([findAlljumps(i) for i in range(1,n)],1) return findAlljumps(number) 矩形覆盖题目：我们可以用21的小矩形横着或者竖着去覆盖更大的矩形。请问用n个21的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？思路：用从后往前的递归方法想一下，先填满，然后依次往外拿，行已经知道是2了，列为n列，那么就是求f(n),递归往前一步想，有两类往外拿的方法，一个是拿一个横的，一个是拿一个竖着的，如果拿出一个竖着的，那么其实就是f(n-1)然后填一个竖着的到f(n),如果拿一个横着的（其实相当于拿出两个横着的，因为不管拿上面还是下面的横着的，它对应的上面或下面的那个横着的没有其它拿法，所以相当于f(n-2),所以这道题其实又是一个斐波那契数列的问题了）123456789101112131415def rectCover(self, number): # write code here if not number: return 0 if number == 1: return 1 else: last = 1 current = 2 while number-2&gt;0: temp = current current = last + current last = temp number-=1 return current 机器人的运动范围题目：地上有一个m行和n列的方格。一个机器人从坐标0,0的格子开始移动，每一次只能向左，右，上，下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于k的格子。 例如，当k为18时，机器人能够进入方格（35,37），因为3+5+3+7 = 18。但是，它不能进入方格（35,38），因为3+5+3+8 = 19。请问该机器人能够达到多少个格子？思路：用递归上下左右的满足条件去走，每走一步count+1，具体的条件：1.下一步不超边界2.下一步满足横纵坐标各个位数之和不大于k3.下一步不重复（可以把走过的位置用字典存起来） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374def movingCount(threshold, rows, cols): # write code here if rows==cols==1: return 1 count = [] hasGone = &#123;&#125; hasGone[0] = [0] def isSmaller(n, x, y): xSum = 0 ySum = 0 for i in range(len(str(x))): xSum += int(str(x)[i]) for j in range(len(str(y))): ySum += int(str(y)[j]) if xSum+ySum &gt; n: return False else: return True def fuck(): print('调用') count.append(1) def move(i,j): global count # 往上走 if i&gt;0: if isSmaller(threshold,i-1,j): if i-1 not in hasGone: fuck() hasGone[i-1] = [j] move(i - 1, j) else: if j not in hasGone[i-1]: fuck() hasGone[i-1] += [j] move(i - 1, j) # 往下走 if i&lt;rows-1: if isSmaller(threshold,i+1,j): if i+1 not in hasGone: fuck() hasGone[i+1] = [j] move(i + 1, j) else: if j not in hasGone[i+1]: fuck() hasGone[i+1] += [j] move(i + 1, j) # 往左 if j&gt;0: if isSmaller(threshold,i,j-1): if i not in hasGone: fuck() hasGone[i] = [j-1] move(i, j-1) else: if j-1 not in hasGone[i]: fuck() hasGone[i] += [j-1] move(i, j-1) # 右 if j&lt;cols-1: if isSmaller(threshold,i,j+1): if i not in hasGone: fuck() hasGone[i] = [j+1] move(i, j+1) else: if j+1 not in hasGone[i]: fuck() hasGone[i] += [j+1] move(i, j+1) move(0,0) return sum(count)+1 矩阵中的路径题目：设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则该路径不能再进入该格子。 例如 a b c e s f c s a d e e 矩阵中包含一条字符串”bcced”的路径，但是矩阵中不包含”abcb”路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。思路：想把输出按照行列换成对应的矩阵，然后找到所有的开头点，对于每个开头点：先把指针cur指向path的下一个字符 递归，上下左右查看 如果满足边界内并且非重复并且是cur当前所指的字符 指针+1，横纵坐标对应操作一并当做参数进行下一轮递归，坐标加入已经走过的坐标字典1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798# -*- coding:utf-8 -*-class Solution: def hasPath(self, matrix, rows, cols, path): if len(path)==0: return False if len(path)==1: if path in matrix: return True else: return False result = [] def finalfind(): result.append(1) def findStr(i,j,cur): if cur == len(path): finalfind() return elif cur &lt; len(path): if i&gt;0: # 往上 if M[i-1][j] == path[cur]: if i-1 in GoneDict: if j not in GoneDict[i-1]: GoneDict[i-1]+=[j] #print('shang',i-1,j) findStr(i-1,j,cur+1) else: GoneDict[i-1] = [j] #print('shang', i - 1, j) findStr(i - 1, j, cur+1) if i&lt;rows-1: # 往下 if M[i + 1][j] == path[cur]: if i + 1 in GoneDict: if j not in GoneDict[i + 1]: GoneDict[i + 1] += [j] #print('xia', i + 1, j) findStr(i + 1, j, cur+1) else: GoneDict[i + 1] = [j] findStr(i + 1, j, cur+1) if j&gt;0: # 左 if M[i][j-1] == path[cur]: if i in GoneDict: if j-1 not in GoneDict[i]: GoneDict[i] += [j-1] #print('zuo', i, j-1) findStr(i, j-1, cur+1) else: GoneDict[i] = [j-1] #print('zuo', i, j - 1) findStr(i, j-1, cur+1) if j&lt;cols-1: # 右 if M[i][j+1] == path[cur]: if i in GoneDict: if j+1 not in GoneDict[i]: GoneDict[i] += [j+1] #print('you', i, j + 1) findStr(i, j+1, cur+1) else: GoneDict[i] = [j+1] #print('you', i, j + 1) findStr(i, j+1, cur+1) # write code here GoneDict = &#123;&#125; M = [] start = 0 end = cols for i in range(rows): M.append([x for x in matrix[start:end]]) start = end end += cols # 记录当前查询的位置 startPoint = [] # 找到所有开头的节点 for i in range(rows): for j in range(cols): if M[i][j] == path[0]: startPoint.append([i,j]) if startPoint: for point in startPoint: # 对于每一个头重置字典 GoneDict = &#123;&#125; # 先把开头的节点放入字典 GoneDict[point[0]] = [point[1]] findStr(point[0],point[1],1) if result ==True: break if result: return True else: return False 滑动窗口的最大值题目：给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}， {2,3,4,2,6,[2,5,1]}。12345678910def maxInWindows(self, num, size): # write code here if size==0: return [] result = [] if size &gt; len(num): return [] for i in range(len(num) -size+ 1): result.append(max(num[i:i + size])) return result 二叉搜索树的第K个节点二叉搜索数的中序遍历是有序的，因此只要把第k个节点返回12345678910111213141516171819202122232425# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回对应节点TreeNode def KthNode(self, pRoot, k): # write code here # 特殊情况1 if k==0: return result = [] def middle(root): if root==None: return middle(root.left) result.append(root) middle(root.right) middle(pRoot) # 特殊情况2 if k&gt;len(result): return return result[k-1] 二进制中1的个数输入一个整数，可能为负数，求其二进制中1的个数12345# -*- coding:utf-8 -*-class Solution: def NumberOf1(self, n): # write code here return sum([1 &amp; n&gt;&gt;i for i in range(32)]) 其二进制长度为32位，依次右移i位，也就是每次比较最后一位 &amp; 1 ，判断最后一位是否为0. 调整数组使得奇数位于偶数前输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。12345# -*- coding:utf-8 -*-class Solution: def reOrderArray(self, array): # write code here return [x for x in array if x%2!=0] + [x for x in array if x%2==0] 链表中倒数第k个节点做法一，遍历一遍，把每个节点保存下来，最后返回第-k个做法二，用两个指针，第一个先走k步，然后一起走，第一个到尾节点的时候后面的那个指针应该正好倒数第k个12345678910111213class Solution: def FindKthToTail(self, head, k): # write code here if not head: return pre = head l = [head] while pre.next!=None: pre = pre.next l.append(pre) if len(l) &lt; k or k &lt;= 0: return return l[-k] 输入一个链表，反转链表后，输出链表的所有元素。1234567891011121314151617class Solution: # 返回ListNode def ReverseList(self, pHead): # write code here if not pHead or not pHead.next: return pHead def revLinked(pre,after): if after == None: return pre if pre == pHead: pre.next = None temp = after.next after.next = pre if after.next == None: return after return revLinked(after,temp) return revLinked(pHead,pHead.next) 递归，每次输入两个节点，让后面的节点指向前面的节点,别忘了处理头结点合并两个排序的链表题目：输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。123456789101112131415161718192021222324252627282930313233class Solution: # 返回合并后列表 def Merge(self, pHead1, pHead2): # write code here if not pHead1: return pHead2 if not pHead2: return pHead1 p1 = pHead1 p2 = pHead2 head = ListNode(0) pre = head while True: if p1!=None and p2!=None: if p1.val &lt; p2.val: pre.next = ListNode(p1.val) p1 = p1.next else: pre.next = ListNode(p2.val) p2 = p2.next pre = pre.next elif p1==None: while p2: pre.next = ListNode(p2.val) p2 = p2.next pre = pre.next elif p2==None: while p1: pre.next = ListNode(p1.val) p1 = p1.next pre = pre.next if p1==None and p2==None: return head.next 每次从两个链表当前指针所在位置选一个最小的，如果一条链表为空，添加剩余链表 魔法币小易准备去魔法王国采购魔法神器,购买魔法神器需要使用魔法币,但是小易现在一枚魔法币都没有,但是小易有两台魔法机器可以通过投入x(x可以为0)个魔法币产生更多的魔法币。魔法机器1:如果投入x个魔法币,魔法机器会将其变为2x+1个魔法币魔法机器2:如果投入x个魔法币,魔法机器会将其变为2x+2个魔法币小易采购魔法神器总共需要n个魔法币,所以小易只能通过两台魔法机器产生恰好n个魔法币,小易需要你帮他设计一个投入方案使他最后恰好拥有n个魔法币。123456789101112#输入描述:输入包括一行,包括一个正整数n(1 ≤ n ≤ 10^9),表示小易需要的魔法币数量。#输出描述:输出一个字符串,每个字符表示该次小易选取投入的魔法机器。其中只包含字符'1'和'2'。#输入例子1:10#输出例子1:122 MySolution12345678910111213141516171819n = int(input()) arr=[]def getStr(n): if n==0: return if n%2!=0: # 奇数 arr.append(1) return getStr(int((n-1)/2)) if n%2==0: # 偶数 arr.append(2) return getStr(int((n-2)/2))getStr(n)arr.reverse()finalStr=''for x in arr: finalStr += str(x)print(finalStr) 思路 递归，如果最终结果是偶数，那么上一步一定用了机器2，奇数机器1同理 ”相反数“”为了得到一个数的”相反数”,我们将这个数的数字顺序颠倒,然后再加上原先的数得到”相反数”。例如,为了得到1325的”相反数”,首先我们将该数的数字顺序颠倒,我们得到5231,之后再加上原先的数,我们得到5231+1325=6556.如果颠倒之后的数字有前缀零,前缀零将会被忽略。例如n = 100, 颠倒之后是1.123456789101112#输入描述:输入包括一个整数n,(1 ≤ n ≤ 10^5)#输出描述:输出一个整数,表示n的相反数#输入例子1:1325#输出例子1:6556 MySolution12345678n = int(input())arr = [x for x in str(n)]arr.reverse()finalNumber = ''for x in arr: finalNumber+=xfinalNumber = int(finalNumber)print (finalNumber + n) 思路：很简单 字符串碎片一个由小写字母组成的字符串可以看成一些同一字母的最大碎片组成的。例如,”aaabbaaac”是由下面碎片组成的:’aaa’,’bb’,’c’。牛牛现在给定一个字符串,请你帮助计算这个字符串的所有碎片的平均长度是多少。123456789101112131415输入描述:输入包括一个字符串s,字符串s的长度length(1 ≤ length ≤ 50),s只含小写字母('a'-'z')输出描述:输出一个整数,表示所有碎片的平均长度,四舍五入保留两位小数。如样例所示: s = "aaabbaaac"所有碎片的平均长度 = (3 + 2 + 3 + 1) / 4 = 2.25输入例子1:aaabbaaac输出例子1:2.25 MySolution12345678910s = str(input())def findS(s): if len(s)==1: return 1.0 sum=1 for i in range(len(s)-1): if s[i]!=s[i+1]: sum+=1 return len(s)/sumprint('%.2f'%findS(s)) 思路：很简单的题目，就注意格式化的时候，我一开始用round，报错3.50要求不能3.5，于是用了%.2f 重排数列小易有一个长度为N的正整数数列A = {A[1], A[2], A[3]…, A[N]}。牛博士给小易出了一个难题:对数列A进行重新排列,使数列A满足所有的A[i] * Ai + 1都是4的倍数。小易现在需要判断一个数列是否可以重排之后满足牛博士的要求。123456789101112131415161718#输入描述:#输入的第一行为数列的个数t(1 ≤ t ≤ 10),#接下来每两行描述一个数列A,第一行为数列长度n(1 ≤ n ≤ 10^5)#第二行为n个正整数A[i](1 ≤ A[i] ≤ 10^9)#输出描述:#对于每个数列输出一行表示是否可以满足牛博士要求,如果可以输出Yes,否则输出No。#示例1#输入231 10 10041 2 3 4#输出YesNo MySolution1234567891011121314151617181920s = int(input())resultArr = []dic = &#123;&#125;for i in range(s): n = int(input()) arr = [int(y) for y in str(input()).split(' ')] dic[n] = arr arr = [int(x%4) for x in arr] ji = arr.count(1)+arr.count(3)-1 # 有能被2整除的 if arr.count(2)&gt;0: arr = [x for x in arr if x != 2] arr.append(1) #如果存在能被2整除的，这些数必须紧邻在一起当做一个奇数 if float(arr.count(0))&gt;=(len(arr)-1)/2: resultArr.append('Yes') else: resultArr.append('No')for x in resultArr: print(x) 思路： 如果有能被2整除的，那为了满足条件，它们必须紧挨着，那就相当于一个奇数，所以可以删除它们加进来一个奇数，那么数组就只剩下奇数和能被4整除的数了，那么满足被4整除的数插在奇数中间即可，即 n* &gt;= (len(array)-1)/2 或者 大于等于奇数-1，都一样. 用两个栈实现队列用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。思路队列是先进先出，栈是先进后出，每次要pop的时候，把stack1中的值按顺序压入到stack2，再按顺序出，这样先进的就可以先出了。12345678910111213141516class Solution: def __init__(self): self.stack1 = [] self.stack2 = [] def push(self, node): # write code here self.stack1.append(node) def pop(self): # return xx if self.stack2 == []: while self.stack1: # 注意这里的pop是list的pop，弹出list的队尾元素，原list改变 a = self.stack1.pop() self.stack2.append(a) if self.stack2: return self.stack2.pop() 判断树的子结构输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构）12345678910111213141516class Solution: def HasSubtree(self, pRoot1, pRoot2): # write code here if not pRoot1 or not pRoot2: return False # 注意这里第一个是subTree来判断根节点，然后下面后面两个or接的是依次遍历的A的左右子树 return self.is_subTree(pRoot1,pRoot2) or self.HasSubtree(pRoot1.left,pRoot2) or self.HasSubtree(pRoot1.right,pRoot2) def is_subTree(self,p1,p2): # 在这个路线上p1和p2的值一直相等知道p2到头了，返回true if not p2: return True # 如果路线出现p1到头了或者p1,p2值不相等，那么构不成子树，返回False if not p1 or p1.val != p2.val: return False # 如果p1,p2都没到头并且值相等，继续判断(p1左，p2左),(p1右，p2右) return self.is_subTree(p1.left,p2.left) and self.is_subTree(p1.right,p2.right) 思路：代码是从讨论区看到的，理解后自己又写了一遍，思路很简单，但是实现用的递归很巧妙，从根节点开始一次遍历判断A的左右子树是否可以作为B的头结点而形成一颗和B一样的子树。 二叉树的镜像操作给定的二叉树，将其变换为源二叉树的镜像。输入描述:123456789101112二叉树的镜像定义：源二叉树 8 / \ 6 10 / \ / \ 5 7 9 11 镜像二叉树 8 / \ 10 6 / \ / \ 11 9 7 5 12345678class Solution: # 返回镜像树的根节点 def Mirror(self, root): # write code here if root: root.left,root.right = root.right,root.left self.Mirror(root.left) self.Mirror(root.right) 思路：直接递归，从上到下每个节点的左右互换即可 顺时针打印矩阵输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字，例如，如果输入如下矩阵： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 则依次打印出数字1,2,3,4,8,12,16,15,14,13,9,5,6,7,11,10.123456789101112131415161718192021222324252627282930313233def printMatrix(matrix): # write code here length = len(matrix[0]) width = len(matrix) if width == 1: return matrix[0] result = [matrix[0][0]] hasGone = &#123;&#125; for i in range(width): hasGone[i] = [] hasGone[0] = [0] x0, y0 = 0, 0 while True: if len(result) &gt;= width*length: break while y0 &lt; length - 1 and y0+1 not in hasGone[x0]: hasGone[x0] += [y0 + 1] result.append(matrix[x0][y0 + 1]) y0 += 1 while x0 &lt; width - 1 and y0 not in hasGone[x0+1]: hasGone[x0 + 1] += [y0] result.append(matrix[x0 + 1][y0]) x0 += 1 while y0 &gt; 0 and y0-1 not in hasGone[x0]: hasGone[x0] += [y0 - 1] result.append(matrix[x0][y0 - 1]) y0 -= 1 # 编写的时候这里出过错，不是matrix[x0-1][y0] not in ,而是y0,注意点 while x0 &gt; 0 and y0 not in hasGone[x0-1]: hasGone[x0 - 1] += [y0] result.append(matrix[x0 - 1][y0]) x0 -= 1 return result 思路：还是老方法，按右下左上的顺序前进直到result长度达到预期，注意判别条件。 包含min函数的栈定义栈的数据结构，请在该类型中实现一个能够得到栈最小元素的min函数。12345678910111213141516class Solution: def __init__(self): self.stack = [] def push(self, node): # write code here self.stack.append(node) def pop(self): # write code here return self.stack.pop() def top(self): # write code here return self.stack[-1] def min(self): # write code here # 偷懒写法 return min(self.stack) 思路：注意在python中stack的写法，在init中用一个list来表示栈，队尾就是栈顶，pop就是从栈顶往外弹，整好对应着list的pop默认参数 栈的压入、弹出序列输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4，5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的）1234567891011121314def IsPopOrder(pushV, popV): # write code here if not pushV or not popV or len(pushV) != len(popV): return False stack = [] for i in range(len(pushV)): stack.append(pushV[i]) while len(stack) and stack[-1] == popV[0]: stack.pop() popV.pop(0) if len(stack): return False return Trueprint(IsPopOrder([1,2,3,4,5],[4,5,3,2,1])) 思路：已知栈的压入顺序和弹出顺序，如果是正确的，那么按照顺序来一遍的话栈应该完全弹空，所以就按照压入顺序和弹出顺序模拟一遍，如果栈无法弹空，那么弹出顺序就是有错的，discuss里有个Alias写的不错： Alias头像Alias【思路】借用一个辅助的栈，遍历压栈顺序，先讲第一个放入栈中，这里是1，然后判断栈顶元素是不是出栈顺序的第一个元素，这里是4，很显然1≠4，所以我们继续压栈，直到相等以后开始出栈，出栈一个元素，则将出栈顺序向后移动一位，直到不相等，这样循环等压栈顺序遍历完成，如果辅助栈还不为空，说明弹出序列不是该栈的弹出顺序。 举例： 入栈1,2,3,4,5 出栈4,5,3,2,1 首先1入辅助栈，此时栈顶1≠4，继续入栈2 此时栈顶2≠4，继续入栈3 此时栈顶3≠4，继续入栈4 此时栈顶4＝4，出栈4，弹出序列向后一位，此时为5，,辅助栈里面是1,2,3 此时栈顶3≠5，继续入栈5 此时栈顶5=5，出栈5,弹出序列向后一位，此时为3，,辅助栈里面是1,2,3 …. 依次执行，最后辅助栈为空。如果不为空说明弹出序列不是该栈的弹出顺序。 从上往下打印二叉树从上往下打印出二叉树的每个节点，同层节点从左至右打印。1234567891011121314151617class Solution: # 返回从上到下每个节点值列表，例：[1,2,3] def PrintFromTopToBottom(self, root): # write code here if not root: return [] result = [root.val] NodeLs = [root] while NodeLs: current = NodeLs.pop(0) if current.left: result.append(current.left.val) NodeLs.append(current.left) if current.right: result.append(current.right.val) NodeLs.append(current.right) return result 思路：用一个队列来存储节点，从根节点开始，依次弹出，然后弹出节点的左右子节点依次入队 判断二叉搜索树的后序遍历序列123456789101112131415161718192021222324class Solution: def VerifySquenceOfBST(self, sequence): # write code here if not sequence: return False return self.Verify(sequence) def Verify(self,sequence): # 终止的正确条件是左右子树队列为1(子节点)或者为0(无左右子树) if len(sequence)==0 or len(sequence)==1: return True # 从左边开始的指针0 left = 0 # 队尾是根节点的值 root_val = sequence[-1] # 左半部分的值应该都小于跟节点的值 while sequence[left] &lt; root_val: left+=1 # 如果剩下的部分(右子树部分)有小于跟节点值的节点，说明有错，返回False for i in range(left,len(sequence)-1): if sequence[i] &lt; root_val: return False # 从根节点开始左右子序列必须全部满足条件才行 return self.Verify(sequence[:left]) and self.Verify(sequence[left:len(sequence)-1]) 思路：Discuss里有一个总结的不错：BST的后序序列的合法序列是，对于一个序列S，最后一个元素是x （也就是根），如果去掉最后一个元素的序列为T，那么T满足：T可以分成两段，前一段（左子树）小于x，后一段（右子树）大于x，且这两段（子树）都是合法的后序序列。完美的递归定义 : )，具体的注释看代码里 二叉树中和为某一值的路径输入一颗二叉树和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。123456789101112131415161718192021class Solution: # 返回二维列表，内部每个列表表示找到的路径 def FindPath(self, root, expectNumber): # write code here if not root: return [] result = [] def goPath(root,exNumber,subList): # 这里一定要重新开辟一个内存建一个新的暂时列表，不然递归的过程中一直用同一个subList tempLst = subList[:]+[root.val] if root.val == exNumber: # 根据题意，要判断是否是叶节点，不加这个判别条件的话可以有更多的路径满足 if not root.left and not root.right: result.append(tempLst) return if root.left: goPath(root.left,exNumber-root.val,tempLst) if root.right: goPath(root.right,exNumber-root.val,tempLst) goPath(root,expectNumber,[]) return result 思路：思路没什么新颖的地方，依然是常规的递归，遍历每一条路径，把子节点，target-子节点的值，subList作为参数进行递归，如果有正确的路径就加入到result，但是有两点需要注意下： tempLst = subList[:]+[root.val]，每个递归体内的subList复制给一个新空间的list，否则sublist只有一个，结果会造成相当于一个前序遍历输出了 注意题目条件是路径必须从根到叶子，所以最后需要判别下改路径的尾是不是到叶了 复杂链表的复制☆输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空）先上代码123456789101112131415161718192021222324252627282930313233343536373839class Solution: # 返回 RandomListNode def Clone(self, pHead): # write code here if not pHead: return None # 追加相同链表 pHeadTemp = pHead while pHeadTemp: newNode = RandomListNode(pHeadTemp.label) newNode.random = pHeadTemp.random temp = pHeadTemp.next pHeadTemp.next = newNode newNode.next = temp pHeadTemp = pHeadTemp.next.next # 处理复制后的链表的random pHeadTemp = pHead while pHeadTemp.next: if pHeadTemp.random: pHeadTemp.next.random = pHeadTemp.next.random.next if pHeadTemp.next.next: pHeadTemp = pHeadTemp.next.next if not pHeadTemp.next.next: break # 拆分 newHead = pHead.next preNew = newHead preOld = pHead while preOld: if preNew.next: preOld.next = preNew.next preNew.next = preNew.next.next preOld = preOld.next preNew = preNew.next else: # 不要忘记断掉最后的那个连接处 preOld.next = None break return newHead 思路：思路其实还是很简单的，借鉴下面这种说法： 1、复制每个节点，如：复制节点A得到A1，将A1插入节点A后面 2、遍历链表，A1-&gt;random = A-&gt;random-&gt;next; 3、将链表拆分成原链表和复制后的链表第一步：复制每个节点，意思是复制value和random，但是next不一样，插入在每个原节点后面第二步：处理random，恰好复制的每个节点的random的正确指向的位置是他们复制过来指向位置的next，所以只需要A1-&gt;random = A-&gt;random-&gt;next就可以处理好random第三步：拆分列表，这一步也是我出错比较多的地方，主要两个点1：不能改变原本的链表，我一开始疏忽了这一点，只管复制后的链表对了 2：拆分的最后位置要注意是否还有连接处 字符串的排列输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。12345678910111213141516171819class Solution: def Permutation(self,ss): # write code here if not ss: return [] result = [] ss = [s for s in ss] length = len(ss) def printAll(subStr, string): if len(subStr) == length: if subStr not in result: result.append(subStr) return newString = string[:] for i in range(len(string)): printAll(subStr+newString[i],newString[0:i]+newString[i+1:len(newString)]) printAll('', ss) return result 思路：之前做过类似的，非常简单的一个递归，但是在操作过程中还是犯错了：1.没注意传入的参数会改变的问题，应该通过[:]新建变量2.没注意字符重复使用问题，比如aa-&gt;[‘aa’,’aa’]是错误的3.递归的时候变量修改直接放参数中，不要在外部赋值，不然for循环过程中就无法控制变量了4.list.remove()返回的不是list! 数组中出现超过一半的数字数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。123456789101112class Solution: def MoreThanHalfNum_Solution(self, numbers): # write code here countDict = &#123;&#125; for num in numbers: if num not in countDict: countDict[num] = 1 else: countDict[num] += 1 if countDict[num] &gt; int(len(numbers)/2): return num return 0 思路：很简单的循环计数，用字典 最小的k个数输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4,。1234class Solution: def GetLeastNumbers_Solution(self, tinput, k): # write code here return sorted(tinput)[:k] if k &lt;= len(tinput) else [] 思路：python大法吼！ 连续子数组的最大和例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为8(从第0个开始,到第3个为止)12345678910111213class Solution: def FindGreatestSumOfSubArray(self, array): # write code here curSum = 0 maxSum = -999 for num in array: if curSum &gt;= 0: curSum += num else: curSum = num if curSum &gt; maxSum: maxSum = curSum return maxSum 思路：动态规划，重点是curSum,它是num之前的子向量的和，如果是个非负数，那么加上num肯定比num本身大或者相等，如果是个负数，那么对于num来说前面的子序列已经没有价值了，令curSum=num,然后判断代替maxSum。 整数中1出现的次数(从1到n)求出1~13的整数中1出现的次数,并算出100~1300的整数中1出现的次数？为此他特别数了一下1~13中包含1的数字有1、10、11、12、13因此共出现6次,但是对于后面问题他就没辙了。ACMer希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数。12345678910111213141516171819202122232425262728class Solution: def NumberOf1Between1AndN_Solution(self,n): # write code here # 从个位开始 i = 1 count = 0 if n==0: return 0 if n &lt; 10: return 1 while i &lt;= n: # 计算weight weight = int(n / (i * 10)) # 当前数 current = (n / i) % 10 # 剩下的数 rest = n % i if current == 1: # 如果当前位为1，那么当前位可能为1的个数由高位和地位共同决定，等于(高位的数*当前i + 地位数 + 1) count += weight * i + rest + 1 elif current &gt; 1: # 如果当前位大于1，那么当前可能为1的个人仅由高位决定，等于(高位+1) * 当前位数i count += (weight+1) * i else: # 如果当前位位0，仅由高位决定，等于(高位*当前位数i) count += weight * i i *= 10 return count 思路：编程之美上给出的规律： 如果第i位（自右至左，从1开始标号）上的数字为0，则第i位可能出现1的次数由更高位决定（若没有高位，视高位为0），等于更高位数字X当前位数的权重10i-1。 如果第i位上的数字为1，则第i位上可能出现1的次数不仅受更高位影响，还受低位影响（若没有低位，视低位为0），等于更高位数字X当前位数的权重10i-1+（低位数字+1）。 如果第i位上的数字大于1，则第i位上可能出现1的次数仅由更高位决定（若没有高位，视高位为0），等于（更高位数字+1）X当前位数的权重10i-1。然后从个位到最高位，依次按照上面的规则计算每一位出现1的次数累加即可 把数组排成最小的数输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组{3，32，321}，则打印出这三个数字能排成的最小数字为321323。123456789101112131415161718class Solution: def PrintMinNumber(self, numbers): # write code here # 依次找最小的数，这个最小的数从每个数的开头第一个找，如果有相同的依次往下顺 if not numbers: return "" numbers = [str(num) for num in numbers] def compare(A,B): if A+B &lt;= B+A: return -1 else: return 1 # cmp定义一个比较函数，接收两个函数，如果A&lt;B，返回负值 newNumber = sorted(numbers,cmp=compare) result = '' for x in newNumber: result += x return int(result) 思路：按照字符串排序，但是排序的规则是如果字符串A+B &lt; B+A,那么字符串A比较小 丑数把只包含因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。12345678910111213141516171819class Solution: def GetUglyNumber_Solution(self,index): # write code here if index &lt;= 0: return 0 A = [1] twoIndex = 0 threeIndex = 0 fiveIndex = 0 for i in range(index-1): newNumber = min(A[twoIndex]*2,A[threeIndex]*3,A[fiveIndex]*5) A += [newNumber] if newNumber%2 == 0: twoIndex +=1 if newNumber%3 == 0: threeIndex +=1 if newNumber%5 == 0: fiveIndex +=1 return A[-1] 思路：先上个详细思路：对于任何丑数p：（一）那么2p,3p,5p都是丑数，并且2p&lt;3p&lt;5p（二）如果p&lt;q, 那么2p&lt;2q,3p&lt;3q,5p&lt;5q现在说说算法思想： 由于1是最小的丑数，那么从1开始，把21，31，51，进行比较，得出最小的就是1的下一个丑数，也就是21， 这个时候，多了一个丑数‘2’，也就又多了3个可以比较的丑数，22，32，52，这个时候就把之前‘1’生成的丑数和‘2’生成的丑数加进来也就是(31,51,22，32，52)进行比较，找出最小的。。。。如此循环下去就会发现，每次选进来一个丑数，该丑数又会生成3个新的丑数进行比较。 上面的暴力方法也应该能解决，但是如果在面试官用这种方法，估计面试官只会摇头吧。下面说一个O（n）的算法。 在上面的特（fei）点（hua）中，既然有p&lt;q, 那么2p&lt;2q，那么“我”在前面比你小的数都没被选上，你后面生成新的丑数一定比“我”大吧，那么你乘2生成的丑数一定比我乘2的大吧，那么在我选上之后你才有机会选上。其实每次我们只用比较3个数：用于乘2的最小的数、用于乘3的最小的数，用于乘5的最小的数。也就是比较(2x , 3y, 5z) ，x&gt;=y&gt;=z的，重点说说下面代码中p的作用：int p[] = new int[] { 0, 0, 0 }; p[0]表示最小用于乘2比较数在数组a中的【位置】讲的通俗一点就是：重点是所有的丑数都是由它前面的丑数2,3,5得到的，最开始数组中只有[1]，那么下个数肯定是12,3,5得到的小的数2，那么我把新得到的丑数2也放入数组，这时候[1,2]都有资格去2,3,5得到下一个丑数，那么下一个比较的是不是12,13,15,22,23,25呢？不是。因为如果23能得到下一个最小的数，那么13就更有资格了，所以每次只需要记录每一个用来乘以2,3,5的最小的数就可以了。最开始只有1，然后2得到了2，这个时候用来乘以2的最小的数就往后移一位，用来乘以3和5的最小的数依然是1,所以这时候比较的是[13,15,22],把3加入数组。这时候用来乘以2的最小的数依然是2，用来乘以3的最小的数往后移一位变成了2，用来乘以5的最小的数依然是1，所以比较的三个数乘是[22,23,1*5],得到最小的丑数4加入数组………… 第一个只出现一次的字符在一个字符串(1&lt;=字符串长度&lt;=10000，全部由字母组成)中找到第一个只出现一次的字符,并返回它的位置python一行解法：就是挨个调用count统计次数1234class Solution: def FirstNotRepeatingChar(self, s): # write code here return [i for i in range(len(s)) if s.count(s[i])==1][0] if s else -1 标准书本解法12345678910111213141516171819class Solution: def FirstNotRepeatingChar(self, s): # write code here charDict = &#123;&#125; if not s: return -1 checkList = [] # 第一遍遍历，计数并存入字典 for c in s: if c in charDict: charDict[c] += 1 else: checkList += [c] charDict[c] = 1 # 第二遍遍历，查询字典是否为1 for i in range(len(s)): if s[i] in charDict and charDict[s[i]]==1: return i return -1 两个链表的第一个公共节点输入两个链表，找出它们的第一个公共结点。123456789101112class Solution: def FindFirstCommonNode(self, pHead1, pHead2): # write code here pHead1Add = &#123;&#125; while pHead1: pHead1Add[id(pHead1)] = 0 pHead1 = pHead1.next while pHead2: if id(pHead2) in pHead1Add: break pHead2 = pHead2.next return pHead2 思路：遍历一遍第一个链表，把地址存入字典，遍历第二个字典，如果有相同地址就返回,取地址 id(obj) 数字在排序数组中出现的次数统计一个数字在排序数组中出现的次数。12345# 无赖写法class Solution: def GetNumberOfK(self, data, k): # write code here return data.count(k) 其实这个题肯定是二分查找了，下面分别给出循环和递归的写法123456789101112131415161718192021222324252627循环写法``` pythondef GetNumberOfK(data, k): # write code here start = 0 end = len(data) - 1 # 二分查找固定格式记住，while必须是&lt;=，因为相等的时候可以判断最后的那个边界值 while start &lt;= end: print start,end mid = (start + end) / 2 if data[mid] == k: start = mid end = mid # 注意边界值 注意条件判别条件 # 找到开头 while start &gt; 0 and data[start-1] == k: start -= 1 # 找到结尾 while end &lt; len(data) - 1 and data[end+1] == k: end += 1 print start,end break elif data[mid] &gt; k: start = mid + 1 elif data[mid] &lt; k: end = mid - 1 return end - start + 1 二叉树的深度输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。1234567891011121314151617181920class Solution: # 注意这里不用加self totalDepth = 1 def TreeDepth(self, pRoot): # write code here if not pRoot: return 0 def findDepth(root,depth): temp = depth if not root: return # 注意这里的判别条件 if (root.left or root.right) and temp &gt;= self.totalDepth: self.totalDepth += 1 if root.left: findDepth(root.left,temp+1) if root.right: findDepth(root.right,temp+1) findDepth(pRoot,1) return self.totalDepth 思路：递归，递归过程中记录当前路径的参数，全局变量记录当前最深路径参数，替换。还有一种更加巧妙的方法12345678def TreeDepth(self, pRoot): # write code here if pRoot == None: return 0 nLeft = self.TreeDepth(pRoot.left) nRight = self.TreeDepth(pRoot.right) return max(nLeft+1,nRight+1)#(nLeft+1 if nLeft &gt; nRight else nRight +1)# 这里返回值一定是数值，而且是从下到上递归的选最大的那个数 平衡二叉树输入一颗二叉树，判断该二叉树是否是平衡二叉树123456789101112131415161718192021222324class Solution: def isAVL(root): if not root: return True if root.left and root.left.val &gt; root.val: return False if root.right and root.right.val &lt; root.val: return False else: return True def findDepth(self,root): if not root: return 0 nLeft = self.findDepth(root.left) nRight = self.findDepth(root.right) return max(nLeft+1,nRight+1) def IsBalanced_Solution(self, pRoot): # write code here if not pRoot: return True if abs(self.findDepth(pRoot.left) - self.findDepth(pRoot.right)) &gt; 1: return False return self.IsBalanced_Solution(pRoot.left) and self.IsBalanced_Solution(pRoot.right) 思路：判断是否是平衡二叉树主要判断两个点 是否平衡，即左右两个子树的深度不能相差超过1。 是否为二叉搜索树，这个可以通过递归判断每个节点或者判断中序遍历是否有序来判断。 数组中只出现一次的数字一个整型数组里除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现一次的数字。123456# 一行流氓解法class Solution: # 返回[a,b] 其中ab是出现一次的两个数字 def FindNumsAppearOnce(self, array): # write code here return [x for x in array if array.count(x)==1] if array else [] 正规解法:两次遍历，存查字典。123456789101112131415def FindNumsAppearOnce(self, array): # write code here if not array return [] result = [] numDict = &#123;&#125; for num in array: if num in numDict: numDict[num] += 1 else: numDict[num] = 1 for num,count in numDict.items(): if numDict[num] == 1: result += [num] return result 和为S的连续正数序列输入一个数S，输出所有和为S的连续正数序列。序列内按照从小至大的顺序，序列间按照开始数字从小到大的顺序 123456789101112131415161718class Solution: def FindContinuousSequence(self, tsum): # write code here result = [] if tsum &lt;3: return [] small = 1 big = 2 while small &lt; (tsum + 1)/2 and big &lt; tsum: nowList = [x for x in range(small,big+1)] if sum(nowList) == tsum: result.append(nowList) big += 1 elif sum(nowList) &lt; tsum: big += 1 else: small += 1 return result 思路： small和big两个数，因为是正数，所以初始化为1和2，如果从small到big的序列和大于sum，那么就去掉前面的值，small+=1,如果小于sum，那么就在后面加上一个值，big+=1 循环条件是 small &lt; (tSum+1)/2 并且 big &lt; tSum否则结束，因为比如100,到50的时候，50,51和后面的数的和必定不满足条件了。 和为S的两个数字输入一个递增排序的数组和一个数字S，在数组中查找两个数，是的他们的和正好是S，如果有多对数字的和等于S，输出两个数的乘积最小的。对应每个测试案例，输出两个数，小的先输出。我自己的基于twoSum的做法，遍历一遍即可12345678910111213141516171819def FindNumbersWithSum(self, array, tsum): # write code here array = set(array) sumDict = &#123;&#125; result = [] for num in array: if num in sumDict: result.append([num, sumDict[num]]) else: sumDict[tsum-num] = num dd = float("inf") final = None if not result: return [] for item in result: if item[0]*item[1] &lt; dd: dd = item[0]*item[1] final = item return min(final[0],final[1]),max(final[0],final[1]) 主流做法：左右夹逼，这样做的好处就是，因为距离越远的两个数乘积越小，这样夹逼找到的第一组数肯定是最优解，这也应该是这个问题的最优做法，比我的做法要好12345678910111213def FindNumbersWithSum(self, array, tsum): # write code here left = 0 right = len(array)-1 while left &lt;= right: nowSum = array[left] + array[right] if nowSum == tsum: return array[left],array[right] elif nowSum &lt; tsum: left += 1 elif nowSum &gt; tsum: right -= 1 return [] 但是看运行时间我的做法更快？ 左旋转字符汇编语言中有一种移位指令叫做循环左移（ROL），现在有个简单的任务，就是用字符串模拟这个指令的运算结果。对于一个给定的字符序列S，请你把其循环左移K位后的序列输出。例如，字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。是不是很简单？1234class Solution: def LeftRotateString(self, s, n): # write code here return s[n:]+s[:n] if n &lt;= s else None 这个东西对python来说可能不算一道题把…. 翻转单词顺序列例如，“student. a am I”。后来才意识到，这家伙原来把句子单词的顺序翻转了，正确的句子应该是“I am a student.”。Cat对一一的翻转这些单词顺序可不在行，你能帮助他么？1234class Solution: def ReverseSentence(self, s): # write code here return " ".join(s.split(" ")[::-1]) python只需要一行代码就够了，” “.join指的是用空格作为分隔符，后面的list中的元素必须为字符串，思路就是先分割，再倒序，再用空格做分隔符连起来ps:print(reversed(“hello”)) 应该为 print(‘’.join(reversed(“hello”)))常规做法先翻转整个句子，然后，依次翻转每个单词。依据空格来确定单词的起始和终止位置 扑克牌顺子题目说的乱七八糟的，意思是就给5个数，从0-13，0可以变成任何数，问这五个数可不可以变成一个连续序列。1234567891011121314151617181920212223242526class Solution: def IsContinuous(self, numbers): # write code here if not numbers: return [] numbers = sorted(numbers) zerocount = 0 gapcount = 0 usedNum = [] for i in range(len(numbers)): if numbers[i]==0: zerocount+=1 else: # 先判断是否重复 if numbers[i] in usedNum: return False else: usedNum.append(numbers[i]) # 累加gapcount if i &lt; len(numbers)-1: gapcount+=numbers[i+1]-numbers[i]-1 # 判断所有的大小王是否够来填补空缺 if zerocount &gt;= gapcount: return True else: return False 思路：主要思路有两点： 非0数字不能重复 先排序，然后看看非0数字之间的间隔有几个坑，这些坑必须用0来填，然后看看0的个数够不够填这些坑即可。 圆圈中最后剩下的数有名的约瑟夫环问题，有两种解法1.第一种就是利用循环列表来求解，删除一个数字需要m步，一共有n个数字，时间复杂度为O(mn)，同时需要一个链表来辅助，空间复杂度为O(n)，代码如下123456789101112131415161718192021222324252627class linkedList: def __init__(self,x): self.val = x self.next = Noneclass Solution: def LastRemaining_Solution(self,n, m): # write code here if n==0: return -1 head = linkedList(0) pre = head for i in range(1,n): newNode = linkedList(i) pre.next = newNode pre = pre.next pre.next = head # 构造好循环链表 # 一共有n个数，需要循环n-1次才能剩下一个数 for i in range(n-1): temp = head # 每次删除第m个数字，也就是前进m-1步删掉当前节点，也就是前进m-2步删掉后面的节点 for i in range(m-2): temp = temp.next temp.next = temp.next.next # 然后下次循环从被删掉的节点后面的那个节点开始 head = temp.next return head.val 第二种解法，数学归纳法循环实现方法1234567891011class Solution: def LastRemaining_Solution(self,n, m): # write code here if n==0: return -1 f = 0 i = 2 while i &lt;= n: f = (f+m)%i i+=1 return f 递归实现方法12345678class Solution: def LastRemaining_Solution(self,n, m): # write code here if n==0: return -1 if n==1: return 0 return (self.LastRemaining_Solution(n-1,m)+m)%n 关于约瑟夫环的问题一定要牢牢记住递归公式： $$f(N,M)=(f(N-1,M)+M)\%N$$求1+2+3+….+n求1+2+3+…+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。1234class Solution: def Sum_Solution(self, n): # write code here return self.Sum_Solution(n-1)+n if n!=1 else 1 思路： 一个简单的递归:(1+2+3….+n-1) + n 把字符串转换成整数将一个字符串转换成一个整数，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0输入： “+2147483647” 输出 2147483647输入：“1a33” 输出：0 1234567891011121314151617181920212223242526272829# -*- coding:utf-8 -*-class Solution: def StrToInt(self, s): # write code here if not s: return 0 start = 0 flag = 1 result = [] final = 0 # 第一位允许有符号位 if s[0] == '-': flag = -1 start =1 elif s[0] =='+': start = 1 # 从start开始就不准有其他符号了 for c in s[start:]: if not c.isdigit(): return 0 result.append(int(c)) # 没有数字返回0 if not result: return 0 length = len(result) # 按所在位数乘以10的k次方 for i in range(length): final += result[i] * ( 10 ** (length - i -1) ) return final*flag 思路：允许的正确格式为 一位符号位[可以无]+只剩下连续的数字字符，违反格式就错。 不用加减乘除做加法感觉没太大意义，用位运算，如何用python来写？ 数组中重复的数字在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。12345678910111213# -*- coding:utf-8 -*-class Solution: # 这里要特别注意~找到任意重复的一个值并赋值到duplication[0] # 函数返回True/False def duplicate(self, numbers, duplication): # write code here numbers = sorted(numbers) for i in range(len(numbers)): if i != numbers[i]: if numbers[numbers[i]] == numbers[i]: duplication[0] = numbers[i] return True return False 思路：因为所有的数都是0到n-1，所以可以先排好序，如果完全没重复的数字，那么下标和值一一对应，如果出现值和下标不一样了，看看这个值作为下标对应的值是否和当前值一样，如果是的话就返回这个重复的数字，而且肯定是第一个。比如[0,1,2,3,4,4,5]前面都是一一对应，到下标为为5的时候，值为4，看下标为4的值，果然等于4，所以重复，返回。 构建乘积数组给定一个数组A[0,1,…,n-1],请构建一个数组B[0,1,…,n-1],其中B中的元素B[i]=A[0]A[1]…A[i-1]A[i+1]…A[n-1]。不能使用除法。 1234567891011121314151617181920# -*- coding:utf-8 -*-class Solution: def multiply(self, A): # write code here B = [] if not A: return [] C,D = [0 for x in range(len(A))],[0 for x in range(len(A))] # 这里只遍历了一次就存满了两个数组，注意这里的操作 for i in range(len(A)): if i == 0: C[i] = 1 D[i] = 1 else: C[i] = C[i-1]*A[i-1] D[i] = D[i-1]*A[-i] for i in range(len(A)): # C[i]从头乘,D[i]从尾乘，注意下标 B.append(C[i]*D[-(i+1)]) return B 思路：利用矩阵，图见剑指offer第313页。 正则表达式匹配请实现一个函数用来匹配包括’.’和’‘的正则表达式。模式中的字符’.’表示任意一个字符，而’‘表示它前面的字符可以出现任意次（包含0次）。 在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串”aaa”与模式”a.a”和”abaca”匹配，但是与”aa.a”和”ab*a”均不匹配1234567891011121314151617181920212223242526272829303132333435363738394041424344# -*- coding:utf-8 -*-class Solution: # s, pattern都是字符串 def match(self, s, pattern): # write code here if not s and not pattern: return True return self.matchPattern(s, pattern) def matchPattern(self, s, pattern): print s,pattern # pattern or s 为空 if not pattern and not s: return True if not s and pattern: if len(pattern) &gt;=2 and pattern[1]=='*': return self.matchPattern(s,pattern[2:]) else: return False if not pattern and s: return False # pattern &lt; 1 if len(pattern) == 1: if pattern[0] == '.': return self.matchPattern(s[1:],pattern[1:]) elif pattern[0] != s[0]: return False elif pattern[0]==s[0]: return self.matchPattern(s[1:],pattern[1:]) # (1) 模式第二个字符不为* if pattern[1] != '*': # 如果第一个字符相等，继续匹配 if pattern[0] == '.' or pattern[0] == s[0]: return self.matchPattern(s[1:],pattern[1:]) else: return False if pattern[1]=='*': # (2) 模式第二个字符为* # 如果第一个字符不匹配，后移两个继续匹配 if pattern[0]!='.' and pattern[0]!=s[0]: return self.matchPattern(s,pattern[2:]) # 如果第一个字符匹配 (.也算？) 有三种匹配方式 or or if pattern[0]=='.' or pattern[0] == s[0]: return self.matchPattern(s,pattern[2:]) or self.matchPattern(s[1:],pattern[2:]) or self.matchPattern(s[1:],pattern) 思路：其实直接把原来的函数写成递归行了，这里没改。这个题就是大量的if-else判断，分情况，我在做的时候分成了以下的几种情况，递归做法：1.判断pattern或s或两者都为空的情况作为结束条件2.判断pattern长度为1的情况(因为后面有个&gt;1的情况需要详细讨论)3.判断pattern长度&gt;=2的情况 3.1 pattern第二个字符不为”“的情况 3.1.1 如果第一个字符相等，s和pattern各自向后移一位，继续递归(注意”.”的情况) 3.1.2 如果第一个字符不相等，直接返回False 3.2 pattern第二个字符为’‘的情况 3.2.1 如果第一个字符不相等，s往后移一位，pattern移两位，相当于跳过这个X* 3.2.2 如果第一个字符相等，可以有以下三种处理方法继续递归： (1) s 移一位， pattern不变 (2) s 不移， pattern 移两位 (3) s 移一位 ,pateern 移两位 表示数值的字符串请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串”+100”,”5e2”,”-123”,”3.1416”和”-1E-16”都表示数值。 但是”12e”,”1a3.14”,”1.2.3”,”+-5”和”12e+4.3”都不是。1234567891011121314151617181920212223242526272829303132333435363738394041# -*- coding:utf-8 -*-class Solution: # s字符串 def isNumeric(self, s): # write code here dotCount = 0 digitCount = 0 for i in range(len(s)): print s[i] if s[i]=='+' or s[i]=='-': if i!=0 and (s[i-1]!='e' and s[i-1]!='E'): return False else: continue elif s[i]=='.': if dotCount: return False dotCount+=1 continue elif s[i]=='e' or s[i]=='E': if i == 0 or i==len(s)-1: return False elif not s[i-1].isdigit(): return False i+=1 if s[i]=='+' or s[i]=='-': i+=1 digitCount+=1 while i&lt;len(s)-1: if not s[i].isdigit(): return False i+=1 digitCount+=1 break elif s[i].isdigit(): digitCount+=1 continue else: return False return True if digitCount else False 思路：就是根据规则写if-else，感觉意义不是很大，规则如下：循环从头遍历一遍： ‘+’or ‘-:只能在开头第一个或者前面必须有E(e) ‘.’只能有一个 ‘E’只能有一个，必须不能在开头结尾，后面可以接一个+(-)，然后后序的必须全是连续整数 ‘x’其他符号，直接返回False 字符流中第一个不重复的字符请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符”go”时，第一个只出现一次的字符是”g”。当从该字符流中读出前六个字符“google”时，第一个只出现一次的字符是”l”。12345678910111213141516171819# coding=utf-8class Solution: s= '' cDict = &#123;&#125; # 返回对应char def FirstAppearingOnce(self): # write code here for c in self.s: if c in self.cDict and self.cDict[c] ==1: return c return '#' def Insert(self, char): # write code here self.s = self.s+char for c in char: if c in self.cDict: self.cDict[c] += 1 else: self.cDict[c] = 1 思路：遍历两边，每次的时间都是O(n)*O(1),第一遍遍历来存每个字符的次数，第二遍遍历找第一个出现为1次的字符注意第二遍遍历的不是字典，因为字典不是按照存储顺序排列的，所以遍历的是s 链表中环的入口点一个链表中包含环，请找出该链表的环的入口结点。123456789101112131415161718192021# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def EntryNodeOfLoop(self, pHead): # write code here linkAdd = set() pre = pHead if not pHead: return None while pre: if not pre: return None if id(pre) in linkAdd: return pre else: linkAdd.add(id(pre)) pre = pre.next return None 思路：很简单的一道题，循环遍历往下找，一遍把地址存HashTable中(这里用的set)，如果发现自己的地址被存过，那么这个节点就是入口点。 删除链表中重复的结点在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;51234567891011121314151617181920212223242526272829303132333435363738394041# -*- coding:utf-8 -*-class ListNode: def __init__(self, x): self.val = x self.next = Noneclass Solution: def deleteDuplication(self, pHead): # write code here if not pHead or (not pHead.next): return pHead while pHead and pHead.next and (pHead.val == pHead.next.val): pHead = self.getNewpHead(pHead) if not pHead: return None pre = pHead while pre.next: if not pre.next.next: break if pre.next.val == pre.next.next.val: # 消除重复数字 tempNum = pre.next.val while pre.next and pre.next.val == tempNum: pre.next = pre.next.next # 如果是消除重复数字结束，那么不用往后移 continue pre = pre.next return pHead def getNewpHead(self, pHead): pre = pHead # 处理头结点 if pre.val == pre.next.val: temp = pre.val while pre and (pre.val == temp): pre = pre.next # 如果直接到头了，返回None if not pre: return None pHead = pre return pHead 思路：首先要把头结点拿出单独处理，头结点一定是不能重复的，做完去重操作以后，得到新的头结点，然后跳过重复的节点往后指，然后主要注意的问题有 边界问题，时刻要注意是否对一个None引用了val或next等方法 or 和 and 的先后问题，先决条件要放到前面，编译器是按照先后顺序来看的 头结点处理完以后要判断是否直接结束了(全都是同样的值) 再就是消除链表除头结点意外的操作的时候的下标边界和循环逻辑等问题了。 二叉树的下一个结点给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。12345678910111213141516171819202122232425262728293031# -*- coding:utf-8 -*-# class TreeLinkNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = None# self.next = Noneclass Solution: def GetNext(self, pNode): # write code here # 如果有右子树 if pNode.right: node = pNode.right while node.left: node = node.left return node # 如果是跟节点 if pNode.next == None: return None # 如果没右子树，自己是父节点的左节点 if pNode.next.left == pNode: return pNode.next # 如果没右子树，自己是父节点的右节点 if pNode.next.right == pNode: node = pNode.next while node == node.next.right: node = node.next # 如果一直追溯到根节点，那么说明遍历完了，返回None if node.next == None: return None return node.next 思路：中序遍历的特点是先遍历完左子树所有部分，再遍历本身节点，再遍历右子树所有部分。给出当前节点(已经遍历完)，求下个节点分如下几种情况：1.有右子树，那么下个节点肯定在右子树里，找的方法也很简单，从该节点的右节点开始，如果有左子树就一直延伸，直到没有左子树为止。2.没有右子树(1) 是跟节点，直接返回None，已经结束了(2) 是父节点的左子节点，那么下个点肯定是父节点(3) 是父节点的右子节点，那对这个节点的父节点来说，它作为根节点的这颗子树已经遍历完了，往回再找它的父节点，如果它的父节点也是它的父节点的父节点的右节点，那么同理说明那颗子树也遍历完了，直到往回找到一个节点它是它父节点的左节点的时候，诶！这个时候它的父节点就是下一个节点啦，那如果一直都到根节点都找不到这样的节点，说明遍历已经结束了，不存在下个节点 对称的二叉树请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。第一种方法，任何一种遍历顺序，调换left和right的位置，如果序列依然相同，则对称。123456789101112131415161718192021222324252627class Solution: preSort =[] afterSort = [] def isSymmetrical(self, pRoot): # write code here if not pRoot: return False pre = pRoot self.preSortBT(pre) pre = pRoot self.afterSortBT(pre) for i in range(len(self.preSort)): if self.preSort[i]!=self.afterSort[i]: return False return True def preSortBT(self,pRoot): if pRoot.left: self.preSortBT(pRoot.left) self.preSort.append(pRoot.val) if pRoot.right: self.preSortBT(pRoot.right) def afterSortBT(self,pRoot): if pRoot.right: self.afterSortBT(pRoot.right) self.afterSort.append(pRoot.val) if pRoot.left: self.afterSortBT(pRoot.left) 第二种方法：递归，每次判断对称的两个点的对称的两组点是否对称，依次递归，二刷写代码1# TODO 按之字形顺序打印二叉树请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。1234567891011121314151617181920212223242526272829303132333435363738# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 需要两个栈来实现，当前打印是奇数层的时候，先左后右入栈，当前是偶数层的时候，先右后左入栈 stack1 = [] stack2 = [] def Print(self, pRoot): # write code here if not pRoot: return [] self.stack1.append(pRoot) result = [] while self.stack1 or self.stack2: tempResult = [] while self.stack1: nowNode = self.stack1.pop() tempResult.append(nowNode.val) if nowNode.left: self.stack2.append(nowNode.left) if nowNode.right: self.stack2.append(nowNode.right) if tempResult: result.append(tempResult) tempResult = [] while self.stack2: nowNode = self.stack2.pop() tempResult.append(nowNode.val) if nowNode.right: self.stack1.append(nowNode.right) if nowNode.left: self.stack1.append(nowNode.left) if tempResult: result.append(tempResult) return result 思路：按照剑指思路来的，利用两个栈来实现，引用一下一篇博客里的说法：以题目中的二叉树为例，一步步分析。 当二叉树的根结点（结点1）打印之后，它的左子结点（结点2）和右子结点（结点3）先后保存到一个容器中。值得注意的是，在打印第二层的结点时，先打印结点3，后打印结点2。由此可见结点在这个容器中是后进先出的，因此该容器可以用栈来实现。 接着打印第二层的两个节点。根据题目定义，先打印结点3，再打印结点2，并把他们的子结点放入一个容器中。注意到，打印第三层时，先打印结点2的两个结点，后打印结点3的两个结点。这意味着，我们还可以用一个栈来保存结点2和结点3的子结点。 此外我们还注意到，第三层的结点是从左向右打印的。按照栈后进先出的特点，应该先保存7到栈中，再保存结点6，之后再分别保存结点5和结点4。也就是说，在打印第二层的时候，我们先保存右子结点到栈中，再保存左子结点到栈中。保存子结点的顺序和打印第一层时不一样。 接下来打印第三层。与之前一样，在打印第三层的同时，我们要把第四层的结点保存到一个栈中。由于第四层的打印顺序是从右到左，因此保存的顺序是从左到右。这和保存根结点的两个子结点的顺序是一样的。 把二叉树打印成多行(按层打印二叉树)从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。1234567891011121314151617181920212223242526272829303132333435363738# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回二维列表[[1,2],[4,5]] quene1 = [] quene2 = [] def Print(self, pRoot): # write code here if not pRoot: return [] self.quene1 = [pRoot] result = [] while self.quene1 or self.quene2: tempResult = [] while self.quene1: nowNode = self.quene1.pop(0) tempResult.append(nowNode.val) if nowNode.left: self.quene2.append(nowNode.left) if nowNode.right: self.quene2.append(nowNode.right) if tempResult: result.append(tempResult) tempResult = [] while self.quene2: nowNode = self.quene2.pop(0) tempResult.append(nowNode.val) if nowNode.left: self.quene1.append(nowNode.left) if nowNode.right: self.quene1.append(nowNode.right) if tempResult: result.append(tempResult) return result 思路：因为这道题跟上面那个非常像，稍微改一下就A了，很舒服。之字形式倒着打么，就用两个栈，那这个先进先打就换成队列就OK了。 二叉搜索树与双向链表输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。123456789101112131415161718192021222324252627282930313233# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: head = None pre = None def Convert(self, pRootOfTree): # write code here if not pRootOfTree: return None def trans(root): if not root: return None templeft = root.left tempright = root.right if root.left: self.Convert(templeft) # 如果是头结点 if not self.head: self.head = root self.pre = root else: # 因为是中序遍历的，所以当前root肯定是pre的下一个节点 root.left = self.pre self.pre.right = root self.pre = root if root.right: self.Convert(tempright) trans(pRootOfTree) return self.head 思路：pre存中序遍历的上一个节点，root是当前节点，递归处理左右子树，根节点不处理。]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017-JDD京东金融算法大赛15th解决方案]]></title>
    <url>%2F2017%2F12%2F30%2FJDD%2F</url>
    <content type="text"><![CDATA[JDD-2017JDD-2017 京东金融大数据竞赛-销量预测-15th解决方案 比赛说明通过竞赛数据中店铺过往的销售记录，商品信息，商品评价，以及广告费用等信息来建立预测模型，预测店铺未来90天内的销售额。训练数据包含2017-04-30日之前270天之内若干店铺的每日订单量、销售额、顾客数、评价数、广告费用等数据，下架时间在2017-04-30之后或者未下架的商品数据，以及这些店铺2016年6月-2017年1月每月末后90天内的销售额。 数据处理特殊的数据主要包含以下几部分 活动促销：比如双11和618 特殊月份：比如过年前后 店铺刷单：有些店铺平时销量很低，会有几天莫名其妙的很高 下降商品：某些店铺会在短时间内下架大量的商品 数据集划分我们试过非常多的方案，最终选择了使用一个月作为训练区间，该月的前三个月作为特征提取区间 特征商品特征 在售总商品数 平均每个商品的订单量 平均每个商品的实际销量 平均每个商品的退货订单数 未售商品占总商品的比例 订单特征 总销售金额 平均每个订单销售金额 总优惠金额 平均每笔订单优惠金额 总优惠金额占总销售金额比 总订单量 总退货订单量 总实际订单量 总退货订单金额 退货金额占总销售额金额的比 总实际销售金额 平均每笔订单实际销售金额 总顾客数 平均每个顾客的订单量 平均每个顾客的购买金额 平均每个顾客的退货订单数 平均每个顾客的退货金额 总优惠笔数 总优惠金额占总退货金额比例 平均每笔订单总优惠金额占总退货金额比例 平均每笔优惠金额 平均销售金额增长率（每个月和前一个月算增长率，所有增长率取平均） 平均订单量增长率 平均退货订单增长率 平均退货金额增长率 评价特征（取平均的时候是按照有效评论日期取平均） 总好评数 总中评数 总差评数 总评论数 平均好评数 平均中评数 平均差评数 好评率 差评率 中评率 平均好评率增长率 交叉特征 平均每个月的充值广告费用占总销售金额比 平均每个月的充值广告费用占实际销售金额比 平均每个订单的好评率 平均每个订单的差评率 平均每个订单的好评数 平均每个订单的差评数 销售额和下架特征 总销售额 当月销售额 前一个月总销售金额 前两个月总销售金额 前三个月总销售金额 前一个周总销售金额 前两个周总销售金额 前三个周总销售金额 前一个月下架商品数 前两个月下架商品数 前三个月下架商品数 最近一周下架商品数 一开始加的特征比较多，因为效果还不错吧，所以也没有根据线上去判断一下哪些特征是否有用，而且这个比赛想构建一个比较稳定的线下验证是非常困难的，因为销量波动还是比较大的，所以更多的时候是以线上来验证我的一些想法。根据特征的重要性来看，跟销售金额有关的特征比较强一些，某些特征重要性非常低但是我也没删除。 后处理因为某些店铺的销量波动实在是太大了，所以除了预测销量的模型之外，我还尝试构建了一个二分类模型，主要来区别销量比较平稳和销量波动很大的商铺，所以最终的模型为：]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>比赛</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode]]></title>
    <url>%2F2017%2F12%2F28%2Fleetcode%2F</url>
    <content type="text"><![CDATA[(Array)Two sum**Q:Given an array of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice.** Example:1234Given nums = [2, 7, 11, 15], target = 9,Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1]. 给定一个数组和一个特定的数字，找出这个数组中两个和为该数字的index.（假定只有一组解） Sample solution:123456789def twoSum(self, nums, target): if len(nums) &lt;= 1: return False buff_dict = &#123;&#125; for i in range(len(nums)): if nums[i] in buff_dict: return [buff_dict[nums[i]], i] else: buff_dict[target - nums[i]] = i 思路：建立一个字典并遍历数组，存入键[和-加数1] = [加数1.index],遍历寻找加数2使得(和-加数1=加数2)，输出[加数1.index,加数2.index] (Array)Reverse IntegerQ:Given a 32-bit signed integer, reverse digits of an integer.Example 1:12Input: 123Output: 321 Example 2:12Input: -123Output: -321 Example 3:12Input: 120Output: 21 Note:Assume we are dealing with an environment which could only hold integers within the 32-bit signed integer range. For the purpose of this problem, assume that your function returns 0 when the reversed integer overflows. My Answer:12345678910111213141516171819class Solution: def reverse(self, nums): if len(str(nums))&gt;0 and nums != 0 and nums &lt;= math.pow(2,31): nums = str(nums) newStr = '' # len(nums)-1 to -1 , -1 not included , interval is -1 for i in range(len(nums)-1,-1,-1): newStr = newStr + nums[i] print(newStr) while newStr[0] == '0': newStr= newStr[1:] if newStr[-1]=='-': newStr='-'+newStr[:len(newStr)-1] if math.fabs(int(newStr)) &gt;= math.pow(2,31): return 0 else: return(int(newStr)) else: return 0 思路：主要注意几点1.32位有符号整数范围判断2.逆序以后0的处理3.切片时注意索引是否会超范围，也就是只输入0的情况4.对于是否为负的处理 (Array)Palindrome Number判断是否是回文数字注意1.负数不为回文数2.不能转为字符串，因为不能使用额外空间，这样空间复杂度将为线性 Mysolution:123456789101112131415class Solution: def isPalindrome(self, nums): if nums &lt; 0: return False if nums&lt; 10: return True else: length = 1 while math.pow(10,length) &lt;= nums: length+=1 for i in range(math.ceil(length/2)): status = math.isclose((math.floor(nums/math.pow(10,i))%10),(math.floor(nums/math.pow(10,length-i-1))%10),abs_tol = 0.5) if status == False: return status return True Hint: 先获取整数的长度，然后依次比较左右两端的数字，我在这里用的math.isclose并指定了绝对距离，因为对python中的精度损失理解不深，先这样模糊处理了。 (Array)Non-decreasing Array非下降数组Q:Given an array with n integers, your task is to check if it could become non-decreasing by modifying at most 1 element. We define an array is non-decreasing if array[i] &lt;= array[i + 1] holds for every i (1 &lt;= i &lt; n).Example 1:1234567Input: [4,2,3]Output: TrueExplanation: You could modify the first 4 to 1 to get a non-decreasing array. 123Input: [4,2,1]Output: FalseExplanation: You can't get a non-decreasing array by modify at most one element. Note:The n belongs to [1, 10,000]. 题意:给定一个数组，判断只修改一个数字（或不修改）是否可以让数组变为非下降数组。 Mysolution: 123456789101112class Solution: def checkPossibility(self, nums): num1 = nums.copy() num2 = nums.copy() if len(nums)==1: return True for i in range(len(nums)-1): if nums[i]&gt;nums[i+1]: num1[i] = num1[i+1] num2[i+1] = num2[i] break return num1==sorted(num1) or num2==sorted(num2) 借鉴了Discuss里 -Yangshun的思路:First, find a pair where the order is wrong. Then there are two possibilities, either the first in the pair can be modified or the second can be modified to create a valid sequence. We simply modify both of them and check for validity of the modified arrays by comparing with the array after sorting. (Array)k-diff Pairs in an ArrayQ:Given an array of integers and an integer k, you need to find the number of unique k-diff pairs in the array. Here a k-diff pair is defined as an integer pair (i, j), where i and j are both numbers in the array and their absolute difference is k.Example 1:1234Input: [3, 1, 4, 1, 5], k = 2Output: 2Explanation: There are two 2-diff pairs in the array, (1, 3) and (3, 5).Although we have two 1s in the input, we should only return the number of unique pairs. Example 2:123Input:[1, 2, 3, 4, 5], k = 1Output: 4Explanation: There are four 1-diff pairs in the array, (1, 2), (2, 3), (3, 4) and (4, 5). Example 3:123Input: [1, 3, 1, 5, 4], k = 0Output: 1Explanation: There is one 0-diff pair in the array, (1, 1). 注意1.(1,3),(3,1)属于同一个数值对2.k为负的时候返回03.注意处理k=0的情况4.不要使用循环嵌套，必超时 Mysolution 1234567# 精简版class Solution: def findPairs(self, nums, k): if k&lt;0:return 0 #if k==0:return len(set(list(filter(lambda x:nums.count(x)&gt;1,nums)))) if k==0:return sum(v&gt;1 for v in collections.Counter(nums).values()) return len(set(sorted(nums)) &amp; set(map(lambda x:x+k,nums))) 12345678910# 这样写会好理解一些class Solution: def findPairs(self, nums, k): if k&lt;0:return 0 if k==0:return sum(v&gt;1 for v in collections.Counter(nums).values()) #nums = list(filter(lambda x:nums.count(x)&gt;1,nums)) #return len(set(nums)) nums = set(sorted(nums)) nums1 = set(map(lambda x:x+k,nums)) return len(nums &amp; nums1) 思路：将数组中每个数+k后与原数组取交集 (Hash Table)Count PrimesDescription:Count the number of prime numbers less than a non-negative number, n.计算小于正整数n的素数的个数 1234567891011121314151617181920def countPrimes(self, n): """ :type n: int :rtype: int """ if n &lt; 3: return 0 # 一个长度为n的，值为True的list primes = [True] * n # 前两位置为False 0和1 都不是素数，最小的素数是2 primes[0] = primes[1] = False for i in range(2, int(n ** 0.5) + 1): # 原始数组是全置为素数的，每一次的置False操作都会保证下一个Ture的数为质数 # 因为如果不是素数的话，那么肯定会被它小的数整数，而这些比它小的数已经做过倍数置False处理了 if primes[i]: # 从i平方 到 n ，步长为i 都置为false，因为i从2开始，是i的倍数的整数肯定不是素数 # 这里从 i * i 开始置False是因为i*1,i*2...i*i-1已经被以前的数计算过了，比如i=2的时候，会计算2*i,所以不必计算i*2了 primes[i * i: n: i] = [False] * len(primes[i * i: n: i]) # 返回剩余的True的个数 return sum(primes) 这个算法实现的是埃拉托斯特尼筛法：算法的核心思想是：要得到自然数n以内的全部素数，必须把不大于 的所有素数的倍数剔除，剩下的就是素数。（代码的实现注释的已经很清楚了） (Array-Medium)Insert Delete GetRandom O(1)Description:Design a data structure that supports all following operations in average O(1) time. 1.insert(val) : Inserts an item val to the set if not already present. 2.remove(val) : Removes an item val from the set if present. 3.getRandom : Returns a random element from current set of elements. Each element must have the same probability of being returned. Example：1234567891011121314151617181920212223// Init an empty set.RandomizedSet randomSet = new RandomizedSet();// Inserts 1 to the set. Returns true as 1 was inserted successfully.randomSet.insert(1);// Returns false as 2 does not exist in the set.randomSet.remove(2);// Inserts 2 to the set, returns true. Set now contains [1,2].randomSet.insert(2);// getRandom should return either 1 or 2 randomly.randomSet.getRandom();// Removes 1 from the set, returns true. Set now contains [2].randomSet.remove(1);// 2 was already in the set, so return false.randomSet.insert(2);// Since 2 is the only number in the set, getRandom always return 2.randomSet.getRandom(); My Solution*12345678910111213141516171819202122232425262728293031323334353637from random import choiceclass RandomizedSet: def __init__(self): """ Initialize your data structure here. """ self.Rset,self.Rlist = set(),[] def insert(self, val): """ Inserts a value to the set. Returns true if the set did not already contain the specified element. :type val: int :rtype: bool """ if val in self.Rset: return False else: self.Rset.add(val) self.Rlist.append(val) return True def remove(self, val): """ Removes a value from the set. Returns true if the set contained the specified element. :type val: int :rtype: bool """ if val in self.Rset: self.Rset.remove(val) self.Rlist.remove(val) return True else: return False def getRandom(self): """ Get a random element from the set. :rtype: int """ return choice(self.Rlist) 思路:用到了集合这个数据结构，只需要注意一点就是集合是非irerative的，所以引入了一个list来用choice函数返回随机元素再添加一个discuss里用字典和list来实现的代码，更快一些：1234567891011121314151617181920212223class RandomizedSet(object): def __init__(self): self.nums, self.pos = [], &#123;&#125; def insert(self, val): if val not in self.pos: self.nums.append(val) self.pos[val] = len(self.nums) - 1 return True return False def remove(self, val): if val in self.pos: idx, last = self.pos[val], self.nums[-1] self.nums[idx], self.pos[last] = last, idx self.nums.pop(); self.pos.pop(val, 0) return True return False def getRandom(self): return self.nums[random.randint(0, len(self.nums) - 1)] (Array-medium)4Sum(拓展为Nsum问题)DescriptionGiven an array S of n integers, are there elements a, b, c, and d in S such that a + b + c + d = target? Find all unique quadruplets in the array which gives the sum of target.Note: The solution set must not contain duplicate quadruplets.12345678For example, given array S = [1, 0, -1, 0, -2, 2], and target = 0.A solution set is:[ [-1, 0, 0, 1], [-2, -1, 1, 2], [-2, 0, 0, 2]] Solution借鉴discuss的思路，很不错1234567891011121314151617181920212223242526272829303132333435class Solution(object): def fourSum(self, nums, target): def Nsum(N,nums,target,result,results): # 特殊情况 if N&gt;len(nums) or target&gt;N*nums[-1] or target&lt;N*nums[0]: return # 高效的2Sum if N == 2: l,r = 0,len(nums)-1 while l &lt; r: if nums[l] + nums[r] == target: # 补充到最终结果 results.append(result + [nums[l], nums[r]]) # 左指针右移，同时右指针左移，因为已经排好序了，一个值不变另一个值变一定不会得到target l += 1 r -= 1 # 解决重复问题 while l &lt; r and nums[l] == nums[l - 1]: l += 1 while r &gt; l and nums[r] == nums[r + 1]: r -= 1 elif nums[l] + nums[r] &lt; target: l += 1 else: r -= 1 else: # 注意这里的范围，是到len(nums)-N+1,因为最后N个数为最后一组，无须再进行递归 for i in range(len(nums)-N+1): # 结合sorted可解决重复问题 if i==0 or (i&gt;0 and nums[i-1]!=nums[i]): # 递归，将nSum问题降为n-1Sum问题 Nsum(N-1,nums[i+1:],target-nums[i],result+[nums[i]],results) results = [] Nsum(4,sorted(nums),target,[],results) return results 思路：给定一个list和一个target，求所有n个list中的数和为target的不重复组合。本题是4Sum，代码拓展为了nSum问题，主要思路是通过迭代来把问题转换为高效的2Sum问题，即nSum-&gt;n-1Sum-&gt;….-&gt;4Sum-&gt;3Sum-&gt;2Sum,具体细节在代码注释中。 (Hash-Table)Repeated DNA SequencesDescriptionAll DNA is composed of a series of nucleotides abbreviated as A, C, G, and T, for example: “ACGAATTCCG”. When studying DNA, it is sometimes useful to identify repeated sequences within the DNA.Write a function to find all the 10-letter-long sequences (substrings) that occur more than once in a DNA molecule.For example,1234Given s = "AAAAACCCCCAAAAACCCCCCAAAAAGGGTTT",Return:["AAAAACCCCC", "CCCCCAAAAA"]. 就是求一个字符串里的所有指定长度的重复子字符串。Mysolutionclass Solution(object): def findRepeatedDnaSequences(self, s): “”” :type s: str :rtype: List[str] “”” if len(s)&lt;10: return [] results = [] DNAdict = {} for i in range(len(s)-9): if s[i:i+10] in DNAdict: DNAdict[s[i:i+10]] +=1 else: DNAdict[s[i:i+10]] =0 for key,value in DNAdict.items(): if value&gt;0: results.append(key) return results1234567891011121314151617**思路** 循环切片判断是否重复，数据结构用的字典，注意的就是range的范围和切片的范围问题。将10替换为N可以变为求一个字符串中长度为N的重复子串。---# Longest Palindromic Substring经典的最大回文子字符串问题样例：``` pythonInput: &quot;babad&quot;Output: &quot;bab&quot;Note: &quot;aba&quot; is also a valid answer.Input: &quot;cbbd&quot;Output: &quot;bb&quot; 思路：从左到右依次把每个字符当做中心，然后依次判断两边的字符是否相等，最后取最大的那个就行了，唯一要注意的一点是连续字符的情况，比如 abccbd,这个时候要把cc整体作为一个字符串，这里用了两个位置变量，left和right，每次判断两边的相同字符之前要确定right的值，也就是相同字符的长度。 123456789101112131415161718192021class Solution(object): def longestPalindrome(self, s): """ :type s: str :rtype: str """ length = len(s) result = '' for i in range(length): right = 1 left = 1 # get the step length while i + right &lt;= length - 1 and s[i + right] == s[i]: right += 1 while ((i -left ) &gt;= 0 and (i + right) &lt;= length - 1 and s[i - left] == s[i + right]): left += 1 right+=1 longest = s[i - left+1:i + right] if len(longest) &gt; len(result): result = longest return result (链表)Add Two NumbersYou are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order and each of their nodes contain a single digit. Add the two numbers and return it as a linked list.You may assume the two numbers do not contain any leading zero, except the number 0 itself. Example123Input: (2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)Output: 7 -&gt; 0 -&gt; 8Explanation: 342 + 465 = 807. 思路:就是由两个链表逆序表示的整数的和，没什么思路可言，这里主要考察的是对链表的操作，具体看代码，讲的很通俗易懂。 12345678910111213141516171819202122232425262728class Solution(object): def addTwoNumbers(self, l1, l2): """ :type l1: ListNode :type l2: ListNode :rtype: ListNode """ s1 = str(l1.val) s2 = str(l2.val) while l1.next: l1 = l1.next s1 = str(l1.val)+s1 while l2.next: l2 = l2.next s2 = str(l2.val)+s2 result = str(int(s1)+int(s2)) # 头节点 head = ListNode(int(result[-1])) after = head # 这里是对数组的操作，[-2::-1]表示从倒数第二个元素向后-1的取全部，注意这里[-2:0:-1]和[-2::-1]是不同的，前者取到下标为1，因为:左右就是[)的，后者取到数组头 for c in result[-2::-1]: # 新开辟一块内存空间 node = ListNode(int(c)) # 让after.next指向这个内存空间，相当于把两个节点连接起来 after.next = node # after指向这块内存空间，方便下一个节点建立以后after代表这块内存空间去实行指向下一块新的节点的任务 after = node return head (字符串)最大字符不重复子串问题 Longest Substring Without Repeating CharactersGiven a string, find the length of the longest substring without repeating characters.Examples:Given “abcabcbb”, the answer is “abc”, which the length is 3. Given “bbbbb”, the answer is “b”, with the length of 1. Given “pwwkew”, the answer is “wke”, with the length of 3. Note that the answer must be a substring, “pwke” is a subsequence and not a substring.给定一个字符串，求其最大子串，要求子串中无重复字符 思路:一开始我使用简单的从左到右每一个字符遍历最大子串，最后一个测试用例超时了，这里discuss里给出了一个特别好的例子： 12345678910111213141516171819202122class Solution(object): def lengthOfLongestSubstring(self, s): """ :type s: str :rtype: int """ maxLength = 0 start = 0 usedChar = &#123;&#125; for i in range(len(s)): # start是一个下表，表示这一轮新的字符串的开始下标 # if s[i] in userChar 表示遍历到的当前字符曾经用过，有跟我当前新字符串重复的嫌疑 # start &lt;= userChar[s[i]]就是说我去查查你上次出现的下标，如果是在start以后，也就是说在当前新字符串你已经出现过一次了，那么这次就算重复了 if s[i] in usedChar and start &lt;= usedChar[s[i]]: # 那么这次新字符串的遍历到此结束，start更新为上次这个下标出现的后一位字符，下面会有详细解释 start = usedChar[s[i]]+1 else: maxLength = max(maxLength,i-start+1) # 每次都更新下标 usedChar[s[i]] = i return maxLength 12345678910111213141516**举个例子说明**abczkoz543q从左到右遍历：(used 误拼为 user 了)a -&gt; not in userChar ; max(0,1)=1 ; userChar[a] = 0b -&gt; not in userChar ; max(1,2)=2 ; userChar[b] = 1c -&gt; not in userChar ; max(2,3)=3 ; userChar[c] = 2z -&gt; not in userChar ; max(3,4)=4 ; userChar[z] = 3k -&gt; not in userChar ; max(4,5)=5 ; userChar[k] = 4o -&gt; not in userChar ; max(5,6)=6 ; userChar[b] = 5(到这儿为止，字符串一直是从start=0到当前下标，所以长度为6)z -&gt; in userChar and start &lt; userChar[z]在这里z在下标为3的时候出现过的，而当前字符串是从下标为0开始的，所以这个字符串到此为止了，它贡献了maxLength=6然后更新start为3+1，也就是从z后面的k作为下一个字符串的开始下标这里就要从第二个开始继续遍历了，因为没必要从b开始再遍历，因为b也肯定会遍历第一个z然后遍历到第二个z，它形成的最大子串其实是a形成的最大子串的子串，即 bczko 属于 abzko所以直接从z的下一个元素，也就是k开始作为start即可，同时这里用到了字典，也降低了时间复杂度 (分治)两个数组的中位数 Median of Two Sorted ArraysThere are two sorted arrays nums1 and nums2 of size m and n respectively.Find the median of the two sorted arrays. The overall run time complexity should be O(log (m+n)).限制时间复杂度是O(log(m+n))Example1234nums1 = [1, 3]nums2 = [2]The median is 2.0 1234nums1 = [1, 2]nums2 = [3, 4]The median is (2 + 3)/2 = 2.5 思路:非常简单的一道题目，先用+连起来，sorted一下，如果是偶数就输出中间两个的均值，如果是奇数长度就输出中间那个值。123456789101112class Solution: def findMedianSortedArrays(self, nums1, nums2): """ :type nums1: List[int] :type nums2: List[int] :rtype: float """ l = sorted(nums1+nums2) if len(l)%2==0: return (l[int(len(l)/2)] + l[int(len(l)/2-1)])/2 else: return int(l[int((len(l)-1)/2)]) 但是其实这个复杂度是O(m+n),这种做法的思想基于下面这种做法一样，而且应该花的时间更长，是最优化最差也是最简单的做法：依次遍历两个数组，每次取最小那个，直到遍历到第i个，这个i就是我们要的中位数或中间两个数，时间复杂度为O(m+n)有更好的做法：分治思路是分治常用的“割”，关键点是一条线割开两个数组，使得左边部分全部小于右边部分，这样就可以确定虚拟数组（两个数组合并排序）的前k个值了，妙啊妙啊。这是O(min(m,n))的做法(感觉不够简练)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def findMedianSortedArrays(self, nums1, nums2): if len(nums1)+len(nums2)&lt;=10: l = sorted(nums1 + nums2) if len(l) % 2 == 0: return (float(l[int(len(l) / 2)]) + float(l[int(len(l) / 2 - 1)])) / 2 else: return int(l[int((len(l) - 1) / 2)]) if len(nums1)&lt;len(nums2): long = nums2 short = nums1 else: long = nums1 short = nums2 if (len(long)+len(short))%2!=0: # 前面找k个数 k = ((len(nums1)+len(nums2))-1)/2 k1 = 0 k2 = k - 2 while ((k2 + 1 &lt;= len(long) - 1 and short[k1] &gt; long[k2 + 1]) or ( k1 + 1 &lt;= len(short) - 1 and long[k2] &gt; short[k1 + 1])): if short[k1]&gt;long[k2+1]: k1-=1 k2 = k-k1-2 else: k1+=1 k2 = k-k1-2 if k1 + 1 &lt;= len(short) - 1: return min(short[k1 + 1], long[k2 + 1]) else: return long[k2 + 1] else: k = (len(nums1)+len(nums2))/2 -1 # 割少的那个尾巴，注意下标 k1 = 0 k2 = k-2 while((k2+1&lt;= len(long)-1 and short[k1]&gt;long[k2+1]) or (k1+1 &lt;= len(short)-1 and long[k2]&gt;short[k1+1])): if short[k1]&gt;long[k2+1]: k1-=1 k2 = k-k1-2 else: k1+=1 k2 = k-k1-2 compareL = [] count=0 while k1+1&lt;=len(short)-1 and count &lt;2: compareL.append(short[k1+1]) k1+=1 count+=1 count=0 while k2+1&lt;=len(long)-1 and count &lt;2: compareL.append(long[k2+1]) k2+=1 count+=1 x1 = min(compareL) compareL.remove(x1) x2 = min(compareL) return (float(x1)+float(x2))/2 可以加个二分查找就变为了O(log(min(m,n))),有空再更新吧 3Sum问题第二次重写Nsum问题，花了一个小时调试边界值，虽然基本思想没忘，但是对细节的处理很差，后面写上注意事项 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class Solution(object): def threeSum(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ if len(nums) &lt; 3: return [] result = [] def Nsum(N, L, subResult, target): # 这种写法的复杂度是nlog(n),超时了，改写为排序数组的2Sum写法 # if N == 2: # sumDict = &#123;&#125; # for i in L: # if i not in sumDict: # sumDict[target - i] = i # else: # # 这里用了一个sorted来解决重复问题，不知道有没有更好的办法 # if sorted(subResult+[target-i,i]) not in result: # print subResult+[target-i,i] # result.append(sorted(subResult+[target-i,i])) # return if N==2: l,r = 0,len(L)-1 while r&gt;l: if L[l]+L[r] == target: result.append(subResult+[L[r],L[l]]) while l&lt;r and L[l] == L[l+1]: l+=1 while l&lt;r and L[r] == L[r-1]: r-=1 r-=1 l+=1 elif L[l]+L[r] &gt; target: r-=1 elif L[l]+L[r] &lt; target: l+=1 else: for i in range(len(L)-N+1): # 每次跳过重复字符 # while i&gt;1 and i&lt;len(L)-1 and L[i]==L[i-1]: # i+=1 if i == 0 or (i &gt; 0 and L[i - 1] != L[i]): Nsum(N - 1, L[i + 1:], subResult + [L[i]], target - L[i]) Nsum(3,sorted(nums),[],0) return result 1：思路依然是递归的将Nsum问题转化为2Sum问题，这里2Sum因为要找的值是不用管下标的，所以可以将数组排好序，这样可以将2Sum问题的时间复杂度降为O(n),而找两个下标的2Sum问题的复杂度是O(nlogN),我一开始将两者弄混了，所以一开始用的找下标的那种方法，然后就超时了2：注意2Sum问题的时候，指针移动的时候，要先跳过所以重复值，加上l&lt;r就可以防止边界问题3：递归的地方我注释掉的部分，while i&gt;1 and i&lt;len(L)-1 and L[i]==L[i-1]:如果用这种方法来去掉遍历时候的重复值，会出现一些问题，比如[0,0,0,0,0]，虽然跳过了0，但是最后一个0还是会进，这样就会出现输出为[[0,0,0],[0,0,0]]的情况，所以用if可以防止这种情况，只要和前面重复都不进。 NSum Closest问题是Nsum问题的变形，但是原理差不多 123456789101112131415161718192021222324252627282930313233343536373839def threeSumClosest(nums, target): """ :type nums: List[int] :type target: int :rtype: int """ # 全局变量问题 global result global Min Min = 9999 result = 0 def findSum(N, L, Sum, target): global result global Min if N == 2: # 从L里找两个数，使得min(|(x1+x2+sum)-1|) l, r = 0, len(L) - 1 while l &lt; r: # 先替换 if Min &gt; abs(L[r] + L[l] + Sum - target): Min = abs(L[r] + L[l] + Sum - target) result = L[r] + L[l] + Sum # 是否特殊情况 if abs(L[r] + L[l] + Sum - target) ==0: result = L[r] + L[l] + Sum return # 移动指针 if L[r] + L[l] + Sum &gt; target and l&lt;r: r-=1 elif L[r] + L[l] + Sum &lt;target and l&lt;r: l+=1 else: for i in range(len(L) - N + 1): if i == 0 or (i &gt; 0 and L[i] != L[i - 1]): findSum(N - 1, L[i + 1:], Sum + L[i], target) findSum(3, sorted(nums), 0, target) return result 思路：还是递归的思想减少的2Sum问题，每次左右指针先算一次去替换最小，如果出现最优情况直接return，否则根据情况移动左右指针，跟Nsum还是有所区别。 Letter Combinations of a Phone Number数字组合，给出手机上的号码，每个号码对应着一组字母，求之间的有多少种组合方式Given a digit string, return all possible letter combinations that the number could represent. A mapping of digit to letters (just like on the telephone buttons) is given below.12Input:Digit string "23"Output: ["ad", "ae", "af", "bd", "be", "bf", "cd", "ce", "cf"]. 思路：就是一个简单的递归 12345678910111213141516171819202122232425262728293031class Solution(object): def letterCombinations(self, digits): """ :type digits: str :rtype: List[str] """ kvmaps = &#123; '2': 'abc', '3': 'def', '4': 'ghi', '5': 'jkl', '6': 'mno', '7': 'pqrs', '8': 'tuv', '9': 'wxyz' &#125; def comLetter(number, L): result = [] if number == '': return [] if L == []:result = [x for x in kvmaps[number[0]]] else: for x in kvmaps[number[0]]: for y in L: result.append(y+x) if len(number) == 1: return result else: return comLetter(number[1:], result) return comLetter(digits, []) 动态规划 最长不下降或不上升子序列参考 O(n)复杂度根据index删除链表中的元素给定一个链表，要求删除掉倒数第n个元素，然后返回head例子：12Given linked list: 1-&gt;2-&gt;3-&gt;4-&gt;5, and n = 2.After removing the second node from the end, the linked list becomes 1-&gt;2-&gt;3-&gt;5. 要求onepass，遍历一次123456789101112131415161718class Solution(object): def removeNthFromEnd(self, head, n): """ :type head: ListNode :type n: int :rtype: ListNode """ fast = slow = head for _ in range(n): fast = fast.next if fast == None: return head.next else: while fast.next: fast = fast.next slow = slow.next slow.next = slow.next.next return head 思路：两个指针，一个在前一个在后，他们之间的间隔为n，那么当前面的那个指针到达链表的尾部的时候，后面那个指针的下一个元素就是要删除掉的元素，直接next = next.next跳过就可以了 括号匹配问题Generate Parentheses给定一个数字，比如3，在3个括号的前提下求所有可能的括号匹配，例如：1234567[ "((()))", "(()())", "(())()", "()(())", "()()()"] 思路：回溯，注意两个点，一个是左括号一定等于右括号，第二个是左括号一定大于右括号1234567891011121314151617class Solution(object): def generateParenthesis(self, n): """ :type n: int :rtype: List[str] """ result = [] def addParentheses(strP,left,right): # 如果还剩下左括号，优先安排左括号 if left: addParentheses(strP+'(',left-1,right) # 如果右括号比左括号剩的多，再安排右括号 if right&gt;left:addParentheses(strP+')',left,right-1) # 这样，在递归的过程中，就可以列出所有可能的情况了 if right==0: result.append(strP) addParentheses('',n,n) return result 下面就3的例子画个图解释一下： 交换链表节点从左到右，一次交换每两个节点例如 1-&gt;2-&gt;3-&gt;4-&gt;5 到 2-&gt;1-&gt;4-&gt;3-&gt;5要求不准使用多余的空间，不准改变链表的值My Solution1234567891011121314151617181920212223242526272829class Solution(object): def swapPairs(self, head): """ :type head: ListNode :rtype: ListNode """ current = None if head==None: return [] if head.next: temp = head.next head.next = head.next.next temp.next = head head = temp current = head.next while current: if current.next: if current.next.next: first = current.next second = current.next.next current.next = second first.next = second.next second.next = first current = current.next.next else: break else: break return head 思路：因为头结点比较特殊，所以我把对头结点的交换和后面的交换给分开了，主要用到三个变量，current,first,scond,其中，first和second是要交换的节点，current是first的上一个节点，所以要判断后面是否有两个值可以交换要判断current.next.next是否为None,具体的交换思路看下图 Reverse Nodes in k-Group翻转链表中k-group，例如：123456For example:Given this linked list: 1-&gt;2-&gt;3-&gt;4-&gt;5For k = 2, you should return: 2-&gt;1-&gt;4-&gt;3-&gt;5For k = 3, you should return: 3-&gt;2-&gt;1-&gt;4-&gt;5 感冒了脑子太僵了，先占个坑 Next Permutation下一个最大的数字串，如果不是最大的，就找一个最小的比它大的，如果已经已经最大了，找一个最小的,例如：1231,2,3 → 1,3,23,2,1 → 1,2,31,1,5 → 1,5,1 思路： 从后往前遍历看看每一位是否能被替换 看每一位后面的所有字符是否有比它大的 挑出比它大的最小的那个来互换 互换完以后把该位后面的数按从小到大排序 123456789101112131415161718192021222324252627282930def nextPermutation(self, nums): """ :type nums: List[int] :rtype: void Do not return anything, modify nums in-place instead. """ if len(nums)==1 or 0: return AlreadyMax = 1 for i in range(-2,-len(nums)-1,-1): if max(nums[i:]) &gt; nums[i]: AlreadyMax = 0 min_index = 0 minvalue = 9999999 for j in range(len(nums)+i,len(nums)): if nums[j] &gt; nums[i] and nums[j] &lt; minvalue: min_index = j minvalue = nums[j] # change temp = nums[i] nums[i] = nums[min_index] nums[min_index] = temp sortedList = sorted(nums[len(nums)+i+1:]) m = 0 for j in range(len(nums) + i + 1, len(nums)): nums[j] = sortedList[m] m+=1 break if AlreadyMax: nums.reverse() return Search in Rotated Sorted Array翻转的有序数组里找个数，就是一个升序排序数组事先翻转了，例如：0 1 2 4 5 6 7 -&gt; 4 5 6 7 0 1 2 12345678910111213if len(nums) &lt; 3: return nums.index(target) if target in nums else -1def findIndex(head, tail): if tail - head &lt; 3: result = [i for i in range(head, tail + 1) if nums[i] == target] return -1 if result == [] else result[0] elif nums[int((tail + head) / 2)]&gt;=nums[head]: return findIndex(head, int((tail + head) / 2)) if nums[head]&lt;=target&lt;=nums[int((tail + head) / 2)] else findIndex(int((tail + head) / 2), tail) else: return findIndex(int((tail + head) / 2), tail) if nums[int((tail + head) / 2)]&lt;=target&lt;=nums[tail] else findIndex(head, int((tail + head) / 2))return findIndex(0, len(nums) - 1) 思路：递归，参数为数组头和尾指针 Search for a Range找数组中某个数开始和结束的index例子：1234567891011121314151617181920212223242526def searchRange(nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ if len(nums) == 0: return [-1, -1] def findIndex(head, tail): halfIndex = int((tail + head) / 2) halfNumber = nums[halfIndex] if nums[head] == nums[tail]: return [head,tail] if nums[head]==target else [-1,-1] if halfNumber != target and halfIndex-head &gt; 1 and tail - halfIndex &gt;1: return findIndex(head,halfIndex) if target &lt; halfNumber else findIndex(halfIndex,tail) else: while halfIndex-head &gt; 1 and nums[int((head+halfIndex)/2)]&lt;target: head = int((head+halfIndex)/2) while tail - halfIndex &gt;1 and nums[int((tail+halfIndex)/2)]&gt;target: tail = int((tail+halfIndex)/2) while tail &gt; head and nums[head]!=target: head+=1 while tail &gt; head and nums[tail]!=target: tail-=1 return findIndex(head,tail) return findIndex(0,len(nums)-1) 思路：二分法，我用的是从两边往中间缩，直到无法用二分法缩了以后，用while找到头和尾，详细一点： 终止条件：如果头的值等于尾的值，判断是否等于target，是返回最终结果，否则[-1,-1]说明不在list里 如果中间值不等于target说明该值在左半部分或右半部分，递归 左边每次缩一半，如果还没遇到target的话，右边同理，缩到最小 左右多余一点儿裁掉，找到head和tail，返回注意：int((head+halfIndex)/2)如果两个index紧邻的话会无限重复，因为int(a,a+1)=a,需要判别 Combination Sum给定一个正整数去重序列，求序列中所有和为target的非重复组合，每个数可以重复用，例如：1234[ [7], [2, 2, 3]] Mysolution12345678910111213141516171819202122232425class Solution(object): def combinationSum(self, candidates, target): """ :type candidates: List[int] :type target: int :rtype: List[List[int]] """ if len(candidates)==0: return [] candidates.sort() minValue = candidates[0] result= [] def findSum(Sum,tempL): if Sum == 0: final = sorted(tempL) if final not in result: result.append(final) else: for x in candidates: if Sum-x &gt;=0: findSum(Sum-x,tempL+[x]) else: break findSum(target,[]) return result 思路：递归，参数为每个子序列及其目前的和，每次用target减去总序列的每个值，如果为0，说明该子序列结束有解，如果为负数则无解，不添加该子序列。 Combination Sum II与上个题基本一样，但是序列是重复的，并且每个数不能重复用，求所有和为target的非重复序列思路：递归参数加一个，nowCandidate，大小为遍历到的该数后面的序列[i+1:]solution123456789101112131415161718192021222324252627class Solution(object): def combinationSum2(self, candidates, target): """ :type candidates: List[int] :type target: int :rtype: List[List[int]] """ if len(candidates) == 0: return [] candidates.sort() minValue = candidates[0] result = [] def findSum(Sum, tempL, nowCan): if Sum == 0: final = sorted(tempL) if final not in result: result.append(final) else: for i in range(len(nowCan)): if Sum - nowCan[i] &gt;= 0: findSum(Sum - nowCan[i], tempL + [nowCan[i]], nowCan[i+1:]) else: break findSum(target, [], candidates) return result Permutations给定一个非重复整数串，求所有可能的子序排列，例如：12345678910input:[1,2,3]output:[ [1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1]] 思路：递归，从剩下的序列中遍历选取一个加入到目前子序列，再将这个数从剩下的子序列中删除12345678910111213141516171819class Solution(object): def permute(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ if len(nums) == 0: return [] result = [] def findAll(nowL, remainL): if remainL == []: result.append(nowL) for i in range(len(remainL)): tempL = remainL[:] tempL.remove(remainL[i]) findAll(nowL + [remainL[i]], tempL) findAll([], nums) return result Permutations II与上题类似，但是给定序列中有重复数字，求所有可能的子序排列（去重复）只要在上题的基础上在每次从剩余序列选值的时候加上跳过重复值得操作即可，首先要对整个序列排序 12345678910111213141516171819202122class Solution(object): def permuteUnique(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ nums.sort() if len(nums) == 0: return [] result = [] def findAll(nowL, remainL): if remainL == []: result.append(nowL) for i in range(len(remainL)): # 加入下面这一行,因为不管在哪一轮，如果有两个以上重复的，那么选他们任意超过两个后续操作必然重复，因为没有区别 if i==0 or (i&gt;0 and remainL[i]!=remainL[i-1]): tempL = remainL[:] tempL.remove(remainL[i]) findAll(nowL + [remainL[i]], tempL) findAll([], nums) return result Valid Sudoku判断给的已经填充一部分的数独是否有效思路：只需要判断三个条件： 行不能重复 列不能重复 九宫格内不能重复所以，依次遍历每一个填充的值，把i,j,position（位于哪个九宫格）加入字典，只要有一个重复就fail 12345678910111213141516171819202122class Solution(object): def isValidSudoku(self, board): """ :type board: List[List[str]] :rtype: bool """ sudokuDict = &#123;&#125; n = len(board) for i in range(n): for j in range(n): # 判断在哪个九宫格内 position = [int(i / 3), int(j / 3)] if board[i][j] != '.': if board[i][j] not in sudokuDict: sudokuDict[board[i][j]] = [[i, j, position]] else: for l in sudokuDict[board[i][j]]: if l[0] == i or l[1] == j or l[2] == position: return False else: sudokuDict[board[i][j]].append([i, j, position]) return True Rotate Image旋转二位数组，例如：123456789101112131415Given input matrix =[ [ 5, 1, 9,11], [ 2, 4, 8,10], [13, 3, 6, 7], [15,14,12,16]], rotate the input matrix in-place such that it becomes:[ [15,13, 2, 5], [14, 3, 4, 1], [12, 6, 8, 9], [16, 7,10,11]] 思路：每次遍历最外层的正方形的第一层，找到四个位置的对应关系，依次替换即可（不用管奇数偶数问题，因为最后中心即使剩下一个单独的值无需替换） 1234567891011121314151617181920212223class Solution(object): def rotate(self, matrix): """ :type matrix: List[List[int]] :rtype: void Do not return anything, modify matrix in-place instead. """ n = len(matrix) for i in range(int(n/2)): start = i # 不包括ends end = n-i-1 for j in range(start,end): print(start,end) print(i,j) # matrix[i][j] -&gt; matrix[j][end] # ↑ ↓ # matrix[n-1-j][start] &lt;- matrix[n-1-i][n-1-j] temp = matrix[j][end] matrix[j][end] = matrix[i][j] matrix[i][j] = matrix[n-1-j][start] matrix[n-1-j][start] = matrix[n-1-i][n-1-j] matrix[n-1-i][n-1-j] = temp return Group Anagrams给定一个字符串数组，把里面字符串内容相同但是顺序不同的归到一起，例如：1234567given:: ["eat", "tea", "tan", "ate", "nat", "bat"]return:[ ["ate", "eat","tea"], ["nat","tan"], ["bat"]] 思路：依次遍历，用字典来存遍历过的字符串，键为排序过的字符串，如果不计算sort的时间的话，是nlog(n)12345678910111213141516171819class Solution(object): def groupAnagrams(self, strs): """ :type strs: List[str] :rtype: List[List[str]] """ if not strs: return [[]] result = [] searchDict = &#123;&#125; for s in strs: sortedS = ''.join(sorted(s)) if sortedS in searchDict: searchDict[sortedS] += [s] else: searchDict[sortedS] = [s] for k,v in searchDict.items(): result.append(v) return result Spiral Matrix 螺旋矩阵例如：123456[ [ 1, 2, 3 ], [ 4, 5, 6 ], [ 7, 8, 9 ]]output:[1,2,3,6,9,8,7,4,5] 思路：沿着一个方向一直走直到不能走，顺序是右下左上。1234567891011121314151617181920212223242526272829303132333435class Solution(object): def spiralOrder(self, matrix): """ :type matrix: List[List[int]] :rtype: List[int] """ if not matrix: return [] rows = len(matrix) cols = len(matrix[0]) hasMoved = &#123;&#125; for i in range(rows): hasMoved[i] = [0 for x in range(cols)] result = [] hasMoved[0][0] = 1 result.append(matrix[0][0]) i,j = 0,0 while len(result) != rows*cols: while j&lt;cols-1 and hasMoved[i][j+1] == 0: hasMoved[i][j+1] = 1 result.append(matrix[i][j+1]) j+=1 while i &lt; rows - 1 and hasMoved[i+1][j] == 0: hasMoved[i+1][j] = 1 result.append(matrix[i+1][j]) i += 1 while j &gt;0 and hasMoved[i][j-1] == 0: hasMoved[i][j-1] = 1 result.append(matrix[i][j-1]) j-=1 while i&gt;0 and hasMoved[i-1][j] == 0: hasMoved[i-1][j] = 1 result.append(matrix[i-1][j]) i -= 1 return result]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow]]></title>
    <url>%2F2017%2F10%2F10%2FTensorflow%2F</url>
    <content type="text"><![CDATA[使用flags定义命令行参数flag在TensorFlow中用于定义命令行参数参考链接1234567891011121314151617import tensorflow as tf#第一个是参数名称，第二个参数是默认值，第三个是参数描述tf.app.flags.DEFINE_string('str_name', 'def_v_1',"descrip1")tf.app.flags.DEFINE_integer('int_name', 10,"descript2")tf.app.flags.DEFINE_boolean('bool_name', False, "descript3")FLAGS = tf.app.flags.FLAGS#必须带参数，否则：'TypeError: main() takes no arguments (1 given)'; main的参数名随意定义，无要求def main(_): print(FLAGS.str_name) print(FLAGS.int_name) print(FLAGS.bool_name)if __name__ == '__main__': tf.app.run() #执行main函数 执行结果12345678[root@AliHPC-G41-211 test]# python tt.pydef_v_110False[root@AliHPC-G41-211 test]# python tt.py --str_name test_str --int_name 99 --bool_name Truetest_str99True Tensorflow实现线性回归(本习题也是港大深度学习的第一次作业) 线性回归原理单变量线性回归 因为是线性回归，所以学习到的函数为线性函数，即直线函数 因为是单变量，因此只有一个x我们能够给出单变量线性回归的模型：我们常称x为feature，h(x)为hypothesis（假设函数）Cost Function：对假设的函数进行评价，Cost Function越小的函数，说明对训练数据拟合的越好。下面给出costFunction公式：如果theta0和theta1都不固定，则theta0、theta1、J的函数为：注意如果是线性回归，则cost function一定是碗状的，即只有一个最小点。 损失函数最优化：梯度下降下面给出梯度下降算法：特点a)初始点不同，获得的最小值也不同，因此梯度下降求得的只是局部最小值；b)越接近最小值，下降速度越慢。问题1：如果和初始值就在local minimum的位置，则、会如何变化？答案：因为、已经在local minimum位置，所以derivative（导数）肯定是0，因此、不会改变。问题2：如果取到一个正确的值，则cost function应该会越来越小。那么，怎么取值？答案：随时观察值，如果cost function变小了，则OK；反之，则再取一个更小的值。 注意：下降的步伐大小非常重要，因为，如果太小，则找到函数最小值的速度就很慢；如果太大，则可能会出现overshoot the minimum现象。那既然梯度下降可以求到损失函数的最小值，线性回归又是需要一个最小的损失函数，那么可以将两者进行整合： Feature Scaling此种方法应用于梯度下降，为了加快梯度下降的执行速度。思想：将各个feature的值标准化，使得取值范围大致都在-1&lt;=x&lt;=1之间。常用的方法是Mean Normalization，即,或者[X-mean(X)]/std(X)。 多变量线性回归其假设函数为:损失函数仍然定义为平方损失函数最小化损失函数仍然可以用梯度下降法： 下面给出python实现12345678910111213141516171819202122232425262728293031323334353637383940import tensorflow as tfimport numpy as np# 构造数据集 -2到2 200个点 等差数列x_train = np.linspace(-2, 2, 200)# 构造 系数为2 偏差为4的y 加入一些随机噪音y_train = x_train * 2 + np.ones(len(x_train)) * 4 + np.random.randn(len(x_train)) * 0.02# Tensorflow构造模型# 1.定义输入和输出的占位符 tf中一般是tf.float32X = tf.placeholder('float')Y = tf.placeholder('float')# 2.定义参数变量,初始化为0w = tf.Variable(0.0, name='weights')b = tf.Variable(0.0, name='bias')# 3.定义假设模型的输出,wx+by_pred = tf.multiply(X, w) + b# 4.定义损失函数,这里是平方损失函数loss = tf.square(y_pred - Y)# 5.定义优化函数来最小化损失函数,学习率，要最小化的损失函数optimizer = tf.train.GradientDescentOptimizer(0.001).minimize(loss)# 到这里搭建完了图模型，但是不会产生任何运算结果，因为需要session来驱动sess = tf.Session()# 变量必须要初始化，如果变量有嵌套关系，则必须按顺序初始化sess.run(tf.initialize_all_variables())# 定义迭代次数iteration_nums = 500for i in range(iteration_nums): # 注意这里，要run的是optimizer这个函数，它又用到了loss这个函数，而loss函数又用到了y_pred这个结果，所以追溯向前，一共 # 用到了 X,Y 这两个被占位符声明的输入与输出以及w,b这两个变量，而参数变量正是我们要优化的目标 sess.run(optimizer, feed_dict=&#123;X: x_train, Y: y_train&#125;)# 学习完成后，变量w,b被重新赋值，打印w和bprint(sess.run(w))print(sess.run(b)) 最终的输出为122.00243.9702 与预期输出相符，注意我在一开始设置步长为0.01的时候发散掉了，最后w和b都是nan，所以合理的调参是必不可少的下面绘制一下样本点和回归函数的图像：12345678# 注意上面y_pred没有实际的值y_pred = sess.run(y_pred,&#123;X:x_train&#125;)plt.xlabel('Population of City in 10,000s')plt.ylabel('Profit in $10,000s')plt.plot(x_train,y_train,marker='x',lw=0,color='r',label='Training data')plt.plot(x_train,y_pred,linestyle='-',color='b',label='Linear regression line')plt.legend()plt.show() 再构造一些测试数据，用学习到的模型进行预测12345678910111213# 1.定义测试数据X_test = tf.placeholder('float')# 2.定义相关参数w = sess.run(w)b = sess.run(b)# 3.定义预测predict = tf.multiply(w,X_test)+b# 4.得到预测结果sess = tf.Session()sess.run(tf.initialize_all_variables())test_pred = sess.run(predict,&#123;X_test:x_test&#125;)print(test_pred) 绘制测试集上的回归函数图像：可以看到拟合的效果还是不错的。 这是freedom098博客上的一点关于梯度下降的一点儿感悟：1、梯度下降法分为批量梯度下降和随机梯度下降法，第一种是所有数据都参与运算后，计算误差函数，根据此误差函数来更新模型参数，实际调试发现，如果定义误差函数为平方误差函数，这个值很快就会飞掉，原因是，批量平方误差都加起来可能会很大，如果此时学习率比较高，那么调整就会过，造成模型参数向一个方向大幅调整，造成最终结果发散。所以这个时候要降低学习率，让参数变化不要太快。2、随机梯度下降法，每次用一个数据计算误差函数，然后更新模型参数，这个方法有可能会造成结果出现震荡，而且麻烦的是由于要一个个取出数据参与运算，而不是像批量计算那样采用了广播或者向量化乘法的机制，收敛会慢一些。但是速度要比使用批量梯度下降要快，原因是不需要每次计算全部数据的梯度了。比较折中的办法是mini-batch，也就是每次选用一小部分数据做梯度下降，目前这也是最为常用的方法了。3、epoch概念：所有样本集过完一轮，就是一个epoch，很明显，如果是严格的随机梯度下降法，一个epoch内更新了样本个数这么多次参数，而批量法只更新了一次。 tf.multiply和tf.matmul区别其中tf.multiply是点乘，tf.matmul是矩阵乘法，点乘要求维度相同，结果是每个对应的元素相乘，矩阵乘法要求MxN NxK,最后得到MxK举个例子:1234567891011A = np.array([[1,2,3],[4,5,6]])B = np.array([[5],[2],[1]])C = np.array([[1,0,-1],[2,-2,0]])print('矩阵A')print(A)print('矩阵B')print(B)print('矩阵A乘以B')print(np.matmul(A,B))print('矩阵A点乘C')print(np.multiply(A,C)) Output12345678910111213矩阵A[[1 2 3] [4 5 6]]矩阵B[[5] [2] [1]]矩阵A乘以B[[12] [36]]矩阵A点乘C[[ 1 0 -3] [ 8 -10 0]] np.dot12345678910111213A = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])B = np.array([[1],[2],[3],[4]])C = np.array([[1],[2],[3],[4]])D = np.array([1,2,3,4,5])E = np.array([1,2,3,4,5])# 当x,y都为一维数组的时候，np.dot表示内积print(np.dot(D,E))#print(np.dot(np.mat(B).T,np.mat(C).T))# 除此之外，表示矩阵乘积print(np.dot(A,C)) 输出12345655[[ 30] [ 70] [110] [150]] 手动实现Logistic参考链接1下面两个图可以看到线性回归和逻辑回归的联系，因为逻辑回归的假设函数的输出是0到1之间的概率，相当于把线性回归的假设函数归一化那现在有一个线性回归模型$ z = \theta^Tx $,要将输出转为y(0,1)，那么就需要将z转为y，如果用单位阶跃函数来表示:由于其不连续，所以用sigmoid函数代替：$ y = \frac{1}{1+e^{-z}} $则其输出为（假设函数）:$ y = \frac{1}{1+e^{-(\theta^Tx)}} $其函数图像为：如果将假设函数进行变形，将theta向量与X矩阵相乘拿出来的话，可以得到：那这也就是对数几率回归，y看成是样本x正例的概率，1-y则为样本x负例的概率，这里将条件概率模型转化为了线性模型:那么接下来要求样本点的联合概率分布，对于每一个样本点来说:那考虑所有样本，由于每个样本点互相独立，因此他们的联合概率分布等于各自的边缘概率分布的积：其对数似然函数为：现在需要使得联合概率分布最大，或者说使得其对数似然函数最大，需要使用梯度下降法来求θ(其损失函数可以说是-1/m * 对数似然函数（联合概率分布）)给出其损失函数并给出从损失函数到偏导的推导: 下面给出其python实现过程：12345678910111213141516171819import numpy as npimport matplotlib.pyplot as plt# 构造数据集x_train = [[1.0, 2.0], [2.0, 1.0], [2.0, 3.0], [3.0, 5.0], [1.0, 3.0], [4.0, 2.0], [7.0, 3.0], [4.0, 5.0], [11.0, 3.0], [8.0, 7.0]]y_train = [1, 1, 0, 1, 1, 0, 0, 1, 0, 1]# 可视化good_index = []bad_index = []for i in range(len(y_train)): if y_train[i]==1: good_index.append(i) else: bad_index.append(i)plt.xlabel('x')plt.ylabel('y')plt.scatter(np.array([x[0] for x in x_train])[good_index],np.array([x[1] for x in x_train])[good_index],marker='o',color='k',label='good')plt.scatter(np.array([x[0] for x in x_train])[bad_index],np.array([x[1] for x in x_train])[bad_index],marker='x',color='g',label='bad')plt.show() 1234567891011121314151617# 初始化变量 由于这里是两维特征 h(θ) = θ0*x0 + θ1*x1 + θ2*x2# 为了构造矩阵相乘，给数据集X添加一列1init_theta = np.array([1,2,1])X = np.array([[1.0]+x for x in x_train])y = np.array(y_train)# 构造假设函数def Objective(theta,X): return 1 / (1 + np.exp(-np.dot(theta.reshape(1,3),X.T)))# 构造损失函数def loss(theta,X,y): pred_y = Objective(theta,X)[0] # 在当前theta下训练集的损失函数值,求原来损失函数的最大，在这里取负号就是最小 lossSum = 0 for i in range(X.shape[0]): lossSum += -y[i]*pred_y[i] + np.log(1+np.exp(pred_y[i])) return lossSum/X.shape[0]print('在初始theta的情况下，训练集的损失函数值为:',loss(init_theta,X,y)) 1在初始theta的情况下，训练集的损失函数值为: 0.71354969579 1234567891011121314# 优化损失函数# 为了寻找最优的theta向量使得损失函数最小，这里使用梯度下降法# 计算当前梯度def Gradient(theta,X,y): grad = [] pred_y = Objective(theta,X)[0] for i in range(X.shape[1]): cur_grad = 0 # 当前梯度跟每一个样本实例都有关系 for j in range(X.shape[0]): cur_grad += (pred_y[j]-y[j]) * X[j][i] grad.append(cur_grad/X.shape[0]) return np.array(grad)print('初始梯度',Gradient(init_theta,X,y)) 1初始梯度 [ 0.39880029 2.39851299 1.09756494] 1234567891011121314151617181920212223242526# 使用梯度下降找到最优参数def logistic(theta,X,y,eta): loss_list = [] minLoss = 1000 minTheta = None # 定义迭代次数 theLoss = 0 iteration_times = 5000 for i in range(iteration_times): # 每一轮迭代更新损失theta向量和损失函数值 theta = theta - eta * Gradient(theta,X,y) theLoss = loss(theta,X,y) if theLoss &lt; minLoss: minLoss = theLoss minTheta = theta loss_list.append(theLoss) return loss_list,minLoss,minThetaloss_list,minLoss,minTheta = logistic(init_theta,X,y,0.01)# 画出收敛曲线的图axis_x = [x for x in range(len(loss_list))]plt.xlabel('iteration times')plt.ylabel('loss function value')plt.plot(axis_x,loss_list,linestyle='-',color='b',label='Convergent curve')plt.legend()plt.show() 交叉熵Corss-entropy和Softmax交叉熵Cross-entropy参考链接 Softmax函数Softmax函数用于多分类或二分类，可以对样本输出的每一个类别给出对应的概率值这里的softmax可以看成是一个激励（activation）函数或者链接（link）函数，把我们定义的线性函数的输出转换成我们想要的格式，也就是关于10个数字类的概率分布。因此，给定一张图片，它对于每一个数字的吻合度可以被softmax函数转换成为一个概率值 Logistic回归：MINIST训练集MNIST是一个入门级的计算机视觉数据集，它包含各种手写数字图片： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import tensorflow as tfimport numpy as npfrom tensorflow.examples.tutorials.mnist import input_data# 读入数据MNIST = input_data.read_data_sets("data", one_hot=True)# 定义相关超参eta = 0.01batch_size = 128n_epochs = 25# 为输入和输出定义placeholder# 因为每个在MNIST中的像素是28*28 = 784# 所以每一个图像都有784个特征，是一个1x784的张量X = tf.placeholder('float32',[batch_size,784])y = tf.placeholder('float32',[batch_size,10])# 创建要调整的参数变量 weights and bias# 784，10 是因为要输出一个长度为10的结果向量w = tf.Variable(tf.random_normal(shape=[784,10],stddev=0.1), name='weights')b = tf.Variable(tf.zeros([1,10]), name='bias')# 定义假设函数logits = tf.matmul(X,w) + b# 定义损失函数entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y)loss = tf.reduce_mean(entropy)# 梯度下降optimizer = tf.train.GradientDescentOptimizer(learning_rate=eta).minimize(loss)# 初始化变量sess = tf.Session()sess.run(tf.global_variables_initializer())# 这里batch_size是每一次训练多少个样本，这里要算每一轮要训练多少次n_batches = int(MNIST.train.num_examples/batch_size)# 训练多少轮for i in range(n_epochs): for _ in range(n_batches): # 该循环的每个步骤中，我们都会随机抓取训练数据中的batch_size个批处理数据点，然后我们用这些数据点作为参数替换之前的占位符来运行train_step X_batch,Y_batch = MNIST.train.next_batch(batch_size) sess.run([optimizer,loss],&#123;X:X_batch,y:Y_batch&#125;)# 测试模型corrects = 0n_batches = int(MNIST.test.num_examples/batch_size)total_correct_preds = 0for i in range(n_batches): X_batch, Y_batch = MNIST.test.next_batch(batch_size) _,loss_batch,logits_batch = sess.run([optimizer,loss,logits],feed_dict=&#123;X:X_batch,y:Y_batch&#125;) preds = tf.nn.softmax(logits_batch) corrects_preds = tf.equal(tf.argmax(preds,1),tf.argmax(Y_batch,1)) accuracy = tf.reduce_sum(tf.cast(corrects_preds,tf.float32)) total_correct_preds += sess.run(accuracy)print('Accuracy',total_correct_preds/MNIST.test.num_examples) 1&gt;&gt;&gt; Accuracy 0.9077 决策树作业$$0.0902763493928-\frac{8}{14}-\frac{4}{8}log2\frac{4}{8}-\frac{6}{14}-\frac{1}{6}log2\frac{1}{6}=0.0902763493928$$]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[加权AUC原理与实现]]></title>
    <url>%2F2017%2F06%2F21%2Fauc%2F</url>
    <content type="text"><![CDATA[AUC原理AUC的直观解释是ROC曲线下的面积，ROC曲线由TPR和FPR组成：（其实AUC只是Area under curve,这个curve不一定是ROC,当样本不均衡的时候可以用precision-recall curve,但是这里只讨论ROC曲线下的面积） &lt;img src=”auc/1.png” width=”200”, alt=”none”, align=”center”&gt; TPR(True positive rate): TP / (TP + FN) 表示预测正确的正类占总实际总正类的比例 ，显然TPR越大，模型对正类的预测越准。(但是考虑全部预测为正呢，TPR为1，但是模型没有任何预测能力，因此还有FPR) FPR(False Positive Rate):FP / (FP + TN) 表示预测错的正类占实际所有负类的比例(本来是负的，预测为正的比例可以修正盲目的预测为正而带来的TPR的提升)， 我们希望的是FPR越小而TPR越大。即图中(0,1)点，故ROC曲线越靠拢(0,1)点，越偏离45度对角线越好，Sensitivity、Specificity越大效果越好。 &lt;img src=”auc/6.png” width=”200”, alt=”none”, align=”center”&gt; ROC曲线的绘制：每个样本都有一个pred值和true label值得pair, 有一个阈值来决定pred值超过多少就判定为正类，将样本按pred值排序，遍历每一个pred值将其作为当前的阈值，可以算出一个(TPR,FPR)对儿，这样有多少个阈值就有多少个R对儿，就可以有多少个点来绘制ROC曲线，即: 阈值总个数 = (FPR,TPR)对数 = ROC曲线点数 举例： &lt;img src=”auc/2.png” width=”150”, alt=”none” align=”center”&gt; 根据每一个阈值可以计算出下表： &lt;img src=”auc/3.png” width=”250”, alt=”none”, align=”center”&gt; 然后根据横轴(FPR)和纵轴(TPR)绘制ROC曲线(间隔根据R值来)： &lt;img src=”auc/4.png” width=”450”, alt=”none”, align=”center”&gt; 然后就可以计算ROC曲线下面积也就是AUC的值了，但是这样做会有缺点就是计算比较麻烦(注意图中的斜向上的梯形部分，这是由于两个样本有相同的pred值但是它们的true label不同导致的)，实际中我们通常使用其他方法。 #AUC实际意义 AUC表示ROC曲线下面积，这是它的物理意义，而它的实际意义(概率)表示随机抽取一个正样本和负样本，正样本的pred大于负样本的pred的概率。这也很好理解，当auc为0.5的时候，正样本大于负样本的概率为0.5相当于随机，auc为1的时候，随机选取正样本的pred一定大于负样本的pred，表示可以将所有的正样本分出来，分类性能完美。 AUC的实际意义来自于其和Wilcoxon-Mann-Witney Test等价。 有了等价这两个字，就可以妥妥的用其他方法来计算auc了： 1.最笨也是最直观的方法就是，假设有M个正样本和N个负样本，那么就依次遍历取MxN个样本对儿，其中(T_score &gt; F_score pairs) / M x N 就是auc值，当T_score = F_score 的时候按0.5算。这种最简单的计算方法的时间复杂度$O(n^2)$ 2.先将score按升序排序 &lt;img src=”auc/7.png” width=”250”, alt=”none”, align=”center”&gt; 先上公式： &lt;img src=”auc/8.png” width=”350”, alt=”none”, align=”center”&gt; 下面我们证明一下这个计算公式和方法2计算是一致的 首先是分母部分，T*F表示所有正负样本pair数，对于分子部分 设每个正样本i的rank值为Ri，其score为si， 若Ri的score唯一，表示si大于Ri-1个样本，样本i和这Ri-1个样本组成的pair权值为1，所有此类正样本的rank之和表示和之前的所有Ri个样本组成的pair数（包括和自己的pair以及之前的正样本pair也算在内） 如Ri的score不唯一，不妨设此时有p个正样本和q个负样本score和i相同，那么此时有p*q个pair权值为0.5。 假设这连续p+q个样本中的第一个rank为t，则第p+q个样本的rank为t+p+q-1，根据方法3所述，这p+q个样本的rank值用平均rank来代替，为(t+t+p+q-1)/2，则p个正样本的ranker和为(t+t+p+q-1)p0.5=p(t+t+p-1)/2+0.5p*q。 我们看到第二项就是p*q个正负样本pair的加权和，而第一项是rank从r,r+1,…,r+p-1这p个正样本的rank和，它表示这p个正样本和r前面的样本组成的pair数（这p个样本和自己的pair和之前的正样本pair也算在内）。 从1和2分析看，所有正样本rank和会把正样本自己（共T个pair）和自己之前的所有正样本组成的pair（共T(T-1)/2个pair）都计算一遍，并且权重为1，因此最后要去掉这个计数，这个计数就是T(T+1)/2个pair，因此方法3的公式的分子算出来是正确正负样本pair的加权和。 加权AUC(WAUC)&lt;img src=”auc/11.png” width=”850”,alt=”none”,align=”center”&gt; 这是论文中给出的WAUC的计算方法，但是我们常说的WAUC如下： &lt;img src=”auc/12.png” width=”850”,alt=”none”,align=”center”&gt; 相当于本来的ranking是针对点击率，但是对score进行bid加权以后，是对价格bid*点击率CTR的结果进行ranking，这对于我们期待的效果是更合理的，因为我们想得到的是最大的利润而不是更大的点击率。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反向传播 | 梯度消失(爆炸) | Relu及其变种]]></title>
    <url>%2F2017%2F06%2F14%2Frelu%2F</url>
    <content type="text"><![CDATA[#]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小知识点记录]]></title>
    <url>%2F2017%2F06%2F14%2Fsuishou%2F</url>
    <content type="text"><![CDATA[Logits: 累积分布函数重要： 1.累积分布函数CDF主要是为了处理连续型随机变量C.R.V用的，因为对于连续型随机变量C.R.V只能求解区域概率，而不能求解数轴上一个点的概率，CDF其实就是一个概率的累加。 2.先有累积分布函数，再有概率密度函数。 3.概率密度函数是累积分布函数的导数，累积分布函数是概率密度函数的积分 4.累积分布函数的一个点是有意义的，它表示某个概率F(x) = F(X&lt;=x),但是概率密度函数单看一个点是没有意义的，要看某个区域的积分才有意义，而其就相当于累积分布函数的F(x1&lt;=x&lt;=x2) 5.累积分布函数的一个点的值代表随机变量落在一个区域内的概率而不是一个点的概率！ 定义累积分布函数(Cumulative Distribution Function)，又叫分布函数，是概率密度函数的积分，能完整描述一个实随机变量X的概率分布。一般以大写CDF标记,，与概率密度函数probability density function（小写pdf）相对。 它表示对于离散变量而言所有小于等于a的值出现的概率之和(注意是离散变量，因为累积分布函数只能表示区域概率) 性质单调递增，有界[0,1]，右连续。X的值落在(a,b]之间的概率为: 反函数 示例 注意： 对于已知的离散的值和一段已知的连续区间去计算累积分布是有区别的，比如离散序列[1 2 3 4 5 6 7],那么F(1) = F(x&lt;=1) = 1/7 , 而如果是[1,7]的连续区间，那么F(1)=0，F(2) = 1/6，因为累积分布计算的是一个区域的值，对于连续区间[1,7]来讲，&lt;=1只有1一个点，它的概率相当于0. 概率密度函数在数学中，连续型随机变量的概率密度函数（在不至于混淆时可以简称为密度函数）是一个描述这个随机变量的输出值，在某个确定的取值点附近的可能性的函数。而随机变量的取值落在某个区域之内的概率则为概率密度函数在这个区域上的积分。当概率密度函数存在的时候，累积分布函数是概率密度函数的积分。概率密度函数一般以小写标记。 定义如果某累积分布函数可导，则其导数就是概率密度函数。 性质性质比较重要，其在负无穷到正无穷区间上的积分为1. 非架构化数据非结构化数据是数据结构不规则或不完整，没有预定义的数据模型，不方便用数据库二维逻辑表来表现的数据。包括所有格式的办公文档、文本、图片、XML, HTML、各类报表、图像和音频/视频信息等等。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[word2vec]]></title>
    <url>%2F2017%2F06%2F09%2Fword2vec%2F</url>
    <content type="text"><![CDATA[语言模型简述在NLP中，最基本的表示一个词的方法是one-hot，将每个词基于一个vocabulary映射为一个高纬度的向量，但是这样这样每个词之间并没有联系。 向量空间模型(Vector space models, VSMs)将词语表示为一个连续的词向量，并且语义接近的词语对应的词向量在空间上也是接近的。VSMs在NLP中拥有很长的历史，但是所有的方法在某种程度上都是基于一种分布式假说，该假说的思想是如果两个词的上下文(context)相同，那么这两个词所表达的语义也是一样的；换言之，两个词的语义是否相同或相似，取决于两个词的上下文内容，上下文相同表示两个词是可以等价替换的。 发掘上述信息有两种方法:”计数法”和“预测法”。计数法是在大型语料中统计词语及邻近的词的共现频率，然后将之为每个词都映射为一个稠密的向量表示；预测法是直接利用词语的邻近词信息来得到预测词的词向量（词向量通常作为模型的训练参数）。 我使用DNN来训练的目的就是为了对每一个词语训练这样一个向量，使得它们与上下文有关联而在空间的分布上产生类似聚类的效果来描述不同词语之间的联系。 word embedding近年来，word embedding可以说已经成为了各种神经网络方法（CNN、RNN乃至各种网络结构，深层也好不深也罢）处理NLP任务的标配。 word embedding（词嵌入；词向量）是指基于神经网络来得到词向量的模型（如CBOW、Skip-gram等，几乎无一例外都是浅层的）所train出来的词的定长向量表示，这种向量表示被称为是分布式表示distributed representation，大概就是说单独看其中一维的话没什么含义，但是组合到一起的vector就表达了这个词的语义信息（粒度上看的话，不止词，字、句子乃至篇章都可以有分布式表示；而且，例如网络节点、知识图谱中的三元组等都可以有自己的embedding，各种“xx2vec”层出不穷） 非监督的“监督学习”从应用角度，新空间内映射函数的学习方法不需要大量的人工标记样本就可以得到质量还不错的embedding向量，没有具体的应用任务导向，从这个角度可以看作非监督的学习过程，而从建模角度，向量提取的建模过程是 个分类模型，又可以看做是监督学习，只是这个监督没有实际的监督意义，当然后来有的应该将word2vec的前段表达方式喂给标注的过文本，形成真正意义上的监督学习，如Facebook的FastText。所以word2vec可以说是一种伪监督学习 skip-gramskip-gram是word2vec的其中一种模型(另一种常用的是CBOW)，其输入为一个单词，输出为这个单词的上下文窗口大小的N个单词，其模型为: 其中，输入向量代表某个单词的one-hot编码，对应的输出向量{y1,…,yC}。输入层与隐藏层之间的权重矩阵W的第i行代表词汇表中第i个单词的权重,而这个输入层到隐藏层的权重矩阵就是我们学习的目标。 下面的图中给出了一些我们的训练样本的例子。我们选定句子“The quick brown fox jumps over lazy dog”，设定我们的窗口大小为2（window_size=2），也就是说我们仅选输入词前后各两个词和输入词进行组合。下图中，蓝色代表input word，方框内代表位于窗口内的单词。 当然还可以设置参数num_skips,它代表从每个窗口中获得的样本数，上图是获得尽可能多的样本。然后将样本转化为神经网络的输入，做法是将所有的词总结为一个vocabulary,然后将输入的单词映射为一个one-hot向量，例如一共有10000个单词，那么输入就是一个维度为10000的向量，其输出层有10000个神经元，每个神经元的值代表当前次是输入词的output word的可能性大小。 由于输入层是one-hot，所以隐藏层没有使用任何激活函数，但是输出层使用了softmax.我们基于成对的单词来对神经网络进行训练，训练样本是 ( input word, output word ) 这样的单词对，input word和output word都是one-hot编码的向量。最终模型的输出是一个概率分布。 隐层如果我们现在想用300个特征来表示一个单词（即每个词可以被表示为300维的向量）。那么隐层的权重矩阵应该为10000行，300列（隐层有300个结点）。 看下面的图片，左右两张图分别从不同角度代表了输入层-隐层的权重矩阵。左图中每一列代表一个10000维的词向量和隐层单个神经元连接的权重向量。从右边的图来看，每一行实际上代表了每个单词的词向量。 我们的输入被one-hot编码以后大多数维度上都是0（实际上仅有一个位置为1），所以这个向量相当稀疏，那么会造成什么结果呢。如果我们将一个1 x 10000的向量和10000 x 300的矩阵相乘，它会消耗相当大的计算资源，为了高效计算,可以根据one-hot向量1的索引从权重矩阵中直接得到结果向量，这样就直接避免了任何的矩阵乘法计算，节约了时间和计算开销。相当于去查询“查找表”，即”lookup table”。 输出层经过神经网络隐层的计算，ants这个词会从一个1 x 10000的向量变成1 x 300的向量，再被输入到输出层。输出层是一个softmax回归分类器，它的每个结点将会输出一个0-1之间的值（概率），这些所有输出层神经元结点的概率之和为1。 下面是一个例子，训练样本为 (input word: “ants”， output word: “car”) 的计算示意图 Softmax既然在输出层提到softmax，这里再介绍一下，定义：假设我们有一个数组，V，Vi表示V中的第i个元素，那么这个元素的Softmax值就是： 也就是该元素的指数和所有元素指数的和的比值。(Si不是该数组中Vi这个元素出现的概率！)，拿上面的word2vec来说，输入层是个10000维的向量(vocabulary的长度为10000),那么softmax的维度也是10000，有10000个数经过softmax函数以后变为0-1之间的概率，分别表示输入对应的输出是这个词的概率，它们的和为1。我们训练全连接层的目的就是让最后softmax输出为正确样本的概率最高。 那么当训练刚开始的时候输出的概率结果不尽如人意的时候如何通过损失来修正参数呢，这里引入了softmax函数对应的soft loss 公式中的sj就是softmax函数输出的第j个值，yj是y的第j个值，y是label，也就是true label对应的one-hot,比如上面的词向量输入是一个10000维的向量，其label也是10000维，其中只有一维是1。也就是说，上述求和公式，只有一个yj是1，其它都是0，也就是只有正确的那个label对应的softmax值才发挥了作用，所以可以简化为如下的公式。 因为si一定是一个0-1之间的数，所以logsj是一个负数，si越大，这个负数离0越近，所以取负以后就越小，即Loss就越小。所以soft loss代表的意义就是正确样本的softmax概率越大，其损失越小。 当我们对分类的Loss进行改进的时候，我们要通过梯度下降，每次优化一个step大小的梯度。然后我们求Loss对每个权重矩阵的偏导，应用链式法则（中间推导省略）。 最后结果的形式非常的简单，只要将算出来的概率的向量对应的真正结果的那一维减1，就可以了 举个例子，通过若干层的计算，最后得到的某个训练样本的向量的分数是[ 1, 5, 3 ], 那么概率分别就是[0.015,0.866,0.117],如果这个样本正确的分类是第二个的话，那么计算出来的偏导就是[0.015,0.866−1,0.117]=[0.015,−0.134,0.117]，是不是很简单！！然后再根据这个进行back propagation就可以了 CBOWCBOW(continuous bag-of-word model)连续词袋模型，CBOW与skip-gram相似，也只有一层hidden layer而且这层没有任何激活函，而仅仅是对权重矩阵求得的词向量求平均(所以也可以称之为映射层，而embedding其实就是一种映射)，区别就是输入是上下文的n个词，求其对应的一个词。下图是CBOW模型: 其实CBOW跟skip-gram非常相似，其映射层到输出层是一样的，都是通过一个NxV矩阵然后softmax激活，区别是映射层的得到一个是直接index得到，另一个是很多个词index得到的向量求平均得到，下面是其损失函数及其链式更新法则公式的推导。 霍夫曼树word2vec有CBOW和skip-gram模型，上面介绍了其DNN训练词向量的过程，其实还有一种效率更高的做法，就是利用霍夫曼树，这里介绍霍夫曼树的基础。霍夫曼树的建立其实并不难，过程如下： 输入：权值为(w1,w2,…wn)的n个节点 输出：对应的霍夫曼树 1）将(w1,w2,…wn)看做是有n棵树的森林，每个树仅有一个节点。 2）在森林中选择根节点权值最小的两棵树进行合并，得到一个新的树，这两颗树分布作为新树的左右子树。新树的根节点权重为左右子树的根节点权重之和。 3） 将之前的根节点权值最小的两棵树从森林删除，并把新树加入森林。 4）重复步骤2）和3）直到森林里只有一棵树为止。 下面我们用一个具体的例子来说明霍夫曼树建立的过程，我们有(a,b,c,d,e,f)共6个节点，节点的权值分布是(16,4,8,6,20,3)。 首先是最小的b和f合并，得到的新树根节点权重是7.此时森林里5棵树，根节点权重分别是16,8,6,20,7。此时根节点权重最小的6,7合并，得到新子树，依次类推，最终得到下面的霍夫曼树。 那么霍夫曼树有什么好处呢？一般得到霍夫曼树后我们会对叶子节点进行霍夫曼编码，由于权重高的叶子节点越靠近根节点，而权重低的叶子节点会远离根节点，这样我们的高权重节点编码值较短，而低权重值编码值较长。这保证的树的带权路径最短，也符合我们的信息论，即我们希望越常用的词拥有更短的编码。如何编码呢？一般对于一个霍夫曼树的节点（根节点除外），可以约定左子树编码为0，右子树编码为1.如上图，则可以得到c的编码是00。在word2vec中，约定编码方式和上面的例子相反，即约定左子树编码为1，右子树编码为0，同时约定左子树的权重不小于右子树的权重。 Negative Sampling在上面举的例子中，假设输入有10000个单词，每个单词映射为300维的稠密向量，那么权重矩阵的大小就是3000000，在神经网络上跑梯度下降来优化这么多的权重参数将会非常的慢，而且需要海量的数据来训练网络来防止过拟合。 有三个解决方案： 把常见的词组作为一个单词。 少采样常见的词(去掉A an the等) 修改优化目标函数，这个策略成为“Negative Sampling“，使得每个训练样本只去更新模型中一小部分的weights。 1.比如“Boston Globe“（一家报社的名字）这种词，两个单词拆开意思相差非常远，所以应该把这两个词组为一个单词加入到字典中。 2.还是以这个句子为例子： 其中比如(the,qucik)并没有传达出关于quick相关的信息，因此我们希望减少类似于”the”这种单词的出现。对于这种词，我们有一个概率去删除它，而这种概率是由该词出现的频率决定的，在word2vec作者的论文中有一个函数来计算保留某一个词的偏好程度。$$p(w_i)=(\sqrt{z(w_i)/sampleRate+1}) * sampleRate/z(w_i)$$$z(w_i)$是该词出现的频率，sampleRate来控制一个阈值，该值越小就会有越多的词被删除。假设当$z(w_i)&lt;=0.0026$的情况下(SampleRate=0.001)，$p(w_i)=1$，因此保留这个词的偏好程度很高，不会扔掉，如果某个词出现的频率为1，即$z(w_i)=1$的时候，$p(w_i)=0.033$因此保留这个词的可能性非常小。 3.像上面提到的，如果我们每次训练一个样本就要调整所有的3000000个参数的话，训练的过程将会非常的慢而且有可能产生过拟合。Negative sampling解决了这个问题，它每次去修改一小部分的weights而不是全部，通过实验来看，这样不仅仅加快了训练的过程，而且提高了最后word vector的质量。 negative的做法就是每次更新权重的时候，只选取一小部分的样本权重去更新，而这种负样本采样方案如下： 引用lyb3b3b的专栏AI研习社]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习降维-原理及实践]]></title>
    <url>%2F2017%2F05%2F11%2Fjiangwei%2F</url>
    <content type="text"></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
